{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification: Now with Text :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle StumbleUpon Competition\n",
    "\n",
    "https://www.kaggle.com/c/stumbleupon\n",
    "\n",
    "** Competition **: \n",
    "1. Some web pages, such as news articles or seasonal recipes, are only relevant for a short period of time. Others continue to be important for a long time.\n",
    "2. The goal is to identify pages which pages will be relevant for a short span of time, and which will be relevant for a long span on time and are thus considered \"evergreen\". \n",
    "\n",
    "** Evaluation **: Area under the curve (AUC) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Python Modules \n",
    "================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick hack to fix import path\n",
    "# import sys; sys.path.append('/Users/julianalverio/code/conda/envs/sac/lib/python3.6/site-packages/')\n",
    "\n",
    "# data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# plots\n",
    "%matplotlib inline\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "\n",
    "# classification algorithms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "# dimensionality reduction\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# cross-validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection\n",
    "\n",
    "# text features\n",
    "import re\n",
    "from nltk import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# model evaluation\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "os.chdir(os.path.join(\"..\", \"data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/machine_learning_aws/data'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-01-13 13:38:08--  https://www.dropbox.com/s/10ch2yhfk8tyfri/train.tsv?dl=0\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.6.1, 2620:100:601c:1::a27d:601\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.6.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: /s/raw/10ch2yhfk8tyfri/train.tsv [following]\n",
      "--2020-01-13 13:38:13--  https://www.dropbox.com/s/raw/10ch2yhfk8tyfri/train.tsv\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://ucb3c76d03568fde68e81c913d07.dl.dropboxusercontent.com/cd/0/inline/AwHu6jPDXSPwttUboRcpgC2IHOjcMBZTawORyg7eCB-tCMSs-E369nHR0eUobWbGQmeGysD-ebRsO50ZWgibvoLfGN5q13if9lYjb1WsQ_KUlpE_j33T-XLCYBePRy3FnFw/file# [following]\n",
      "--2020-01-13 13:38:14--  https://ucb3c76d03568fde68e81c913d07.dl.dropboxusercontent.com/cd/0/inline/AwHu6jPDXSPwttUboRcpgC2IHOjcMBZTawORyg7eCB-tCMSs-E369nHR0eUobWbGQmeGysD-ebRsO50ZWgibvoLfGN5q13if9lYjb1WsQ_KUlpE_j33T-XLCYBePRy3FnFw/file\n",
      "Resolving ucb3c76d03568fde68e81c913d07.dl.dropboxusercontent.com (ucb3c76d03568fde68e81c913d07.dl.dropboxusercontent.com)... 162.125.6.6, 2620:100:601c:6::a27d:606\n",
      "Connecting to ucb3c76d03568fde68e81c913d07.dl.dropboxusercontent.com (ucb3c76d03568fde68e81c913d07.dl.dropboxusercontent.com)|162.125.6.6|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 21972916 (21M) [text/plain]\n",
      "Saving to: ‘/home/ubuntu/machine_learning_aws/data/train.tsv’\n",
      "\n",
      "/home/ubuntu/machin 100%[===================>]  20.95M  84.2MB/s    in 0.2s    \n",
      "\n",
      "2020-01-13 13:38:15 (84.2 MB/s) - ‘/home/ubuntu/machine_learning_aws/data/train.tsv’ saved [21972916/21972916]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget -O /home/ubuntu/machine_learning_aws/data/train.tsv \"https://www.dropbox.com/s/10ch2yhfk8tyfri/train.tsv?dl=0\"\n",
    "data = pd.read_table(\"/home/ubuntu/machine_learning_aws/data/train.tsv\", sep= \"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Numerical Features (same as last week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alchemy category, converting to one-hots\n",
    "df = data['alchemy_category']   # 2K ? values\n",
    "one_hots = pd.get_dummies(data['alchemy_category'])\n",
    "df = one_hots\n",
    "rename_dict = {'?': 'alchemy_cat_?'}\n",
    "df = df.rename(columns=rename_dict)\n",
    "\n",
    "# FrameTagRatio, leaving as continuous number\n",
    "df_var = data['frameTagRatio']\n",
    "df['frame_tag_ratio'] = df_var\n",
    "\n",
    "\n",
    "\n",
    "# link word score, 0-100 gaussian, keeping continuous\n",
    "df['link_word_score'] = data['linkwordscore']\n",
    "\n",
    "\n",
    "# alchemy category score, with replacing missing values with random\n",
    "df_var = data['alchemy_category_score']\n",
    "df_var_temp = df_var.apply(lambda x: np.random.random() if x == '?' else float(x)).astype('float32')\n",
    "df['alchemy_category_score'] = df_var_temp\n",
    "\n",
    "\n",
    "# num word in url -- discrete 0-25 to custom binning from looking at the histogram\n",
    "df_var = data['numwords_in_url']\n",
    "bins = [0, 6, 8, 13, 25]\n",
    "df_var_temp = pd.cut(x=df_var, bins=bins, right=True, labels=['num_words_url_bin_0', 'num_words_url_bin_1', 'num_words_url_bin_2', 'num_words_url_bin_3'])\n",
    "dummies = pd.get_dummies(df_var_temp)\n",
    "df = pd.concat([df, dummies], axis=1)\n",
    "\n",
    "\n",
    "# parameterized_link_ratio -- leaving as continuous, right-half gaussian\n",
    "df['parameterized_link_ratio'] = data['parametrizedLinkRatio']\n",
    "\n",
    "# spelling errors ratio -- leaving as continuous\n",
    "df['spelling_errors_ratio'] = data['spelling_errors_ratio']\n",
    "\n",
    "# embed_ratio -- bimodal continuous binned into 2 bins\n",
    "df_var = pd.DataFrame(data['embed_ratio'])\n",
    "df_var = df_var['embed_ratio'].apply(lambda x: 1 if x > -1 else 0)\n",
    "dummies = pd.get_dummies(df_var)\n",
    "rename = {0: 'embed_ratio_0', 1: 'embed_ratio_1'}\n",
    "dummies = dummies.rename(columns=rename)\n",
    "df = pd.concat([df, dummies], axis=1)\n",
    "\n",
    "\n",
    "# html_ratio -- leaving continuous\n",
    "df['html_ratio'] = data['html_ratio']\n",
    "\n",
    "# lengthy_link_domain\n",
    "df_var = pd.get_dummies(data['lengthyLinkDomain'])\n",
    "rename = {0: 'lengthy_link_domain_0', 1: 'lengthy_link_domain_1'}\n",
    "df_var = df_var.rename(columns=rename)\n",
    "df = pd.concat([df, df_var], axis=1)\n",
    "\n",
    "df['labels'] = data['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(df, test_size=0.5, train_size=0.5, random_state=234)\n",
    "val, test = train_test_split(val, test_size=0.5, train_size=0.5, random_state= 675)\n",
    "train_labels = train['labels']\n",
    "train = train.drop(['labels'], axis=1, inplace=False)\n",
    "val_labels = val['labels']\n",
    "val = val.drop(['labels'], axis=1, inplace=False)\n",
    "test_labels = test['labels']\n",
    "test = test.drop(['labels'], axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last Time : logistic regression with numericla features : AUC=0.71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7104607925041362"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(train, train_labels)\n",
    "preds = model.predict_proba(val)[:,1]\n",
    "score = roc_auc_score(val_labels, preds)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 : Textual feature with Count Vectorizer (Bag of Words)\n",
    "\n",
    "- min_df = minimum freuencey cut-off\n",
    "- max_features = take the top 1000 most common feature\n",
    "- strip_accents = to handle non english letters\n",
    "- ngram_range = we are doing bag of word features here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF add in the text features with tfidf\n",
    "unigram_dtm = CountVectorizer(min_df= 10,  max_features= 1000, strip_accents= \"unicode\",\n",
    "                          ngram_range=(1, 1))\n",
    "unigram_dtm.fit(data[\"boilerplate\"])\n",
    "data_text = unigram_dtm.transform(data[\"boilerplate\"])\n",
    "train_text, val_text = train_test_split(data_text, test_size=0.5, train_size=0.5, random_state=234)\n",
    "val_text, test_text = train_test_split(val_text, test_size=0.5, train_size=0.5, random_state= 675)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['uses', 'running', 'came', 'stay', 'spinach', 'cupcakes', 'hit',\n",
       "       'until', 'pan', 'index'], dtype='<U11')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(unigram_dtm.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3697, 1000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 2,  2,  0, ...,  2,  3,  0],\n",
       "       [ 0,  0,  0, ..., 30, 33,  0],\n",
       "       [ 0,  0,  0, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [ 0,  0,  0, ...,  0,  2,  0],\n",
       "       [ 0,  0,  0, ...,  0,  0,  0],\n",
       "       [ 0,  7,  1, ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx = train_text.toarray()\n",
    "print (xx.shape)\n",
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3697, 28)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3697, 1000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alchemy_cat_?</th>\n",
       "      <th>arts_entertainment</th>\n",
       "      <th>business</th>\n",
       "      <th>computer_internet</th>\n",
       "      <th>culture_politics</th>\n",
       "      <th>gaming</th>\n",
       "      <th>health</th>\n",
       "      <th>law_crime</th>\n",
       "      <th>recreation</th>\n",
       "      <th>religion</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1028 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   alchemy_cat_?  arts_entertainment  business  computer_internet  \\\n",
       "0              1                   0         0                  0   \n",
       "1              1                   0         0                  0   \n",
       "2              1                   0         0                  0   \n",
       "3              0                   0         0                  0   \n",
       "4              1                   0         0                  0   \n",
       "\n",
       "   culture_politics  gaming  health  law_crime  recreation  religion ...   \\\n",
       "0                 0       0       0          0           0         0 ...    \n",
       "1                 0       0       0          0           0         0 ...    \n",
       "2                 0       0       0          0           0         0 ...    \n",
       "3                 0       0       0          0           0         0 ...    \n",
       "4                 0       0       0          0           0         0 ...    \n",
       "\n",
       "   990  991  992  993  994  995  996  997  998  999  \n",
       "0    0    0    0    0    0    0    0    2    3    0  \n",
       "1    0    0    0    0    0    1    0   30   33    0  \n",
       "2    0    0    0    0    0    0    0    0    0    0  \n",
       "3    0    0    0    0    1    0    0    7    1    0  \n",
       "4    0    0    0    0    0    0    0    1    0    0  \n",
       "\n",
       "[5 rows x 1028 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_with_text = pd.concat([train.reset_index(drop = True), pd.DataFrame(train_text.toarray())], axis=1)\n",
    "val_with_text = pd.concat([val.reset_index(drop = True), pd.DataFrame(val_text.toarray())], axis=1)\n",
    "train_with_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7942767750375004"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(train_with_text, train_labels)\n",
    "preds = model.predict_proba(val_with_text)[:,1]\n",
    "score = roc_auc_score(val_labels, preds)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Textual Features with Count Vectorizer (Bi-Gram)\n",
    "- ngram_range = (2,2) now so we only bi-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alchemy_cat_?</th>\n",
       "      <th>arts_entertainment</th>\n",
       "      <th>business</th>\n",
       "      <th>computer_internet</th>\n",
       "      <th>culture_politics</th>\n",
       "      <th>gaming</th>\n",
       "      <th>health</th>\n",
       "      <th>law_crime</th>\n",
       "      <th>recreation</th>\n",
       "      <th>religion</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1028 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   alchemy_cat_?  arts_entertainment  business  computer_internet  \\\n",
       "0              1                   0         0                  0   \n",
       "1              1                   0         0                  0   \n",
       "2              1                   0         0                  0   \n",
       "3              0                   0         0                  0   \n",
       "4              1                   0         0                  0   \n",
       "\n",
       "   culture_politics  gaming  health  law_crime  recreation  religion ...   \\\n",
       "0                 0       0       0          0           0         0 ...    \n",
       "1                 0       0       0          0           0         0 ...    \n",
       "2                 0       0       0          0           0         0 ...    \n",
       "3                 0       0       0          0           0         0 ...    \n",
       "4                 0       0       0          0           0         0 ...    \n",
       "\n",
       "   990  991  992  993  994  995  996  997  998  999  \n",
       "0    0    0    0    0    0    0    0    0    0    0  \n",
       "1    0    4    0    0    0    0    0    0    0    0  \n",
       "2    0    0    0    0    0    0    0    0    0    0  \n",
       "3    0    0    0    0    0    0    0    0    0    0  \n",
       "4    0    0    0    0    0    0    0    0    0    0  \n",
       "\n",
       "[5 rows x 1028 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TFIDF add in the text features with tfidf\n",
    "bigram_dtm = CountVectorizer(min_df= 10,  max_features= 1000, strip_accents= \"unicode\",\n",
    "                          ngram_range=(2, 2))\n",
    "bigram_dtm.fit(data[\"boilerplate\"])\n",
    "data_text = bigram_dtm.transform(data[\"boilerplate\"])\n",
    "train_text, val_text = train_test_split(data_text, test_size=0.5, train_size=0.5, random_state=234)\n",
    "val_text, test_text = train_test_split(val_text, test_size=0.5, train_size=0.5, random_state= 675)\n",
    "\n",
    "train_with_text = pd.concat([train.reset_index(drop = True), pd.DataFrame(train_text.toarray())], axis=1)\n",
    "val_with_text = pd.concat([val.reset_index(drop = True), pd.DataFrame(val_text.toarray())], axis=1)\n",
    "train_with_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['few minutes', 'can find', 'the oven', 'used to', 'out the',\n",
       "       'out there', 'during the', 'over the', 'bottom of', 'in his'],\n",
       "      dtype='<U20')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(bigram_dtm.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7734719618841689"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(train_with_text, train_labels)\n",
    "preds = model.predict_proba(val_with_text)[:,1]\n",
    "score = roc_auc_score(val_labels, preds)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Textual Features with tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alchemy_cat_?</th>\n",
       "      <th>arts_entertainment</th>\n",
       "      <th>business</th>\n",
       "      <th>computer_internet</th>\n",
       "      <th>culture_politics</th>\n",
       "      <th>gaming</th>\n",
       "      <th>health</th>\n",
       "      <th>law_crime</th>\n",
       "      <th>recreation</th>\n",
       "      <th>religion</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036841</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006448</td>\n",
       "      <td>0.005204</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025413</td>\n",
       "      <td>0.102643</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.115260</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.073187</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034443</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1028 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   alchemy_cat_?  arts_entertainment  business  computer_internet  \\\n",
       "0              1                   0         0                  0   \n",
       "1              1                   0         0                  0   \n",
       "2              1                   0         0                  0   \n",
       "3              0                   0         0                  0   \n",
       "4              1                   0         0                  0   \n",
       "\n",
       "   culture_politics  gaming  health  law_crime  recreation  religion ...   \\\n",
       "0                 0       0       0          0           0         0 ...    \n",
       "1                 0       0       0          0           0         0 ...    \n",
       "2                 0       0       0          0           0         0 ...    \n",
       "3                 0       0       0          0           0         0 ...    \n",
       "4                 0       0       0          0           0         0 ...    \n",
       "\n",
       "        990       991  992       993  994       995       996  997  998  999  \n",
       "0  0.000000  0.000000  0.0  0.000000  0.0  0.000000  0.036841  0.0  0.0  0.0  \n",
       "1  0.006448  0.005204  0.0  0.000000  0.0  0.025413  0.102643  0.0  0.0  0.0  \n",
       "2  0.000000  0.000000  0.0  0.000000  0.0  0.000000  0.000000  0.0  0.0  0.0  \n",
       "3  0.000000  0.115260  0.0  0.073187  0.0  0.000000  0.034443  0.0  0.0  0.0  \n",
       "4  0.000000  0.000000  0.0  0.000000  0.0  0.000000  0.000000  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 1028 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TFIDF add in the text features with tfidf\n",
    "idf_dtm = TfidfVectorizer(min_df= 10,  max_features= 1000, strip_accents= \"unicode\", ngram_range=(1, 2))\n",
    "idf_dtm.fit(data[\"boilerplate\"])\n",
    "data_text = idf_dtm.transform(data[\"boilerplate\"])\n",
    "train_text, val_text = train_test_split(data_text, test_size=0.5, train_size=0.5, random_state=234)\n",
    "val_text, test_text = train_test_split(val_text, test_size=0.5, train_size=0.5, random_state= 675)\n",
    "\n",
    "train_with_text = pd.concat([train.reset_index(drop = True), pd.DataFrame(train_text.toarray())], axis=1)\n",
    "val_with_text = pd.concat([val.reset_index(drop = True), pd.DataFrame(val_text.toarray())], axis=1)\n",
    "train_with_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['added', 'heart', 'art', 'fact', 'cup', 'pastry', 'cup of', 'five',\n",
       "       'is the', 'favorite'], dtype='<U13')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(idf_dtm.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8622057009938479"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(train_with_text, train_labels)\n",
    "preds = model.predict_proba(val_with_text)[:,1]\n",
    "score = roc_auc_score(val_labels, preds)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
