{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Keras Logo](notebook_diagrams/keras.png)\n",
    "\n",
    "# Keras Tutorial\n",
    "\n",
    "\n",
    "Keras is a high-level neural network/deep learning package that is quite powerful - you can actually create a neural network model and train, test, and evaluate it in under 15 lines of code!  Keras is built on top of [TensorFlow](https://www.tensorflow.org/), a powerful, flexible framework that is used to build machine learning applications at scale.  While we won't study TensorFlow or PyTorch (another powerful neural network/deep learning framework) in this course, you are strongly encouraged to explore these packages as well!\n",
    "\n",
    "Keras, and other machine learning packages, leverage the modularity of object-oriented code to create multi-million parameter neural networks using very few lines of code.\n",
    "\n",
    "This Jupyter notebook uses documentation from Keras to provide a quick, hands-on introduction to this high-level machine learning API.  For reference, we used code from the following resource for the examples below: https://keras.io/.  If you'd like to look at another quick introduction to Keras, I would strongly recommend [30 Seconds to Keras](https://keras.io/#getting-started-30-seconds-to-keras), which is what this code is largely based off of.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If not already installed\n",
    "!pip install keras\n",
    "import keras\n",
    "\n",
    "# OR\n",
    "from tensorflow import keras\n",
    "\n",
    "# Sequential model is very useful in Keras\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Import different layers for defining our networks\n",
    "from keras.layers import Dense\n",
    "\n",
    "# We also want to import an image reader to visualize data\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Data Structure in Keras: The Model\n",
    "The core data structure in Keras is the model class, which has methods for compiling with an optimizer and loss function, fitting (training), evaluation (testing), and prediction.\n",
    "\n",
    "The `model` class is one we will use for essentially all of our operations when we define, train, and test different neural network models.  For more information/documentation on Keras models, visit the link [here](https://keras.io/models/model/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward, Stacked Models from Sequential() Class\n",
    "For this tutorial and course, our focus will be primarily on using the `Sequential` type of model.  Other types of models exist, and we encourage you to investigate them!  \n",
    "\n",
    "The main idea with the `Sequential()` model is that we stack layers sequentially, one after one another.  Each layer we add to our model is cascaded with the rest of the layers in our model, by adding it to the network output.  The result we see after stacking these layers below is something similar to the picture below.\n",
    "\n",
    "![Deep NN](notebook_diagrams/deep_nn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Define a Sequential Keras Model\n",
    "We can define our first model below!  We must first \"instantiate\" our Python model object, which creates the `Sequential` model object and runs the `__init__` (also known as constructor) method, which is run every time a Python object is created.\n",
    "\n",
    "Next, we can add layers one at a time.  `Dense` is a type of neural network layer corresponding to fully connected layers.  We can think of fully connected layers as a set of weights in which each node/neuron in one layer is connected to every node/neuron in the next layer.  See an example in the diagram below.\n",
    "\n",
    "![Fully Connected Layer](notebook_diagrams/fclayer.png)\n",
    "\n",
    "Notice how each node/neuron in the first layer has a weight connecting it to a node/neuron in the second layer.'  With this in mind, let's create a **fully connected neural network** below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create the model\n",
    "model = Sequential()  # Enables for stacking of layers\n",
    "\n",
    "# Now add layers\n",
    "model.add(Dense(units=64, activation='relu', input_dim=100))  # Input/first hidden layer\n",
    "model.add(Dense(units=32, activation='relu'))  # Second layer\n",
    "model.add(Dense(units=16, activation='relu'))  # Third layer\n",
    "model.add(Dense(units=12, activation='relu'))  # Fourth layer\n",
    "model.add(Dense(units=10, activation='softmax'))  # Output layer\n",
    "\n",
    "# Now get information about the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling, Training, and Evaluation\n",
    "Like other supervised learning algorithms, the next machine learning development step will be to train and test our model.  We will use our fully-connected model from above.\n",
    "\n",
    "We will first `compile` our model, which is a function used in Keras that provides our model with a loss function (which is needed for training), an optimizer (such as Stochastic Gradient Descent), and metrics over which to evaluate the model (such as accuracy or mean squared error).  Visit [this page](https://keras.io/models/model/) for more information on how the `compile` function is used.  \n",
    "\n",
    "### Loss Functions We Will Use For Keras:\n",
    "\n",
    "1. **Categorical Cross-Entropy**: Used for multi-class classification.  A network using this loss function should have the **SoftMax** activation function applied at the output.  Can be used by setting: `loss='categorical_crossentropy'`.\n",
    "\n",
    "\n",
    "2. **Binary Cross-Entropy**: Used for binary classification.  A network using this loss function should have the **sigmoid** activation function applied at the output.  Can be used by setting: `loss='categorical_crossentropy'`.  Intuition: When we have linear activation functions in the hidden layers and a **sigmoid** activation at the output, this is just logistic regression!\n",
    "\n",
    "\n",
    "3. **Mean Squared Error (MSE)**: Used for regression.  A network using this loss function should have the **Linear** or **ReLU** activation function applied at the output.  Can be used by setting: `loss='mean_squared_error'`.  Intuition: When we have linear activation functions in the hidden layers and output layer, this is just linear regression!\n",
    "\n",
    "Keras has many other **loss**, **optimizer**, and **metric** options!  We highly encourage you to investigate these on your own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use our model from above!\n",
    "print(model.layers)\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fit model to data (x_train and y_train in this case) - data inputs need only be numpy arrays\n",
    "from keras.datasets import cifar10  # Popular image dataset\n",
    "\n",
    "# Split data into training and testing\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Cifar Data\n",
    "We'll be using the Cifar dataset for this example, which is a famous dataset that is used to train and provide a baseline evaluation for many neural network models in computer vision.\n",
    "\n",
    "Let's look at some of the data we'll be visualizing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "N = len(x_train)\n",
    "\n",
    "# Get random indices for showing data\n",
    "random_indices = np.random.randint(low=0, high=N, size=3)\n",
    "\n",
    "# Show images randomly\n",
    "for index in random_indices:\n",
    "    IMG = x_train[index]\n",
    "    plt.imshow(IMG)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluating Our Model\n",
    "With our data split, we are now ready to train and evaluate our neural network model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we are ready to fit our model!\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32)\n",
    "\n",
    "# Evaluate the model on the training dataset\n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)\n",
    "\n",
    "# Find predictions on the test dataset\n",
    "classes = model.predict(x_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next week, we'll be diving into how our neural network models compare to some of the other models we've been analyzing in this course!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Model Specs\n",
    "We can also get model information using the specs below.  The `model` Python object contains useful information about the neural network, such as its inputs/outputs, parameters, layers, etc.  We can write a function to retrieve all of this information at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_specs(model):\n",
    "    print(model.layers)  # Flattened list of tensors comprising model\n",
    "    print(model.inputs)  # List of input tensors to model\n",
    "    print(model.outputs)  # List of output tensors of model \n",
    "    print(model.summary())  # Brief summary of your Keras model\n",
    "    print(model.get_config())  # Dict containing configuration of model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading Models\n",
    "Often times, we don't want to have to retrain models, and want to re-use them whenever we can.  We can do that by saving, loading, and sharing models!\n",
    "\n",
    "This is especially true for when we try to solve problems in machine learning that have been solved in a similar way before.  For example, if we wanted to train a neural network to classify between mate and coffee using images of these objects, a good place to start would be a general object detector trained on a giant dataset called ImageNet.  \n",
    "\n",
    "We won't have time to discuss it in this course, but if you're interested, training on pre-trained models for specific applications such as classifying between mate and coffee is a technique used in the field of [transfer learning](https://machinelearningmastery.com/transfer-learning-for-deep-learning/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle is also a very useful package for saving models\n",
    "import pickle\n",
    "\n",
    "# Get model information using function above\n",
    "get_model_specs(model)\n",
    "\n",
    "# Now define a path where we can save weights\n",
    "savepath = os.path.join(\"/home\", \"ubuntu\", \"machine_learning_aws\", \"daily_user\", \"ml_package_tutorials\")\n",
    "\n",
    "# Save model to HDF5 file, and then reload it\n",
    "model.save_weights(savepath)\n",
    "model.load_weights(savepath, by_name=False) \n",
    "\n",
    "# Save as JSON representation, and then reload it\n",
    "json_string = model.to_json()\n",
    "json_pickle_fname = os.path.join(savepath, \"json_weights_ex.pkl\")\n",
    "\n",
    "with open(json_pickle_fname, \"wb\") as pkl_file:\n",
    "    pkl_file.dump(json_string)\n",
    "    pkl_file.close()\n",
    "    \n",
    "model_reloaded = model_from_json(json_string)\n",
    "\n",
    "# Save as yaml representation, and then reload it\n",
    "yaml_string = model.to_yaml()\n",
    "model_reloaded = model_from_yaml(yaml_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get and Set Weights\n",
    "We can get information about weights from above, and can also set weights using our own weight files, or, more commonly, using pre-trained weights!  The intuition here is that some person/company/school went through a lot of trouble to find weights that generally work well for solving problems, so we should use them!\n",
    "\n",
    "Typically, you will use `get_weights` when saving a model, and `set_weights` when loading a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_weights()  # Returns weights of model\n",
    "\n",
    "model.set_weights(weights)  # Sets weights of model to be weights arg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Loading Keras Datasets\n",
    "Next week, we will be analyzing the MNIST dataset, which consists of a series of handwritten digits that have labels ranging from 0 to 9.  We will briefly dive into how we can use neural networks/deep learning with Keras to develop a classifier to predict a number given a handwritten digit - a.k.a. how to train a neural network to recognize handwritten digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters for \"Core\" Keras Layers\n",
    "\n",
    "Below are some of the most important parameters that can be utilized when creating the Keras `layer()` object:\n",
    "\n",
    "1. **units**: Positive integer, dimensionality of the output space.\n",
    "2. **activation**: Activation function to use (see activations). If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x).\n",
    "3. **use_bias**: Boolean, whether the layer uses a bias vector.\n",
    "\n",
    "The `layer()` object has other properties as well, but it is only recommended you change these if you have a strong reason to:\n",
    "\n",
    "4. **kernel_initializer**: Initializer for the kernel weights matrix (see initializers).\n",
    "5. **bias_initializer**: Initializer for the bias vector (see initializers).\n",
    "6. **kernel_regularizer**: Regularizer function applied to the kernel weights matrix (see regularizer).\n",
    "7. **bias_regularizer**: Regularizer function applied to the bias vector (see regularizer).\n",
    "8. **activity_regularizer**: Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer).\n",
    "9. **kernel_constraint**: Constraint function applied to the kernel weights matrix (see constraints).\n",
    "10. **bias_constraint**: Constraint function applied to the bias vector (see constraints)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Keras Layers\n",
    "\n",
    "Keras also supports the creation of different types of layers for different neural network applications.  For this course, we will mostly be focused on using **1, 4, 6, and 8**.    \n",
    "\n",
    "\n",
    "1. **Dense**: Fully connected layer.\n",
    "2. **Activation**: Layer for applying an activation function to an output.  \n",
    "3. **Dropout**: Applies a dropout layer.  Probability of dropout is an argument in layer init.\n",
    "4. **Flatten**: Layer typically used for turning 2D/3D into 1D vector (e.g. Conv layers to Dense).\n",
    "5. **Conv1D**: 1D convolutional layer.\n",
    "6. **Conv2D**: 2D convolutional layer.\n",
    "7. **MaxPooling1D**: Layer for max pooling in 1D.\n",
    "8. **MaxPooling2D**: Layer for max pooling in 2D.\n",
    "9. **RNN & GRU**: Base classes for recurrent layers.\n",
    "10. **BatchNormalization**: Batch normalization layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Example - 1D CNN for Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "\n",
    "# set parameters:\n",
    "max_features = 5000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 2\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "# we use max pooling:\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADDITIONAL CONTENT: Modifying the Model() Class for Fully-Customizable Models\n",
    "We can modify the `Model()` class in Keras to create flexible models with our choice of layers, actvation functions, etc.  \n",
    "\n",
    "The code below is more similar to the code we would see when building models in `pytorch` or `tensorflow`, which are machine learning packages that provide the user more flexibility with defining models and training and evaluation procedures, at the price of being somewhat more lower-level.  We will not be using these frameworks in this course, but we strongly encourage you to explore these frameworks in greater detail if you're interested.  Both `pytorch` and `tensorflow` are free and open-source, so you could start using them right now if you wanted to!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "class SimpleMLP(keras.Model):\n",
    "    \n",
    "    # This function is known as the \"constructor\" for class objects\n",
    "    def __init__(self, use_bn=False, use_dp=False, num_classes=10):\n",
    "        \n",
    "        # \"Inherits from the base keras.Model class\"\n",
    "        super(SimpleMLP, self).__init__(name='mlp')\n",
    "        self.use_bn = use_bn\n",
    "        self.use_dp = use_dp\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Define layers of neural network model\n",
    "        self.dense1 = keras.layers.Dense(32, activation='relu')\n",
    "        self.dense2 = keras.layers.Dense(num_classes, activation='softmax')\n",
    "        if self.use_dp:\n",
    "            self.dp = keras.layers.Dropout(0.5)\n",
    "        if self.use_bn:\n",
    "            self.bn = keras.layers.BatchNormalization(axis=-1)\n",
    "    \n",
    "    # Function for passing an input through the neural network\n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        if self.use_dp:\n",
    "            x = self.dp(x)\n",
    "        if self.use_bn:\n",
    "            x = self.bn(x)\n",
    "        return self.dense2(x)\n",
    "\n",
    "# Create a model, compile it with a loss function, optimizer, and metric, and then train it!\n",
    "model = SimpleMLP()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "# x_train and y_train are numpy arrays\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
