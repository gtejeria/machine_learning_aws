{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Training a Classifier in PyTorch\n",
    "\n",
    "![classifier](notebook_diagrams/pytorch_classifier.png)\n",
    "\n",
    "Now that we've worrked through the mechanics of how PyTorch works, we're ready to train our own neural network to classify images!  The reference for this notebook can be found [here](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#training-an-image-classifier).\n",
    "\n",
    "Now you might be thinking...\n",
    "\n",
    "## What about data?\n",
    "\n",
    "Generally, when you have to deal with image, text, audio or video data,\n",
    "you can use standard python packages that load data into a numpy array.\n",
    "Then you can convert this array into a ``torch.*Tensor``.\n",
    "\n",
    "-  For images, packages such as Pillow, OpenCV, and NumPy are useful\n",
    "-  For audio, packages such as scipy and librosa\n",
    "-  For text, either raw Python or Cython based loading, or NLTK and\n",
    "   SpaCy are useful\n",
    "\n",
    "Specifically for vision, PyTorch has created a package called\n",
    "``torchvision``, that has data loaders for common datasets such as\n",
    "Imagenet, CIFAR10, MNIST, etc. and data transformers for images, viz.,\n",
    "``torchvision.datasets`` and ``torch.utils.data.DataLoader``.\n",
    "\n",
    "This provides a huge convenience and avoids writing \"boilerplate\" code.\n",
    "\n",
    "For this tutorial, we will use the CIFAR10 dataset.\n",
    "It has ten classes: \n",
    "\n",
    "* ’airplane’\n",
    "* ‘automobile’\n",
    "* ‘bird’\n",
    "* ‘cat’\n",
    "* ‘deer’\n",
    "* ‘dog’\n",
    "* ‘frog’\n",
    "* ‘horse’\n",
    "* ‘ship’\n",
    "* ‘truck’\n",
    "\n",
    "The images in CIFAR-10 are of size 3x32x32, i.e. 3-channel color images of 32x32 pixels in size.  Example images from this dataset are shown below:\n",
    "\n",
    "![CIFAR](notebook_diagrams/cifar10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview: How will We Train this Classifier?\n",
    "\n",
    "To train our image classifier, we will do the following steps in order:\n",
    "\n",
    "1. Load and normalizing the CIFAR10 training and test datasets using torchvision\n",
    "\n",
    "2. Define a Convolutional Neural Network\n",
    "\n",
    "3. Define a loss function\n",
    "\n",
    "4. Train the network on the training data\n",
    "\n",
    "5. Test the network on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Installation Block\n",
    "Let's import what we need from pytorch and torchvision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use for formatting plots\n",
    "%matplotlib inline\n",
    "\n",
    "# Import pytorch package and modules for vision\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Import other packages for numerical computation and plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Image Data Pre-Processing\n",
    "Like other problems we've analyzed in computer vision, it's really important that we pre-process our image data in a way that results in optimal and efficient training for our neural network.\n",
    "\n",
    "The output of torchvision datasets are `PILImage` image objects, with values ranging from `[0, 1]`.  For our network, we will standardize these tensors to be in the normalized range `[-1, 1]` using the `torchvision.transforms` module..\n",
    "\n",
    "**NOTE**: If you are running on Windows and you get a BrokenPipeError, try setting the `num_worker` of `torch.utils.data.DataLoader()` to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Create Normalized Training and Testing Datasets and Dataloaders\n",
    "We will use the transform framework described above, as well as the PyTorch [DataLoader](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html) object, which is an efficient way to create customizable and augmented datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our PyTorch transform - often these numbers are standardized and easy to find online\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# Use torchvision to extract training dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "# DataLoaders are another important part of PyTorch - they are similar to the generators we saw with Keras\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=0)\n",
    "\n",
    "# Use torchvision to extract test dataset\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "# Define DataLoader for test data\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=0)\n",
    "\n",
    "# Define labels for CIFAR-10\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Visualize Training Data\n",
    "Depending on your vision application, displaying your data may be helpful to gain a better intuition for what models to use/why a network will/not work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for showing un-normalized image\n",
    "def imshow(img):\n",
    "    # Un-normalize the iamge\n",
    "    img = img / 2 + 0.5     \n",
    "    \n",
    "    # Convert to numpy\n",
    "    npimg = img.numpy()\n",
    "    \n",
    "    # Plot image\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Our Model: CNN\n",
    "For this image classifier, we'll be using a Convolutional Neural Network (CNN) to classify our images, since again these models preserve local structure in an image better than standard fully connected networks.  Recall with defining neural network models in PyTorch that we need to define two functions/class methods:\n",
    "\n",
    "1.  `__init__` (called the constructor) - Make sure this inherits from the `nn.Module` class by calling `super(Net, self).__init__()`.\n",
    "\n",
    "\n",
    "2.  `forward` - This is the method that tells the neural network how it will make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch-specific imports\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Class definition for our neural network\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    # Constructor method\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Class inherits from nn.Module\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Now add custom layers - start with convolution layers\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        \n",
    "        # Now add fully connected layers\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "    \n",
    "    # Forward method for prediction\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Make an instance of our classifier network\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define a Loss Function and Optimizer\n",
    "Remember that in order to train a model, the model needs to have a way for its performance to be evaluated (a **loss function**), as well as a way that tells the computer how we'll train and update the parameters in our model (an **optimizer**).\n",
    "\n",
    "Since this is a classification problem, let's use **Cross-Entropy** for our loss function and **ADAM** for our optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the PyTorch optimizer module\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Our Image Classification Network\n",
    "We'll combine our `DataLoaders`, `Net`, `criterion`, and `optimizer` objects above to train our neural network to classify objects in the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Training Loop for Our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define how many epochs we train over\n",
    "EPOCHS = 2\n",
    "\n",
    "# Loop over the dataset multiple times\n",
    "for epoch in range(EPOCHS): \n",
    "    \n",
    "    # Keep track of loss\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # Iterate over trainloader\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        \n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Save Our Trained Model\n",
    "Let's quickly save our trained model.  When training over extended periods of time, it is often advisable to train in a `tmux` or `screen` session (easy to install if using an Ubuntu/Linux-based AWS instance), and to save your model after every couple of epochs.\n",
    "\n",
    "For more information on how to save models, see the link [here](https://pytorch.org/docs/stable/notes/serialization.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save fpath\n",
    "PATH = './cifar_net.pth'\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate and Test the Network\n",
    "We have trained the network for 2 passes over the training dataset.\n",
    "But we need to check if the network has learned anything at all.\n",
    "\n",
    "We will check this by predicting the class label that the neural network\n",
    "outputs, and checking it against the ground-truth. If the prediction is\n",
    "correct, we add the sample to the list of correct predictions.\n",
    "\n",
    "Okay, first step. Let us display an image from the test set to get familiar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Analyze Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define iterator object\n",
    "dataiter = iter(testloader)\n",
    "\n",
    "# Get images and labels from iterator object\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Load Saved Model for Making Predictions\n",
    "Next, let's load back in our saved model (note: saving and re-loading the model\n",
    "wasn't necessary here, we only did it to illustrate how to do so):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new neural network\n",
    "net = Net()\n",
    "\n",
    "# Load saved weights\n",
    "net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Make Predictions for Neural Network\n",
    "Okay, now let us see what the neural network thinks these examples above are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on all images from iterator\n",
    "outputs = net(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputs are energies/logits (depending on your interpretation; technically both of these are correct) for the 10 classes.  The higher the energy for a class, the more the network thinks that the image is of the particular class.  So, let's get the index of the highest energy, which is the class the network thinks is the most likely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get highest-predicted classes\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "# Show these\n",
    "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
    "                              for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Make Predictions on Whole Dataset\n",
    "Now that we've evaluated performance on a subset of the test dataset, let's look at how our models does with the entire thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set counters\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Use the line below for \"evaluation\"\n",
    "with torch.no_grad():\n",
    "    \n",
    "    # Iterate over test data\n",
    "    for data in testloader:\n",
    "        \n",
    "        # Get data and make predictions\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        \n",
    "        # Hard assignment of the predicted class\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        # Updated counters\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks way better than chance, (in this case, chance is 10% accuracy (randomly picking\n",
    "a class out of 10 classes)).  Our network did a pretty good job of learning!\n",
    "\n",
    "Another important aspect of evaluation is looking specifically at classes the neural network did a good job classifying, as well as classes the neural network did not do so well on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counters for specific classes\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "# Make sure we aren't making weight updates\n",
    "with torch.no_grad():\n",
    "    \n",
    "    # Iterate over test data\n",
    "    for data in testloader:\n",
    "        \n",
    "        # Get data and make predictions\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        \n",
    "        # Hard assignment of class\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        \n",
    "        # Update class-specific counters\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "# Display accuracies for all ten labels\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exercise: Try Increasing the Overall Accuracy of This Network!\n",
    "You have many options for how you can improve the overall performance of this network.  These include, but are not limited to:\n",
    "\n",
    "- Changing the architecture of the network\n",
    "- Changing the learning rate\n",
    "- Changing the optimizer\n",
    "- Changing how long we train for (epochs)\n",
    "\n",
    "Try experimenting with these to see if you can improve classification performance.  The code from above has been copy/pasted below - please make your desired modifications **below**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Your Network Architecture\n",
    "Make any changes to your neural network architecture below.  These modifications could be:\n",
    "\n",
    "- Changing the number, types, or size of different layers (in the `__init__` method).\n",
    "- Changing how the input is mapped to the output (in the `forward` method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch-specific imports\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Class definition for our neural network\n",
    "class BetterNet(nn.Module):\n",
    "    \n",
    "    # Constructor method\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Class inherits from nn.Module\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Now add custom layers - start with convolution layers\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        \n",
    "        # Now add fully connected layers\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "    \n",
    "    # Forward method for prediction\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Make an instance of our classifier network\n",
    "net = BetterNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Define Your Optimizer, Learning Rate, and Epochs\n",
    "Modify these training parameters below to adjust how your network will be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# DON'T CHANGE THIS\n",
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# DON'T CHANGE THIS\n",
    "####################################\n",
    "\n",
    "# Define learning rate and optimizer\n",
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "# Define number of epochs\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Train and Evaluate on Your Modified Network\n",
    "Now that we've changed your network and parameters for training it, let's train and evaluate it to see if our performance improves!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE FOR TRAINING\n",
    "for epoch in range(epochs): \n",
    "    \n",
    "    # Keep track of loss\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # Iterate over trainloader\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        \n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "            \n",
    "print('Finished Training')\n",
    "\n",
    "# CODE FOR EVALUATION\n",
    "# Set counters\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Use the line below for \"evaluation\"\n",
    "with torch.no_grad():\n",
    "    \n",
    "    # Iterate over test data\n",
    "    for data in testloader:\n",
    "        \n",
    "        # Get data and make predictions\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        \n",
    "        # Hard assignment of the predicted class\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        # Updated counters\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))\n",
    "\n",
    "# CODE FOR CLASS-SPECIFIC EVALUATION\n",
    "# Counters for specific classes\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "\n",
    "# Make sure we aren't making weight updates\n",
    "with torch.no_grad():\n",
    "    \n",
    "    # Iterate over test data\n",
    "    for data in testloader:\n",
    "        \n",
    "        # Get data and make predictions\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        \n",
    "        # Hard assignment of class\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        \n",
    "        # Update class-specific counters\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "# Display accuracies for all ten labels\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did your network do?  Can you think why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. (Optional) Training on GPU\n",
    "\n",
    "Just like how you transfer a Tensor onto the GPU, you transfer the neural\n",
    "net onto the GPU.\n",
    "\n",
    "Let's first define our device as the first visible cuda device if we have\n",
    "CUDA available:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there is CUDA available, and if so display CUDA-enabled device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "print(\"CUDA device is: %s\" % (device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. (Optional) Training on multiple GPUs\n",
    "If you want to see even more MASSIVE speedup using all of your GPUs,\n",
    "please check out [data_parallel_tutorial](https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
