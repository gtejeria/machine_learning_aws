{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification: End-to-end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle StumbleUpon Competition\n",
    "==================\n",
    "https://www.kaggle.com/c/stumbleupon\n",
    "\n",
    "** Competition **: \n",
    "1. Some web pages, such as news articles or seasonal recipes, are only relevant for a short period of time. Others continue to be important for a long time.\n",
    "2. The goal is to identify pages which pages will be relevant for a short span of time, and which will be relevant for a long span on time and are thus considered \"evergreen\". \n",
    "\n",
    "** Evaluation **: Area under the curve (AUC) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Python Modules \n",
    "================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick hack to fix import path\n",
    "# import sys; sys.path.append('/Users/julianalverio/code/conda/envs/sac/lib/python3.6/site-packages/')\n",
    "\n",
    "# data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# plots\n",
    "%matplotlib inline\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "\n",
    "# classification algorithms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "# dimensionality reduction\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# cross-validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection\n",
    "\n",
    "\n",
    "# model evaluation\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "os.chdir(os.path.join(\"..\", \"data\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_table(\"stumbleupon/train.tsv\", sep= \"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable descriptions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **url**: Url of the webpage to be classified\n",
    "2. **urlid**: StumbleUpon's unique identifier for each url\n",
    "3. **boilerplate**: Boilerplate text\n",
    "4. **alchemy_category**:\tAlchemy category\n",
    "5. **alchemy_category_score**:\tAlchemy category score\n",
    "6. **avglinksize**:\tAverage number of words in each link\n",
    "7. **commonLinkRatio_1**:\t# of links sharing at least 1 word with 1 other links / # of links\n",
    "8. **commonLinkRatio_2**:\t# of links sharing at least 1 word with 2 other links / # of links\n",
    "9. **commonLinkRatio_3**:\t# of links sharing at least 1 word with 3 other links / # of links\n",
    "10. **commonLinkRatio_4**:\t# of links sharing at least 1 word with 4 other links / # of links\n",
    "11. **compression_ratio**:\tCompression achieved on this page via gzip (measure of redundancy)\n",
    "12. **embed_ratio**: Count of number of \"embed\" usage\n",
    "13. **frameBased**: A page is frame-based (1) if it has no body markup but have a frameset markup\n",
    "14. **frameTagRatio**: Ratio of iframe markups over total number of markups\n",
    "15. **hasDomainLink**:\tTrue (1) if it contains an \"a\" with an url with domain\n",
    "16. **html_ratio**:\tRatio of tags vs text in the page\n",
    "17. **image_ratio**: Ratio of \"img\" tags vs text in the page\n",
    "18. **is_news**: True (1) if StumbleUpon's news classifier determines that this webpage is news\n",
    "19. **lengthyLinkDomain**: True (1) if at least 3 \"a\"'s text contains more than 30 alphanumeric characters\n",
    "20. **linkwordscore**: Percentage of words on the page that are in hyperlink's text\n",
    "21. **news_front_page**: True (1) if StumbleUpon's news classifier determines that this webpage is front-page news\n",
    "22. **non_markup_alphanum_characters**:\tinteger\tPage's text's number of alphanumeric characters\n",
    "23. **numberOfLinks**: Number of \"a\"  markups\n",
    "24. **numwords_in_url**: Number of words in url\n",
    "25. **parametrizedLinkRatio**: A link is parametrized if it's url contains parameters  or has an attached onClick event\n",
    "26. **spelling_errors_ratio**: Ratio of words not found in wiki (considered to be a spelling mistake)\n",
    "27. **label**: User-determined label. Either evergreen (1) or non-evergreen (0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "No Good\n",
    "url -- object type, the literal url\n",
    "urlid -- uid for the url, drop it\n",
    "avglinksize -- huge spread, mostly around 1 or two. This would need to be hella binned carefully. I think useless from Madhav's results\n",
    "frameBased -- 0 everywhere, garbage\n",
    "is_news -- 1 everywhere\n",
    "\n",
    "TEXT\n",
    "boilerplate -- text feature\n",
    "\n",
    "NUMERICAL\n",
    "alchemy_category -- business, health, sports, etc.\n",
    "alchemy_category_score -- 0 to 1 STRING with LOTS of '?' values\n",
    "commonlinkratio -- cont value from 0 to 1 that works nicely, needs binning. Nice gaussian on 1, moves to left with increasing idx\n",
    "compression_ratio -- continuous value with huge bimodal spread -- needs to be binned carefully\n",
    "embed_ratio -- perfect bimodal spread -- two humps. -1 and 0.25\n",
    "frameTagRatio -- 0 to 0.44 cont, spread leans left -- it is half a gaussian\n",
    "hasDomainLink -- 0/1 categorical, almost all 0\n",
    "html_ratio -- cont 0 to 0.7, gaussian spread\n",
    "image_ratio -- spread -1 to 113, but almost exclusively right at 0\n",
    "lengthy-link domain -- categorical 0/1, good spread\n",
    "linkwordscore -- 0 to 100 good spread\n",
    "news_front_page -- STRING, ? values, 0-1 categorical, almost all 0\n",
    "non_markup_alphnum_characters -- huge spread, highly concentrated near 0\n",
    "numberoflinks -- huge spread, mostly close to 0\n",
    "numwordsinurl -- good spread, 0 to 25, has a long upper tail\n",
    "Parametrizedlinkratio -- 0 to 1 continuous, skewed toward 0 -- right half of gaussian\n",
    "spelling_errors_ratio -- 0 to 1, heavy leaning to 0\n",
    "\n",
    "\n",
    "Entropy measures on each feature\n",
    "\n",
    "[(-0.0017029098026004608, 'avglinksize'),\n",
    " (-0.0015477640384510272, 'image_ratio'),\n",
    " (-0.0013366491854696072, 'numberOfLinks'),\n",
    " (-0.0012422148274934264, 'urlid'),\n",
    " (-0.0012276386004722584, 'non_markup_alphanum_characters'),\n",
    " (-0.0003632131231506852, 'hasDomainLink'),\n",
    " (-1.6445368175466157e-05, 'compression_ratio'),\n",
    " (0, 'alchemy_category_score'),\n",
    " (0, 'boilerplate'),\n",
    " (0, 'commonLinkRatio_1'),\n",
    " (0, 'commonLinkRatio_2'),\n",
    " (0, 'commonLinkRatio_3'),\n",
    " (0, 'commonLinkRatio_4'),\n",
    " (0, 'frameBased'),\n",
    " (0, 'is_news'),\n",
    " (0, 'label'),\n",
    " (0, 'news_front_page'),\n",
    " (0, 'url'),\n",
    " (0.00016331922818457745, 'lengthyLinkDomain'),\n",
    " (0.0007063846400122697, 'html_ratio'),\n",
    " (0.0011190978216005787, 'embed_ratio'),\n",
    " (0.0020862547468744053, 'spelling_errors_ratio'),\n",
    " (0.002983052198938685, 'parametrizedLinkRatio'),\n",
    " (0.004883582510772921, 'numwords_in_url'),\n",
    " (0.016376170421172676, 'linkwordscore'),\n",
    " (0.016942353001673127, 'frameTagRatio'),\n",
    " (0.040352206434091764, 'alchemy_category')]\n",
    "\n",
    "'''\n",
    "\n",
    "# Alchemy category, converting to one-hots\n",
    "df = data['alchemy_category']   # 2K ? values\n",
    "one_hots = pd.get_dummies(data['alchemy_category'])\n",
    "df = one_hots\n",
    "rename_dict = {'?': 'alchemy_cat_?'}\n",
    "df = df.rename(columns=rename_dict)\n",
    "\n",
    "# FrameTagRatio, leaving as continuous number\n",
    "df_var = data['frameTagRatio']\n",
    "df['frame_tag_ratio'] = df_var\n",
    "\n",
    "\n",
    "\n",
    "# link word score, 0-100 gaussian, keeping continuous\n",
    "df['link_word_score'] = data['linkwordscore']\n",
    "\n",
    "\n",
    "# alchemy category score, with replacing missing values with random\n",
    "df_var = data['alchemy_category_score']\n",
    "df_var_temp = df_var.apply(lambda x: np.random.random() if x == '?' else float(x)).astype('float32')\n",
    "df['alchemy_category_score'] = df_var_temp\n",
    "\n",
    "\n",
    "# num word in url -- discrete 0-25 to custom binning from looking at the histogram\n",
    "df_var = data['numwords_in_url']\n",
    "bins = [0, 6, 8, 13, 25]\n",
    "df_var_temp = pd.cut(x=df_var, bins=bins, right=True, labels=['num_words_url_bin_0', 'num_words_url_bin_1', 'num_words_url_bin_2', 'num_words_url_bin_3'])\n",
    "dummies = pd.get_dummies(df_var_temp)\n",
    "df = pd.concat([df, dummies], axis=1)\n",
    "\n",
    "\n",
    "# parameterized_link_ratio -- leaving as continuous, right-half gaussian\n",
    "df['parameterized_link_ratio'] = data['parametrizedLinkRatio']\n",
    "\n",
    "# spelling errors ratio -- leaving as continuous\n",
    "df['spelling_errors_ratio'] = data['spelling_errors_ratio']\n",
    "\n",
    "# embed_ratio -- bimodal continuous binned into 2 bins\n",
    "df_var = pd.DataFrame(data['embed_ratio'])\n",
    "df_var = df_var['embed_ratio'].apply(lambda x: 1 if x > -1 else 0)\n",
    "dummies = pd.get_dummies(df_var)\n",
    "rename = {0: 'embed_ratio_0', 1: 'embed_ratio_1'}\n",
    "dummies = dummies.rename(columns=rename)\n",
    "df = pd.concat([df, dummies], axis=1)\n",
    "\n",
    "\n",
    "# html_ratio -- leaving continuous\n",
    "df['html_ratio'] = data['html_ratio']\n",
    "\n",
    "# lengthy_link_domain\n",
    "df_var = pd.get_dummies(data['lengthyLinkDomain'])\n",
    "rename = {0: 'lengthy_link_domain_0', 1: 'lengthy_link_domain_1'}\n",
    "df_var = df_var.rename(columns=rename)\n",
    "df = pd.concat([df, df_var], axis=1)\n",
    "\n",
    "df['labels'] = data['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = train_test_split(df, test_size=0.5, train_size=0.5, random_state=234)\n",
    "val, test = train_test_split(val, test_size=0.5, train_size=0.5, random_state= 675)\n",
    "train_labels = train['labels']\n",
    "train = train.drop(['labels'], axis=1, inplace=False)\n",
    "val_labels = val['labels']\n",
    "val = val.drop(['labels'], axis=1, inplace=False)\n",
    "test_labels = test['labels']\n",
    "test = test.drop(['labels'], axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_search(budget):\n",
    "    ret = []\n",
    "    for i in range(budget):\n",
    "        seed = random.randrange(500)\n",
    "        rand_C =    random.choice([0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000])\n",
    "        rand_fit_intercept = random.choice([True, False])\n",
    "        rand_penalty = random.choice(['l1', 'l2'])\n",
    "        model = LogisticRegression(random_state=seed, C = rand_C, fit_intercept = rand_fit_intercept, penalty=rand_penalty)\n",
    "        model.fit(train, train_labels)\n",
    "        preds = model.predict_proba(val)[:,1]\n",
    "        score = roc_auc_score(val_labels, preds)\n",
    "        ret.append((score, model))\n",
    "    return list(reversed(sorted(ret, key=lambda x: x[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7186933252833297\n"
     ]
    }
   ],
   "source": [
    "# Best logistic regression: AUC=0.71\n",
    "seed = 61\n",
    "C = 1\n",
    "fit_intercept = False\n",
    "penalty = 'l1'\n",
    "model = LogisticRegression(random_state=seed, C=C, fit_intercept=fit_intercept, penalty=penalty)\n",
    "model.fit(train, train_labels)\n",
    "preds = model.predict_proba(val)[:,1]\n",
    "score = roc_auc_score(val_labels, preds)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.7187015156939969,\n",
       "  LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=False,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l1', random_state=475, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7186816246966623,\n",
       "  LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=False,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l1', random_state=386, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7186769444619953,\n",
       "  LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=False,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l1', random_state=84, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7186746043446618,\n",
       "  LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=False,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l1', random_state=110, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7186687540513281,\n",
       "  LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=False,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l1', random_state=350, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7186008906486571,\n",
       "  LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l1', random_state=230, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.718588020003323,\n",
       "  LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l1', random_state=37, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7176250617205947,\n",
       "  LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=False,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l1', random_state=67, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7176238916619277,\n",
       "  LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l1', random_state=189, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7176133611339273,\n",
       "  LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l1', random_state=122, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7176098509579271,\n",
       "  LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=False,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l1', random_state=493, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.717609850957927,\n",
       "  LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=False,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l1', random_state=18, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.717609850957927,\n",
       "  LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=False,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l1', random_state=235, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7175337971445888,\n",
       "  LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l1', random_state=172, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7175150762059209,\n",
       "  LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l1', random_state=400, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7175139061472542,\n",
       "  LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=False,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l1', random_state=61, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7175127360885873,\n",
       "  LogisticRegression(C=10000, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l2', random_state=92, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.717510395971254, LogisticRegression(C=10000, class_weight=None, dual=False,\n",
       "            fit_intercept=False, intercept_scaling=1, max_iter=100,\n",
       "            multi_class='warn', n_jobs=None, penalty='l1', random_state=173,\n",
       "            solver='warn', tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7175092259125874,\n",
       "  LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=False,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l1', random_state=282, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7175092259125873,\n",
       "  LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=False,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l1', random_state=171, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7175092259125873,\n",
       "  LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=False,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l1', random_state=226, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7175092259125873,\n",
       "  LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l2', random_state=265, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7175092259125873,\n",
       "  LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l2', random_state=180, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7175092259125871,\n",
       "  LogisticRegression(C=10000, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l2', random_state=323, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7175080558539204,\n",
       "  LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l2', random_state=364, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7175080558539204,\n",
       "  LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l2', random_state=344, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7175080558539204,\n",
       "  LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=False,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l2', random_state=99, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7175080558539204,\n",
       "  LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=False,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l1', random_state=385, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7175068857952538,\n",
       "  LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=False,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l1', random_state=449, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7175068857952537,\n",
       "  LogisticRegression(C=10000, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l2', random_state=245, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7175068857952537,\n",
       "  LogisticRegression(C=10000, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l2', random_state=385, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7175045456779203,\n",
       "  LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l1', random_state=350, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7175045456779203,\n",
       "  LogisticRegression(C=10000, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l1', random_state=207, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7175045456779202,\n",
       "  LogisticRegression(C=10000, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l2', random_state=337, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7175033756192536,\n",
       "  LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=False,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l1', random_state=290, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7175022055605869,\n",
       "  LogisticRegression(C=1000, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l1', random_state=72, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7175022055605867,\n",
       "  LogisticRegression(C=10000, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l2', random_state=211, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7175010355019201,\n",
       "  LogisticRegression(C=10000, class_weight=None, dual=False,\n",
       "            fit_intercept=False, intercept_scaling=1, max_iter=100,\n",
       "            multi_class='warn', n_jobs=None, penalty='l2', random_state=240,\n",
       "            solver='warn', tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.71750103550192,\n",
       "  LogisticRegression(C=10000, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l1', random_state=135, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7174998654432533,\n",
       "  LogisticRegression(C=10000, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l1', random_state=91, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7174975253259198,\n",
       "  LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=False,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l2', random_state=357, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7174916750325862,\n",
       "  LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=False,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l2', random_state=463, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7174916750325862,\n",
       "  LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=False,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l2', random_state=419, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7174905049739193,\n",
       "  LogisticRegression(C=10000, class_weight=None, dual=False,\n",
       "            fit_intercept=False, intercept_scaling=1, max_iter=100,\n",
       "            multi_class='warn', n_jobs=None, penalty='l1', random_state=399,\n",
       "            solver='warn', tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7174869947979192,\n",
       "  LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l2', random_state=102, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7174869947979192,\n",
       "  LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l2', random_state=37, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7174869947979192,\n",
       "  LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l2', random_state=348, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7174823145632522,\n",
       "  LogisticRegression(C=10000, class_weight=None, dual=False,\n",
       "            fit_intercept=False, intercept_scaling=1, max_iter=100,\n",
       "            multi_class='warn', n_jobs=None, penalty='l2', random_state=274,\n",
       "            solver='warn', tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7174823145632522,\n",
       "  LogisticRegression(C=10000, class_weight=None, dual=False,\n",
       "            fit_intercept=False, intercept_scaling=1, max_iter=100,\n",
       "            multi_class='warn', n_jobs=None, penalty='l2', random_state=90,\n",
       "            solver='warn', tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7174823145632522,\n",
       "  LogisticRegression(C=10000, class_weight=None, dual=False,\n",
       "            fit_intercept=False, intercept_scaling=1, max_iter=100,\n",
       "            multi_class='warn', n_jobs=None, penalty='l2', random_state=307,\n",
       "            solver='warn', tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.717270533944572,\n",
       "  LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=False,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l2', random_state=87, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.717270533944572,\n",
       "  LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=False,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l2', random_state=361, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7172564932405711,\n",
       "  LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l2', random_state=285, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7172553231819043,\n",
       "  LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l2', random_state=449, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7172529830645707,\n",
       "  LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l2', random_state=219, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7172447926539036,\n",
       "  LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=False,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l2', random_state=491, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7172401124192367,\n",
       "  LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=False,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l2', random_state=400, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7172401124192367,\n",
       "  LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=False,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l2', random_state=215, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7135380467976665,\n",
       "  LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l2', random_state=493, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7135380467976665,\n",
       "  LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l2', random_state=451, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7135380467976664,\n",
       "  LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=False,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l2', random_state=291, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7135380467976664,\n",
       "  LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l2', random_state=324, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7050141694104543,\n",
       "  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=False,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l2', random_state=358, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7050141694104543,\n",
       "  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=False,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l2', random_state=247, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7048772725464455,\n",
       "  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l2', random_state=228, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7037434856983731,\n",
       "  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=False,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l1', random_state=49, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.703742315639706,\n",
       "  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=False,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l1', random_state=467, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7037364653463725,\n",
       "  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=False,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l1', random_state=173, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7037341252290389,\n",
       "  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=False,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l1', random_state=499, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7037294449943721,\n",
       "  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l1', random_state=366, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7037282749357053,\n",
       "  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l1', random_state=463, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.7037259348183718,\n",
       "  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=False,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l1', random_state=379, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.703724764759705,\n",
       "  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=False,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l1', random_state=243, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.703724764759705,\n",
       "  LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=False,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l1', random_state=188, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.6983705763006958,\n",
       "  LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=False,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l2', random_state=158, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.6983705763006958,\n",
       "  LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=False,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l2', random_state=48, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.6983705763006958,\n",
       "  LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=False,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l2', random_state=321, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.6983705763006958,\n",
       "  LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=False,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l2', random_state=147, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.6983705763006958,\n",
       "  LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=False,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l2', random_state=448, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.6973783665512989,\n",
       "  LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l2', random_state=470, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.6973783665512989,\n",
       "  LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l2', random_state=244, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.6973783665512989,\n",
       "  LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l2', random_state=410, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.6973783665512989,\n",
       "  LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l2', random_state=475, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.6644423851411909,\n",
       "  LogisticRegression(C=0.001, class_weight=None, dual=False,\n",
       "            fit_intercept=False, intercept_scaling=1, max_iter=100,\n",
       "            multi_class='warn', n_jobs=None, penalty='l2', random_state=159,\n",
       "            solver='warn', tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.6602687858769238,\n",
       "  LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l2', random_state=489, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.6572611500740647,\n",
       "  LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=False,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l1', random_state=131, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.6571663753220587,\n",
       "  LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=False,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l1', random_state=166, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.6564924215300155,\n",
       "  LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l1', random_state=64, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.6564924215300155,\n",
       "  LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l1', random_state=196, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.6564924215300155,\n",
       "  LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l1', random_state=65, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.6267840469521141,\n",
       "  LogisticRegression(C=0.0001, class_weight=None, dual=False,\n",
       "            fit_intercept=False, intercept_scaling=1, max_iter=100,\n",
       "            multi_class='warn', n_jobs=None, penalty='l2', random_state=393,\n",
       "            solver='warn', tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.6267840469521141,\n",
       "  LogisticRegression(C=0.0001, class_weight=None, dual=False,\n",
       "            fit_intercept=False, intercept_scaling=1, max_iter=100,\n",
       "            multi_class='warn', n_jobs=None, penalty='l2', random_state=281,\n",
       "            solver='warn', tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.6267840469521141,\n",
       "  LogisticRegression(C=0.0001, class_weight=None, dual=False,\n",
       "            fit_intercept=False, intercept_scaling=1, max_iter=100,\n",
       "            multi_class='warn', n_jobs=None, penalty='l2', random_state=96,\n",
       "            solver='warn', tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.625371786141357,\n",
       "  LogisticRegression(C=0.0001, class_weight=None, dual=False,\n",
       "            fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "            multi_class='warn', n_jobs=None, penalty='l2', random_state=202,\n",
       "            solver='warn', tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.625371786141357,\n",
       "  LogisticRegression(C=0.0001, class_weight=None, dual=False,\n",
       "            fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "            multi_class='warn', n_jobs=None, penalty='l2', random_state=449,\n",
       "            solver='warn', tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.6047641278733716,\n",
       "  LogisticRegression(C=0.001, class_weight=None, dual=False,\n",
       "            fit_intercept=False, intercept_scaling=1, max_iter=100,\n",
       "            multi_class='warn', n_jobs=None, penalty='l1', random_state=113,\n",
       "            solver='warn', tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.6047641278733716,\n",
       "  LogisticRegression(C=0.001, class_weight=None, dual=False,\n",
       "            fit_intercept=False, intercept_scaling=1, max_iter=100,\n",
       "            multi_class='warn', n_jobs=None, penalty='l1', random_state=352,\n",
       "            solver='warn', tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.6047641278733716,\n",
       "  LogisticRegression(C=0.001, class_weight=None, dual=False,\n",
       "            fit_intercept=False, intercept_scaling=1, max_iter=100,\n",
       "            multi_class='warn', n_jobs=None, penalty='l1', random_state=31,\n",
       "            solver='warn', tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.6047641278733716,\n",
       "  LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "            n_jobs=None, penalty='l1', random_state=379, solver='warn',\n",
       "            tol=0.0001, verbose=0, warm_start=False)),\n",
       " (0.5, LogisticRegression(C=0.0001, class_weight=None, dual=False,\n",
       "            fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "            multi_class='warn', n_jobs=None, penalty='l1', random_state=292,\n",
       "            solver='warn', tol=0.0001, verbose=0, warm_start=False))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_search(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbdt_search(budget):\n",
    "    ret = []\n",
    "    for i in range(budget):\n",
    "        # choose a set of random hyper-parameters\n",
    "        seed = random.randrange(500)\n",
    "        ccp_alpha = random.choice([0.0, 0.1])\n",
    "        learning_rate = random.choice([0.001, 0.01, 0.1])\n",
    "        n_estimators = random.choice([10, 30, 80, 160, 300])\n",
    "        max_depth = random.choice([1,3,5,10])\n",
    "        model = GradientBoostingClassifier(random_state=seed, learning_rate=learning_rate, n_estimators=n_estimators, max_depth = max_depth)\n",
    "        model.fit(train, train_labels)\n",
    "        preds = model.predict_proba(val)[:,1]\n",
    "        score = roc_auc_score(val_labels, preds)\n",
    "        ret.append((score, model))\n",
    "    return list(reversed(sorted(ret, key=lambda x: x[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7345142735456758\n"
     ]
    }
   ],
   "source": [
    "# Best Gradient Boosted Decision Tree: AUC=0.73\n",
    "learning_rate = 0.01\n",
    "max_depth = 3\n",
    "n_estimators = 300\n",
    "random_state = 239\n",
    "model = GradientBoostingClassifier(random_state=seed, learning_rate=learning_rate, n_estimators=n_estimators, max_depth = max_depth)\n",
    "model.fit(train, train_labels)\n",
    "preds = model.predict_proba(val)[:,1]\n",
    "score = roc_auc_score(val_labels, preds)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.7345142735456758,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.01, loss='deviance', max_depth=3,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=300,\n",
       "                n_iter_no_change=None, presort='auto', random_state=211,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.7345142735456758,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.01, loss='deviance', max_depth=3,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=300,\n",
       "                n_iter_no_change=None, presort='auto', random_state=366,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.7345142735456758,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.01, loss='deviance', max_depth=3,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=300,\n",
       "                n_iter_no_change=None, presort='auto', random_state=182,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.7340521003723127,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=30,\n",
       "                n_iter_no_change=None, presort='auto', random_state=466,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.731083076505456,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.01, loss='deviance', max_depth=5,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=300,\n",
       "                n_iter_no_change=None, presort='auto', random_state=8,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.7310596753321212,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.01, loss='deviance', max_depth=5,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=300,\n",
       "                n_iter_no_change=None, presort='auto', random_state=62,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.7309151730867786,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.1, loss='deviance', max_depth=5,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=30,\n",
       "                n_iter_no_change=None, presort='auto', random_state=249,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.7307847115454369,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.01, loss='deviance', max_depth=5,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=160,\n",
       "                n_iter_no_change=None, presort='auto', random_state=358,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.7307145080254324,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.01, loss='deviance', max_depth=5,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=300,\n",
       "                n_iter_no_change=None, presort='auto', random_state=384,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.7305401692840879,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.01, loss='deviance', max_depth=5,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=160,\n",
       "                n_iter_no_change=None, presort='auto', random_state=418,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.7268919263611877,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.1, loss='deviance', max_depth=1,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=160,\n",
       "                n_iter_no_change=None, presort='auto', random_state=302,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.7268919263611877,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.1, loss='deviance', max_depth=1,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=160,\n",
       "                n_iter_no_change=None, presort='auto', random_state=355,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.7268919263611877,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.1, loss='deviance', max_depth=1,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=160,\n",
       "                n_iter_no_change=None, presort='auto', random_state=221,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.7268381036625178,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.1, loss='deviance', max_depth=1,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=300,\n",
       "                n_iter_no_change=None, presort='auto', random_state=247,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.7268381036625178,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.1, loss='deviance', max_depth=1,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=300,\n",
       "                n_iter_no_change=None, presort='auto', random_state=66,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.7263595496678203,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=160,\n",
       "                n_iter_no_change=None, presort='auto', random_state=72,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.726078150558469,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.01, loss='deviance', max_depth=5,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=80,\n",
       "                n_iter_no_change=None, presort='auto', random_state=452,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.7259038118171246,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.01, loss='deviance', max_depth=5,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=80,\n",
       "                n_iter_no_change=None, presort='auto', random_state=38,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.7253187824837537,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.1, loss='deviance', max_depth=10,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=30,\n",
       "                n_iter_no_change=None, presort='auto', random_state=336,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.724172124990347,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.1, loss='deviance', max_depth=5,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                n_iter_no_change=None, presort='auto', random_state=387,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.7239568341956666,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.1, loss='deviance', max_depth=5,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                n_iter_no_change=None, presort='auto', random_state=286,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.7238181822436578,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.1, loss='deviance', max_depth=5,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=80,\n",
       "                n_iter_no_change=None, presort='auto', random_state=459,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.7211972508301566,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.1, loss='deviance', max_depth=10,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=80,\n",
       "                n_iter_no_change=None, presort='auto', random_state=132,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.7202161566380938,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.1, loss='deviance', max_depth=10,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=30,\n",
       "                n_iter_no_change=None, presort='auto', random_state=104,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.7193696191927063,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.01, loss='deviance', max_depth=10,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=300,\n",
       "                n_iter_no_change=None, presort='auto', random_state=430,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.7193497281953718,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.1, loss='deviance', max_depth=10,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=30,\n",
       "                n_iter_no_change=None, presort='auto', random_state=159,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.7192186816246966,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.1, loss='deviance', max_depth=10,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=80,\n",
       "                n_iter_no_change=None, presort='auto', random_state=342,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.7186816246966623,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.1, loss='deviance', max_depth=10,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=300,\n",
       "                n_iter_no_change=None, presort='auto', random_state=457,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.7172483028299039,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.1, loss='deviance', max_depth=10,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=80,\n",
       "                n_iter_no_change=None, presort='auto', random_state=150,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.7171394873738968,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=300,\n",
       "                n_iter_no_change=None, presort='auto', random_state=213,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.7169347271072172,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=300,\n",
       "                n_iter_no_change=None, presort='auto', random_state=469,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.7153001551497793,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.01, loss='deviance', max_depth=10,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=160,\n",
       "                n_iter_no_change=None, presort='auto', random_state=407,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.7146051403017346,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.1, loss='deviance', max_depth=5,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=160,\n",
       "                n_iter_no_change=None, presort='auto', random_state=345,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.714551902632398,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                n_iter_no_change=None, presort='auto', random_state=271,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.714551902632398,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                n_iter_no_change=None, presort='auto', random_state=277,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.7133976397576574,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.01, loss='deviance', max_depth=3,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=80,\n",
       "                n_iter_no_change=None, presort='auto', random_state=12,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.7129910443709648,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.001, loss='deviance', max_depth=5,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=300,\n",
       "                n_iter_no_change=None, presort='auto', random_state=84,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.712989874312298,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.001, loss='deviance', max_depth=5,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=300,\n",
       "                n_iter_no_change=None, presort='auto', random_state=386,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.7127500122856161,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.01, loss='deviance', max_depth=5,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=30,\n",
       "                n_iter_no_change=None, presort='auto', random_state=406,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.7031894629196708,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.1, loss='deviance', max_depth=10,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                n_iter_no_change=None, presort='auto', random_state=455,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.7030291648823273,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.1, loss='deviance', max_depth=10,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                n_iter_no_change=None, presort='auto', random_state=270,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.702593318028966,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.01, loss='deviance', max_depth=10,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=80,\n",
       "                n_iter_no_change=None, presort='auto', random_state=465,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.7025336450369623,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.1, loss='deviance', max_depth=5,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=300,\n",
       "                n_iter_no_change=None, presort='auto', random_state=173,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.7023119189196145,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.1, loss='deviance', max_depth=10,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                n_iter_no_change=None, presort='auto', random_state=334,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.7007130337515124,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.1, loss='deviance', max_depth=1,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=30,\n",
       "                n_iter_no_change=None, presort='auto', random_state=6,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.7007130337515124,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.1, loss='deviance', max_depth=1,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=30,\n",
       "                n_iter_no_change=None, presort='auto', random_state=440,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.7007130337515124,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.1, loss='deviance', max_depth=1,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=30,\n",
       "                n_iter_no_change=None, presort='auto', random_state=469,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.7007130337515124,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.1, loss='deviance', max_depth=1,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=30,\n",
       "                n_iter_no_change=None, presort='auto', random_state=269,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.6955840815858506,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.001, loss='deviance', max_depth=5,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=160,\n",
       "                n_iter_no_change=None, presort='auto', random_state=73,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.695460055367176,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.001, loss='deviance', max_depth=5,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=160,\n",
       "                n_iter_no_change=None, presort='auto', random_state=436,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.6954255386365071,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.001, loss='deviance', max_depth=5,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=160,\n",
       "                n_iter_no_change=None, presort='auto', random_state=80,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.6952827914791647,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.001, loss='deviance', max_depth=5,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=160,\n",
       "                n_iter_no_change=None, presort='auto', random_state=426,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.6922499994149707,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.01, loss='deviance', max_depth=5,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                n_iter_no_change=None, presort='auto', random_state=390,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.6904639048601896,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.001, loss='deviance', max_depth=5,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=30,\n",
       "                n_iter_no_change=None, presort='auto', random_state=135,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.6904194426308533,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.001, loss='deviance', max_depth=5,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=30,\n",
       "                n_iter_no_change=None, presort='auto', random_state=136,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.688962134561427,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.01, loss='deviance', max_depth=3,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=30,\n",
       "                n_iter_no_change=None, presort='auto', random_state=381,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.688962134561427,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.01, loss='deviance', max_depth=3,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=30,\n",
       "                n_iter_no_change=None, presort='auto', random_state=374,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.6879044015266925,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.001, loss='deviance', max_depth=5,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                n_iter_no_change=None, presort='auto', random_state=241,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.687587900657339,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.001, loss='deviance', max_depth=5,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                n_iter_no_change=None, presort='auto', random_state=79,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.6855952907478782,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.01, loss='deviance', max_depth=1,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=160,\n",
       "                n_iter_no_change=None, presort='auto', random_state=345,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.6855952907478782,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.01, loss='deviance', max_depth=1,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=160,\n",
       "                n_iter_no_change=None, presort='auto', random_state=136,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.6855952907478782,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.01, loss='deviance', max_depth=1,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=160,\n",
       "                n_iter_no_change=None, presort='auto', random_state=178,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.6817253217076304,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.1, loss='deviance', max_depth=1,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                n_iter_no_change=None, presort='auto', random_state=138,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.6788967048807828,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.001, loss='deviance', max_depth=3,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=160,\n",
       "                n_iter_no_change=None, presort='auto', random_state=450,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.6788967048807828,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.001, loss='deviance', max_depth=3,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=160,\n",
       "                n_iter_no_change=None, presort='auto', random_state=377,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.6788967048807828,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.001, loss='deviance', max_depth=3,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=160,\n",
       "                n_iter_no_change=None, presort='auto', random_state=379,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.6759774085072625,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.01, loss='deviance', max_depth=10,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=30,\n",
       "                n_iter_no_change=None, presort='auto', random_state=314,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.6739783632751346,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.001, loss='deviance', max_depth=3,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=80,\n",
       "                n_iter_no_change=None, presort='auto', random_state=215,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.6733143549817588,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.001, loss='deviance', max_depth=3,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=30,\n",
       "                n_iter_no_change=None, presort='auto', random_state=413,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.6733143549817588,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.001, loss='deviance', max_depth=3,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=30,\n",
       "                n_iter_no_change=None, presort='auto', random_state=117,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.6731540569444152,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.001, loss='deviance', max_depth=3,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                n_iter_no_change=None, presort='auto', random_state=168,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.6731540569444152,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.001, loss='deviance', max_depth=3,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                n_iter_no_change=None, presort='auto', random_state=123,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.6731540569444152,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.001, loss='deviance', max_depth=3,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                n_iter_no_change=None, presort='auto', random_state=349,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.6731540569444152,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.001, loss='deviance', max_depth=3,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                n_iter_no_change=None, presort='auto', random_state=50,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.6731540569444152,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.001, loss='deviance', max_depth=3,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                n_iter_no_change=None, presort='auto', random_state=390,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.6723297506136958,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.01, loss='deviance', max_depth=3,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                n_iter_no_change=None, presort='auto', random_state=42,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.6698474711522036,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.01, loss='deviance', max_depth=1,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=80,\n",
       "                n_iter_no_change=None, presort='auto', random_state=5,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.6690787426081544,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.001, loss='deviance', max_depth=10,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=160,\n",
       "                n_iter_no_change=None, presort='auto', random_state=406,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.6686990585707968,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.001, loss='deviance', max_depth=10,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=160,\n",
       "                n_iter_no_change=None, presort='auto', random_state=51,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.6673575863093776,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.01, loss='deviance', max_depth=10,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                n_iter_no_change=None, presort='auto', random_state=172,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.6672201044160354,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.001, loss='deviance', max_depth=10,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=80,\n",
       "                n_iter_no_change=None, presort='auto', random_state=281,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.6663718118826478,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.001, loss='deviance', max_depth=10,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=80,\n",
       "                n_iter_no_change=None, presort='auto', random_state=202,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.6663630364426472,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.001, loss='deviance', max_depth=10,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=80,\n",
       "                n_iter_no_change=None, presort='auto', random_state=287,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.6659856925226231,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.01, loss='deviance', max_depth=10,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                n_iter_no_change=None, presort='auto', random_state=456,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.6602319290289215,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.001, loss='deviance', max_depth=10,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=30,\n",
       "                n_iter_no_change=None, presort='auto', random_state=93,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.6597217834502221,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.001, loss='deviance', max_depth=10,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=30,\n",
       "                n_iter_no_change=None, presort='auto', random_state=349,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.6384009744248577,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.01, loss='deviance', max_depth=1,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=30,\n",
       "                n_iter_no_change=None, presort='auto', random_state=87,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.6384009744248577,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.01, loss='deviance', max_depth=1,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=30,\n",
       "                n_iter_no_change=None, presort='auto', random_state=42,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.6384009744248577,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.01, loss='deviance', max_depth=1,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=30,\n",
       "                n_iter_no_change=None, presort='auto', random_state=422,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.6384009744248577,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.01, loss='deviance', max_depth=1,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=30,\n",
       "                n_iter_no_change=None, presort='auto', random_state=490,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.6315900629257551,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.001, loss='deviance', max_depth=1,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=160,\n",
       "                n_iter_no_change=None, presort='auto', random_state=23,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.6096976802416874,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.001, loss='deviance', max_depth=1,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=30,\n",
       "                n_iter_no_change=None, presort='auto', random_state=341,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.6096976802416874,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.001, loss='deviance', max_depth=1,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=80,\n",
       "                n_iter_no_change=None, presort='auto', random_state=217,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.6096976802416874,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.001, loss='deviance', max_depth=1,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=80,\n",
       "                n_iter_no_change=None, presort='auto', random_state=474,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.6096976802416874,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.001, loss='deviance', max_depth=1,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=30,\n",
       "                n_iter_no_change=None, presort='auto', random_state=355,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.6096976802416874,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.01, loss='deviance', max_depth=1,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                n_iter_no_change=None, presort='auto', random_state=265,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.6096976802416874,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.001, loss='deviance', max_depth=1,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=80,\n",
       "                n_iter_no_change=None, presort='auto', random_state=120,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.6096976802416874,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.01, loss='deviance', max_depth=1,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                n_iter_no_change=None, presort='auto', random_state=52,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.6096976802416874,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.001, loss='deviance', max_depth=1,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=30,\n",
       "                n_iter_no_change=None, presort='auto', random_state=307,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False)),\n",
       " (0.6096976802416874,\n",
       "  GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                learning_rate=0.001, loss='deviance', max_depth=1,\n",
       "                max_features=None, max_leaf_nodes=None,\n",
       "                min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                min_samples_leaf=1, min_samples_split=2,\n",
       "                min_weight_fraction_leaf=0.0, n_estimators=10,\n",
       "                n_iter_no_change=None, presort='auto', random_state=223,\n",
       "                subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                verbose=0, warm_start=False))]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbdt_search(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_search(budget):\n",
    "    ret = []\n",
    "    for i in range(budget):\n",
    "        # choose a set of random hyper-parameters\n",
    "        seed = random.randrange(500)\n",
    "        C = random.choice([0.0001, 0.001, 0.01, 0.1, 1.0])\n",
    "        kernel = random.choice(['linear', 'poly', 'rbf', 'sigmoid'])\n",
    "        print (\"running for \", C, kernel)\n",
    "        model = svm.SVC(random_state=seed, C=C, kernel=kernel, probability=True)\n",
    "        model.fit(train, train_labels)\n",
    "        preds = model.predict_proba(val)[:,1]\n",
    "        score = roc_auc_score(val_labels, preds)\n",
    "        ret.append((score, model))\n",
    "        print (score, model)\n",
    "    return list(reversed(sorted(ret, key=lambda x: x[0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running for  0.0001 linear\n",
      "0.6091231814363173 SVC(C=0.0001, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "  kernel='linear', max_iter=-1, probability=True, random_state=131,\n",
      "  shrinking=True, tol=0.001, verbose=False)\n",
      "running for  0.1 sigmoid\n",
      "0.6006542968064419 SVC(C=0.1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "  kernel='sigmoid', max_iter=-1, probability=True, random_state=91,\n",
      "  shrinking=True, tol=0.001, verbose=False)\n",
      "running for  0.1 linear\n",
      "0.7016912027969083 SVC(C=0.1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "  kernel='linear', max_iter=-1, probability=True, random_state=184,\n",
      "  shrinking=True, tol=0.001, verbose=False)\n",
      "running for  0.0001 linear\n",
      "0.6091208413189837 SVC(C=0.0001, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "  kernel='linear', max_iter=-1, probability=True, random_state=475,\n",
      "  shrinking=True, tol=0.001, verbose=False)\n",
      "running for  1.0 poly\n"
     ]
    }
   ],
   "source": [
    "svm_search(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best SVM: AUC=0.7\n",
    "C = 0.1\n",
    "kernel = 'linear'\n",
    "seed = 184\n",
    "model = svm.SVC(random_state=seed, C=C, kernel=kernel, probability=True)\n",
    "model.fit(train, train_labels)\n",
    "preds = model.predict_proba(val)[:,1]\n",
    "score = roc_auc_score(val_labels, preds)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Note **: Scikit-learn training algorithms do not accept categorical features and hence they need to be converted to numeric or binary before fitting the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did really well for only using numerical features, but there's much more to be done! Dimensionality reduction, text processing, and more. Stay tuned for next week's natural language processing (NLP) theme.\n",
    "==========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
