{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification: End-to-end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle StumbleUpon Competition\n",
    "\n",
    "https://www.kaggle.com/c/stumbleupon\n",
    "\n",
    "** Competition **: \n",
    "1. Some web pages, such as news articles or seasonal recipes, are only relevant for a short period of time. Others continue to be important for a long time.\n",
    "2. The goal is to identify pages which pages will be relevant for a short span of time, and which will be relevant for a long span on time and are thus considered \"evergreen\". \n",
    "\n",
    "** Evaluation **: Area under the curve (AUC) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Python Modules \n",
    "================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick hack to fix import path\n",
    "# import sys; sys.path.append('/Users/julianalverio/code/conda/envs/sac/lib/python3.6/site-packages/')\n",
    "\n",
    "# data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# plots\n",
    "%matplotlib inline\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "\n",
    "# classification algorithms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "# dimensionality reduction\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# cross-validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection\n",
    "\n",
    "\n",
    "# model evaluation\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "os.chdir(os.path.join(\"..\", \"data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use pandas to read this .tsv table\n",
    "data = pd.read_table(\"stumbleupon/train.tsv\", sep= \"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable descriptions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **url**: Url of the webpage to be classified\n",
    "2. **urlid**: StumbleUpon's unique identifier for each url\n",
    "3. **boilerplate**: Boilerplate text\n",
    "4. **alchemy_category**:\tAlchemy category\n",
    "5. **alchemy_category_score**:\tAlchemy category score\n",
    "6. **avglinksize**:\tAverage number of words in each link\n",
    "7. **commonLinkRatio_1**:\t# of links sharing at least 1 word with 1 other links / # of links\n",
    "8. **commonLinkRatio_2**:\t# of links sharing at least 1 word with 2 other links / # of links\n",
    "9. **commonLinkRatio_3**:\t# of links sharing at least 1 word with 3 other links / # of links\n",
    "10. **commonLinkRatio_4**:\t# of links sharing at least 1 word with 4 other links / # of links\n",
    "11. **compression_ratio**:\tCompression achieved on this page via gzip (measure of redundancy)\n",
    "12. **embed_ratio**: Count of number of \"embed\" usage\n",
    "13. **frameBased**: A page is frame-based (1) if it has no body markup but have a frameset markup\n",
    "14. **frameTagRatio**: Ratio of iframe markups over total number of markups\n",
    "15. **hasDomainLink**:\tTrue (1) if it contains an \"a\" with an url with domain\n",
    "16. **html_ratio**:\tRatio of tags vs text in the page\n",
    "17. **image_ratio**: Ratio of \"img\" tags vs text in the page\n",
    "18. **is_news**: True (1) if StumbleUpon's news classifier determines that this webpage is news\n",
    "19. **lengthyLinkDomain**: True (1) if at least 3 \"a\"'s text contains more than 30 alphanumeric characters\n",
    "20. **linkwordscore**: Percentage of words on the page that are in hyperlink's text\n",
    "21. **news_front_page**: True (1) if StumbleUpon's news classifier determines that this webpage is front-page news\n",
    "22. **non_markup_alphanum_characters**:\tinteger\tPage's text's number of alphanumeric characters\n",
    "23. **numberOfLinks**: Number of \"a\"  markups\n",
    "24. **numwords_in_url**: Number of words in url\n",
    "25. **parametrizedLinkRatio**: A link is parametrized if it's url contains parameters  or has an attached onClick event\n",
    "26. **spelling_errors_ratio**: Ratio of words not found in wiki (considered to be a spelling mistake)\n",
    "27. **label**: User-determined label. Either evergreen (1) or non-evergreen (0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Features\n",
    "\n",
    "Before we dive into engineering our features for this problem, let's first understand how we can organize different features based off of their properties.\n",
    "\n",
    "### Types of Numerical Features\n",
    "\n",
    "1. Continuous number\n",
    "2. Discrete number\n",
    "3. One-hot vector\n",
    "4. Binned number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "------------------------------\n",
    "FEATURES THAT ARE NOT SUITABLE\n",
    "------------------------------\n",
    "url -- object type, the literal url\n",
    "urlid -- uid for the url, drop it\n",
    "avglinksize -- huge spread, mostly around 1 or two. This would need to be hella binned carefully. I think useless from Madhav's results\n",
    "frameBased -- 0 everywhere, garbage\n",
    "is_news -- 1 everywhere\n",
    "\n",
    "------------------------------\n",
    "TEXT\n",
    "------------------------------\n",
    "boilerplate -- text feature\n",
    "\n",
    "------------------------------\n",
    "NUMERICAL\n",
    "------------------------------\n",
    "alchemy_category -- business, health, sports, etc.\n",
    "alchemy_category_score -- 0 to 1 STRING with LOTS of '?' values\n",
    "commonlinkratio -- cont value from 0 to 1 that works nicely, needs binning. Nice gaussian on 1, moves to left with increasing idx\n",
    "compression_ratio -- continuous value with huge bimodal spread -- needs to be binned carefully\n",
    "embed_ratio -- perfect bimodal spread -- two humps. -1 and 0.25\n",
    "frameTagRatio -- 0 to 0.44 cont, spread leans left -- it is half a gaussian\n",
    "hasDomainLink -- 0/1 categorical, almost all 0\n",
    "html_ratio -- cont 0 to 0.7, gaussian spread\n",
    "image_ratio -- spread -1 to 113, but almost exclusively right at 0\n",
    "lengthy-link domain -- categorical 0/1, good spread\n",
    "linkwordscore -- 0 to 100 good spread\n",
    "news_front_page -- STRING, ? values, 0-1 categorical, almost all 0\n",
    "non_markup_alphnum_characters -- huge spread, highly concentrated near 0\n",
    "numberoflinks -- huge spread, mostly close to 0\n",
    "numwordsinurl -- good spread, 0 to 25, has a long upper tail\n",
    "Parametrizedlinkratio -- 0 to 1 continuous, skewed toward 0 -- right half of gaussian\n",
    "spelling_errors_ratio -- 0 to 1, heavy leaning to 0\n",
    "\n",
    "------------------------------\n",
    "Entropy measures on each feature - Entropy captures the amount of \"randomness\" of a vector. \n",
    "------------------------------\n",
    "\n",
    "[(-0.0017029098026004608, 'avglinksize'),\n",
    " (-0.0015477640384510272, 'image_ratio'),\n",
    " (-0.0013366491854696072, 'numberOfLinks'),\n",
    " (-0.0012422148274934264, 'urlid'),\n",
    " (-0.0012276386004722584, 'non_markup_alphanum_characters'),\n",
    " (-0.0003632131231506852, 'hasDomainLink'),\n",
    " (-1.6445368175466157e-05, 'compression_ratio'),\n",
    " (0, 'alchemy_category_score'),\n",
    " (0, 'boilerplate'),\n",
    " (0, 'commonLinkRatio_1'),\n",
    " (0, 'commonLinkRatio_2'),\n",
    " (0, 'commonLinkRatio_3'),\n",
    " (0, 'commonLinkRatio_4'),\n",
    " (0, 'frameBased'),\n",
    " (0, 'is_news'),\n",
    " (0, 'label'),\n",
    " (0, 'news_front_page'),\n",
    " (0, 'url'),\n",
    " (0.00016331922818457745, 'lengthyLinkDomain'),\n",
    " (0.0007063846400122697, 'html_ratio'),\n",
    " (0.0011190978216005787, 'embed_ratio'),\n",
    " (0.0020862547468744053, 'spelling_errors_ratio'),\n",
    " (0.002983052198938685, 'parametrizedLinkRatio'),\n",
    " (0.004883582510772921, 'numwords_in_url'),\n",
    " (0.016376170421172676, 'linkwordscore'),\n",
    " (0.016942353001673127, 'frameTagRatio'),\n",
    " (0.040352206434091764, 'alchemy_category')]\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering - Getting Features into a Useful Form\n",
    "\n",
    "Now we can dive into feature engineering itself - the manipulation of features into a form in which they can be leveraged to solve useful problems in machine learning.  A few of the feature engineering techniques we use are:\n",
    "\n",
    "1. **One-hot encoding**: Converting discrete numbers in a range bounded by N, and converting this into a vector of zeros of length N.  The only non-zero index of this all-zeros vector corresponds to the number of the feature.  For example:\n",
    "\n",
    "If we have the numbers 0, 1, and 2, we can convert these to a **one-hot encoding** with:\n",
    "\n",
    "0 --> (1 0 0), 1 --> (0 1 0), 2 --> (0 0 1)\n",
    "\n",
    "2. **Replace missing with random**: Replace missing data with random value.\n",
    "\n",
    "3. **Binning**: Taking a set of discrete or continuous numbers, and reducing the size of the feature space for that feature by assigning values based off of different categories.  An example would be binning age (discrete numbers greater than zero) into different decades.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Alchemy category, converting to one-hot encodings: \n",
    "# Link: https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/\n",
    "\n",
    "df = data['alchemy_category']   # 2K ? values\n",
    "one_hots = pd.get_dummies(data['alchemy_category'])  # Convert to one-hot representation\n",
    "df = one_hots\n",
    "rename_dict = {'?': 'alchemy_cat_?'}\n",
    "df = df.rename(columns=rename_dict)\n",
    "\n",
    "# FrameTagRatio, leaving as continuous number\n",
    "df_var = data['frameTagRatio']\n",
    "df['frame_tag_ratio'] = df_var\n",
    "\n",
    "\n",
    "# link word score, 0-100 gaussian, keeping continuous\n",
    "df['link_word_score'] = data['linkwordscore']\n",
    "\n",
    "\n",
    "#2.  alchemy category score, with replacing missing values with random\n",
    "df_var = data['alchemy_category_score']\n",
    "df_var_temp = df_var.apply(lambda x: np.random.random() if x == '?' else float(x)).astype('float32')\n",
    "df['alchemy_category_score'] = df_var_temp\n",
    "\n",
    "\n",
    "#3. num word in url -- discrete 0-25 to custom binning from looking at the histogram\n",
    "df_var = data['numwords_in_url']\n",
    "bins = [0, 6, 8, 13, 25]\n",
    "df_var_temp = pd.cut(x=df_var, bins=bins, right=True, labels=['num_words_url_bin_0', 'num_words_url_bin_1', 'num_words_url_bin_2', 'num_words_url_bin_3'])\n",
    "dummies = pd.get_dummies(df_var_temp)\n",
    "df = pd.concat([df, dummies], axis=1)\n",
    "\n",
    "\n",
    "# parameterized_link_ratio -- leaving as continuous, right-half gaussian\n",
    "df['parameterized_link_ratio'] = data['parametrizedLinkRatio']\n",
    "\n",
    "# spelling errors ratio -- leaving as continuous\n",
    "df['spelling_errors_ratio'] = data['spelling_errors_ratio']\n",
    "\n",
    "# embed_ratio -- bimodal continuous binned into 2 bins\n",
    "df_var = pd.DataFrame(data['embed_ratio'])\n",
    "df_var = df_var['embed_ratio'].apply(lambda x: 1 if x > -1 else 0)\n",
    "dummies = pd.get_dummies(df_var)\n",
    "rename = {0: 'embed_ratio_0', 1: 'embed_ratio_1'}\n",
    "dummies = dummies.rename(columns=rename)\n",
    "df = pd.concat([df, dummies], axis=1)\n",
    "\n",
    "\n",
    "# html_ratio -- leaving continuous\n",
    "df['html_ratio'] = data['html_ratio']\n",
    "\n",
    "# lengthy_link_domain\n",
    "df_var = pd.get_dummies(data['lengthyLinkDomain'])\n",
    "rename = {0: 'lengthy_link_domain_0', 1: 'lengthy_link_domain_1'}\n",
    "df_var = df_var.rename(columns=rename)\n",
    "df = pd.concat([df, df_var], axis=1)\n",
    "\n",
    "df['labels'] = data['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training and Testing Data Splits\n",
    "Whenever we create and implement a **supervised** machine learning algorithm, we want to split our data into **training (known as in-sample)** and **testing (known as out-of-sample)** datasets.  The ratio for this can vary, but some common examples for train: test are 0.5: 0.5 and 0.8: 0.2.  The **testing** dataset can also be split into **validation** and **test** datasets - the difference between these two **testing** subsets are given below, courtesy of [machinelearningmastery.com](https://machinelearningmastery.com/difference-test-validation-datasets/).\n",
    "\n",
    "**Validation Dataset**: The sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters. The evaluation becomes more biased as skill on the validation dataset is incorporated into the model configuration.\n",
    "\n",
    "**Test Dataset**: The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing\n",
    "train, val = train_test_split(df, test_size=0.5, train_size=0.5, random_state=234)\n",
    "\n",
    "# Split testing into validation and test\n",
    "val, test = train_test_split(val, test_size=0.5, train_size=0.5, random_state= 675)\n",
    "\n",
    "# Get labels for training dataset\n",
    "train_labels = train['labels']\n",
    "train = train.drop(['labels'], axis=1, inplace=False)\n",
    "\n",
    "# Get labels for validation dataset\n",
    "val_labels = val['labels']\n",
    "val = val.drop(['labels'], axis=1, inplace=False)\n",
    "\n",
    "# Get labels for testing dataset\n",
    "test_labels = test['labels']\n",
    "test = test.drop(['labels'], axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Search with Logistic Regression\n",
    "Now we will investigate the role hyperparameters play in machine learning algorithms.  We will test many different hyperparameter values on our model, and will evaulate how well these hyperparameters work based off of the score of the model on the **validation dataset**.  We will select the model and hyperparameters that yield the best performance on this **validation dataset**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_search(budget):\n",
    "    ret = []\n",
    "    for i in range(budget):  # Iterate through models with different hyperparameters\n",
    "        \n",
    "        # Use seeds to make sure your results are reproducible\n",
    "        seed = random.randrange(500)\n",
    "        \n",
    "        # Hyperparameters to randomly choose from\n",
    "        rand_C =    random.choice([0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000])\n",
    "        rand_fit_intercept = random.choice([True, False])\n",
    "        rand_penalty = random.choice(['l1', 'l2'])\n",
    "        \n",
    "        # Create model using randomly-selected hyperparameters\n",
    "        model = LogisticRegression(random_state=seed, C = rand_C, fit_intercept = rand_fit_intercept, penalty=rand_penalty)\n",
    "        \n",
    "        # Train the model, make predictions, and evaluate the model\n",
    "        model.fit(train, train_labels)\n",
    "        preds = model.predict_proba(val)[:,1]\n",
    "        score = roc_auc_score(val_labels, preds)\n",
    "        \n",
    "        # Add score to all the scores\n",
    "        ret.append((score, model))\n",
    "    \n",
    "    # Return all scores, beginning with the model/hyperparameters that performed the best\n",
    "    return list(reversed(sorted(ret, key=lambda x: x[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best logistic regression: AUC=0.71\n",
    "**Hyperparameters**:\n",
    "seed = 61\n",
    "C = 1\n",
    "fit_intercept = False\n",
    "penalty = 'l1'\n",
    "model = LogisticRegression(random_state=seed, C=C, fit_intercept=fit_intercept, penalty=penalty)\n",
    "model.fit(train, train_labels)\n",
    "preds = model.predict_proba(val)[:,1]\n",
    "score = roc_auc_score(val_labels, preds)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will run logistic search over our logistic regression model with 100 random combinations of hyperparameters\n",
    "logistic_search(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Search with Rule-Based Decision Trees (RBDTs)\n",
    "We will now apply the same random hyperparameter approach as the one used above, but for **(gradient-boosted) Rule-Based Decision Trees**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbdt_search(budget):\n",
    "    ret = []\n",
    "    for i in range(budget):  # Iterate through different random combinations of hyperparameters\n",
    "\n",
    "        # Use seeds to make sure your results are reproducible\n",
    "        seed = random.randrange(500)\n",
    "        \n",
    "        # Hyperparameters to randomly choose from\n",
    "        ccp_alpha = random.choice([0.0, 0.1])\n",
    "        learning_rate = random.choice([0.001, 0.01, 0.1])\n",
    "        n_estimators = random.choice([10, 30, 80, 160, 300])\n",
    "        max_depth = random.choice([1,3,5,10])\n",
    "        \n",
    "        # Create model using randomly-selected hyperparameters\n",
    "        model = GradientBoostingClassifier(random_state=seed, learning_rate=learning_rate, n_estimators=n_estimators, max_depth = max_depth)\n",
    "        \n",
    "        # Train the model, make predictions, and evaluate the model\n",
    "        model.fit(train, train_labels)\n",
    "        preds = model.predict_proba(val)[:,1]\n",
    "        score = roc_auc_score(val_labels, preds)\n",
    "        \n",
    "        # Add score to all scores\n",
    "        ret.append((score, model))\n",
    "        \n",
    "    # Return all scores, in order of best to worst\n",
    "    return list(reversed(sorted(ret, key=lambda x: x[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Gradient Boosted Decision Tree: AUC=0.73\n",
    "learning_rate = 0.01\n",
    "max_depth = 3\n",
    "n_estimators = 300\n",
    "random_state = 239\n",
    "model = GradientBoostingClassifier(random_state=seed, learning_rate=learning_rate, n_estimators=n_estimators, max_depth = max_depth)\n",
    "model.fit(train, train_labels)\n",
    "preds = model.predict_proba(val)[:,1]\n",
    "score = roc_auc_score(val_labels, preds)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run RBDT over 100 combinations of hyperparameters\n",
    "rbdt_search(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Search with Support Vector Machines (SVMs)\n",
    "We will now apply the same random hyperparameter approach as the one used above, but for **Support Vector Machines**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_search(budget):\n",
    "    ret = []\n",
    "    for i in range(budget): # Iterate through random combinations of hyperparameters\n",
    "\n",
    "        # Use seeds to make sure your results are reproducible\n",
    "        seed = random.randrange(500)\n",
    "        \n",
    "        # Hyperparameters to randomly choose from\n",
    "        C = random.choice([0.0001, 0.001, 0.01, 0.1, 1.0])\n",
    "        kernel = random.choice(['linear', 'poly', 'rbf', 'sigmoid'])\n",
    "        print (\"running for \", C, kernel)\n",
    "        \n",
    "        # Create model using randomly-selected hyperparameters\n",
    "        model = svm.SVC(random_state=seed, C=C, kernel=kernel, probability=True)\n",
    "        \n",
    "        # Train the model, make predictions, and evaluate the model\n",
    "        model.fit(train, train_labels)\n",
    "        preds = model.predict_proba(val)[:,1]\n",
    "        score = roc_auc_score(val_labels, preds)\n",
    "        \n",
    "        # Add score to all scores\n",
    "        ret.append((score, model))\n",
    "        print (score, model)\n",
    "    \n",
    "    # Return all scores, in order of best to worst\n",
    "    return list(reversed(sorted(ret, key=lambda x: x[0])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run SVM search over 20 different combinations of hyperparameters\n",
    "svm_search(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Parameters for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best SVM: AUC=0.7\n",
    "C = 0.1\n",
    "kernel = 'linear'\n",
    "seed = 184\n",
    "model = svm.SVC(random_state=seed, C=C, kernel=kernel, probability=True)\n",
    "model.fit(train, train_labels)\n",
    "preds = model.predict_proba(val)[:,1]\n",
    "score = roc_auc_score(val_labels, preds)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Note **: Scikit-learn training algorithms do not accept categorical features and hence they need to be converted to numeric or binary before fitting the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did really well for only using numerical features, but there's much more to be done! Dimensionality reduction, text processing, and more. Stay tuned for next week's natural language processing (NLP) theme.\n",
    "==========="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
