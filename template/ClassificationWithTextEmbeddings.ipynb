{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification: Now with Text (Word Embeddings) :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle StumbleUpon Competition\n",
    "\n",
    "https://www.kaggle.com/c/stumbleupon\n",
    "\n",
    "** Competition **: \n",
    "1. Some web pages, such as news articles or seasonal recipes, are only relevant for a short period of time. Others continue to be important for a long time.\n",
    "2. The goal is to identify pages which pages will be relevant for a short span of time, and which will be relevant for a long span on time and are thus considered \"evergreen\". \n",
    "\n",
    "** Evaluation **: Area under the curve (AUC) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Python Modules \n",
    "================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick hack to fix import path\n",
    "# import sys; sys.path.append('/Users/julianalverio/code/conda/envs/sac/lib/python3.6/site-packages/')\n",
    "\n",
    "# data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# plots\n",
    "%matplotlib inline\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "\n",
    "# classification algorithms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "# dimensionality reduction\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# cross-validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection\n",
    "\n",
    "# text features\n",
    "import re\n",
    "from nltk import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# model evaluation\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "os.chdir(os.path.join(\"..\", \"data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull data remotely from Dropbox\n",
    "! wget -O /home/ubuntu/machine_learning_aws/data/train.tsv \"https://www.dropbox.com/s/10ch2yhfk8tyfri/train.tsv?dl=0\"\n",
    "data = pd.read_table(\"/home/ubuntu/machine_learning_aws/data/train.tsv\", sep= \"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Numerical Features (same as Week 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alchemy category, converting to one-hots\n",
    "df = data['alchemy_category']   # 2K ? values\n",
    "one_hots = pd.get_dummies(data['alchemy_category'])\n",
    "df = one_hots\n",
    "rename_dict = {'?': 'alchemy_cat_?'}\n",
    "df = df.rename(columns=rename_dict)\n",
    "\n",
    "# FrameTagRatio, leaving as continuous number\n",
    "df_var = data['frameTagRatio']\n",
    "df['frame_tag_ratio'] = df_var\n",
    "\n",
    "\n",
    "\n",
    "# link word score, 0-100 gaussian, keeping continuous\n",
    "df['link_word_score'] = data['linkwordscore']\n",
    "\n",
    "\n",
    "# alchemy category score, with replacing missing values with random\n",
    "df_var = data['alchemy_category_score']\n",
    "df_var_temp = df_var.apply(lambda x: np.random.random() if x == '?' else float(x)).astype('float32')\n",
    "df['alchemy_category_score'] = df_var_temp\n",
    "\n",
    "\n",
    "# num word in url -- discrete 0-25 to custom binning from looking at the histogram\n",
    "df_var = data['numwords_in_url']\n",
    "bins = [0, 6, 8, 13, 25]\n",
    "df_var_temp = pd.cut(x=df_var, bins=bins, right=True, labels=['num_words_url_bin_0', 'num_words_url_bin_1', 'num_words_url_bin_2', 'num_words_url_bin_3'])\n",
    "dummies = pd.get_dummies(df_var_temp)\n",
    "df = pd.concat([df, dummies], axis=1)\n",
    "\n",
    "\n",
    "# parameterized_link_ratio -- leaving as continuous, right-half gaussian\n",
    "df['parameterized_link_ratio'] = data['parametrizedLinkRatio']\n",
    "\n",
    "# spelling errors ratio -- leaving as continuous\n",
    "df['spelling_errors_ratio'] = data['spelling_errors_ratio']\n",
    "\n",
    "# embed_ratio -- bimodal continuous binned into 2 bins\n",
    "df_var = pd.DataFrame(data['embed_ratio'])\n",
    "df_var = df_var['embed_ratio'].apply(lambda x: 1 if x > -1 else 0)\n",
    "dummies = pd.get_dummies(df_var)\n",
    "rename = {0: 'embed_ratio_0', 1: 'embed_ratio_1'}\n",
    "dummies = dummies.rename(columns=rename)\n",
    "df = pd.concat([df, dummies], axis=1)\n",
    "\n",
    "\n",
    "# html_ratio -- leaving continuous\n",
    "df['html_ratio'] = data['html_ratio']\n",
    "\n",
    "# lengthy_link_domain\n",
    "df_var = pd.get_dummies(data['lengthyLinkDomain'])\n",
    "rename = {0: 'lengthy_link_domain_0', 1: 'lengthy_link_domain_1'}\n",
    "df_var = df_var.rename(columns=rename)\n",
    "df = pd.concat([df, df_var], axis=1)\n",
    "\n",
    "df['labels'] = data['label']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall Week2-Day1 : Logistic Regression with numerical and text features (Tf-Idf with ngram_range=1,2 i.e. using unigrams and bigrams): AUC=0.86"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall: Different vectorization techniques in NLP (how to represent the feature BEFORE I feed this into the model?)\n",
    "\n",
    "Note: In this context we will use `document` as a general word, but in our context this means `website` from the EvergreenClassificationChallenge\n",
    "\n",
    "***CountVectorizer*** is a Bag-of-Words model which means we will count the frequency of separate words and include those as features in our model. There are variations to Bag-of-Words models like unigram, bigram, trigram (using 1 word, 2 words, 3 words together as feature etc). Generally, we use unigram and bigram together to capture nuances present in the text eg: Sentiment Analysis might care about words like `not good` and bigram will be useful in a case like that. \n",
    "\n",
    "***TfIdfVectorizer*** is a Tf-Idf model meaning we will use the output of CountVectorizer but multiply it by a scaling factor of $log(\\frac{N}{w})$. This scaling factor will reduce the importance of words that are very commonly present ***across*** documents. For example, **I** is too commonly used in many documents but does not hold much meaning to the document. If we want to highlight words that are present less frequently across documents, Tf-Idf is useful. \n",
    "\n",
    "You can still use `ngram_range` for the Tf-Idf Vectorizer\n",
    "\n",
    "*Main assumption here is that words that are present less frequently across documents are more indicative of the meaning of the document*\n",
    "\n",
    "***In practice, Tf-Idf will work better than Bag-Of-Words***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reference original Logistic Regression Model without Text Features AUC = 0.71\n",
    "train_old, val_old = train_test_split(df, test_size=0.5, train_size=0.5, random_state=234)\n",
    "val_old, test_old = train_test_split(val_old, test_size=0.5, train_size=0.5, random_state= 675)\n",
    "train_labels_old = train_old['labels']\n",
    "train_old = train_old.drop(['labels'], axis=1, inplace=False)\n",
    "val_labels_old = val_old['labels']\n",
    "val_old = val_old.drop(['labels'], axis=1, inplace=False)\n",
    "test_labels_old = test_old['labels']\n",
    "test_old = test_old.drop(['labels'], axis=1, inplace=False)\n",
    "\n",
    "model_old = LogisticRegression()\n",
    "model_old.fit(train_old, train_labels_old)\n",
    "preds_old = model_old.predict_proba(val_old)[:,1]\n",
    "score_old = roc_auc_score(val_labels_old, preds_old)\n",
    "score_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF add in the text features with tfidf\n",
    "idf_dtm_old = TfidfVectorizer(min_df= 10,  max_features= 1000, strip_accents= \"unicode\", ngram_range=(1, 2))\n",
    "idf_dtm_old.fit(data[\"boilerplate\"])\n",
    "data_text_old = idf_dtm_old.transform(data[\"boilerplate\"])\n",
    "train_text_old, val_text_old = train_test_split(data_text_old, test_size=0.5, train_size=0.5, random_state=234)\n",
    "val_text_old, test_text_old = train_test_split(val_text_old, test_size=0.5, train_size=0.5, random_state= 675)\n",
    "# We don't need to grab the train_labels_old again because we set the SAME random seed for the splitting\n",
    "# This is the power of setting the same random seed - with a particular seed, the data will be split randomly, \n",
    "# but in a consistent manner\n",
    "\n",
    "train_with_text_old = pd.concat([train_old.reset_index(drop = True), pd.DataFrame(train_text_old.toarray())], axis=1)\n",
    "val_with_text_old = pd.concat([val_old.reset_index(drop = True), pd.DataFrame(val_text_old.toarray())], axis=1)\n",
    "train_with_text_old.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the logistic regression model\n",
    "model_old = LogisticRegression()\n",
    "model_old.fit(train_with_text_old, train_labels_old)\n",
    "preds_old = model_old.predict_proba(val_with_text_old)[:,1]\n",
    "score_old = roc_auc_score(val_labels_old, preds_old)\n",
    "score_old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**With Tf-Idf our purpose was to show you a performance improvement compared to the Bag-Of-Words model, but we also wanted to show you that using text as a feature is a powerful way to improve the performance of your machine learning classifier.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 : Textual feature with Tf-Idf on a different data split\n",
    "\n",
    "- min_df = minimum frequencey cut-off\n",
    "- max_features = take the top 1000 most common feature\n",
    "- strip_accents = to handle non english letters\n",
    "- ngram_range = we are doing bag of word features here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we will try to answer the question of how to handle out of vocabulary words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By purposefully showing you a larger test size than the train size and ONLY fitting to the train data, we will introduce a performance drop on the test data due to out of vocabulary words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note difference 1 from before: Different Data Split and random state is None\n",
    "# We are simulating a limited dataset in this case by not randomizing\n",
    "# When we randomized, performance was similar indicating that this dataset is limited in its\n",
    "# vocabulary and probably the task of evergreen classification is too simple\n",
    "train, val = train_test_split(df, test_size=0.8, train_size=0.2, random_state=None, shuffle=False)\n",
    "train_labels = train['labels']\n",
    "train = train.drop(['labels'], axis=1, inplace=False)\n",
    "val_labels = val['labels']\n",
    "val = val.drop(['labels'], axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF add in the text features with tfidf\n",
    "idf_dtm = TfidfVectorizer(min_df= 10,  max_features= 1000, strip_accents= \"unicode\", ngram_range=(1, 2))\n",
    "\n",
    "# Note difference 2: Previously, we fitted on the full data's boilerplate, so we had no\n",
    "# out of vocabulary words. In this case, we fit ONLY on the train data boilerplate\n",
    "# splitted using the same random seed as the above cell\n",
    "train_boilerplate, val_boilerplate = train_test_split(data['boilerplate'], \n",
    "                                                      test_size=0.8, train_size=0.2, random_state=None)\n",
    "idf_dtm.fit(train_boilerplate)\n",
    "train_text = idf_dtm.transform(train_boilerplate)\n",
    "val_text = idf_dtm.transform(val_boilerplate)\n",
    "\n",
    "# Note difference 3: You don't need to explicitly split train_text and val_text from data_text\n",
    "# We will directly compute the transformation i.e. conversion into vectors for the \n",
    "# train and val\n",
    "# train_text, val_text = train_test_split(data_text, test_size=0.8, train_size=0.2, random_state=234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.choice(idf_dtm.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = train_text.toarray() # Note difference 4: Your train data is of a smaller size\n",
    "print (xx.shape)\n",
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_text = pd.concat([train.reset_index(drop = True), pd.DataFrame(train_text.toarray())], axis=1)\n",
    "val_with_text = pd.concat([val.reset_index(drop = True), pd.DataFrame(val_text.toarray())], axis=1)\n",
    "train_with_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(train_with_text, train_labels)\n",
    "preds = model.predict_proba(val_with_text)[:,1]\n",
    "score = roc_auc_score(val_labels, preds)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Word Embeddings\n",
    "\n",
    "- Let's try using Glove, which is a pre-trained word embedding scheme\n",
    "- Glove tries to represent the words in a more semantically meaningful manner such that you can find similarity statistics between different words. eg: You can find that the similarity between Football and Cricket is higher than the similarity between Football and Wine. \n",
    "- Glove representations are based on words in context. The idea is to use the words surrounding the main word to calculate the embeddings. Naturally, Football and Cricket will be present in similar contexts compared to Football and Wine. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe: Pre-trained word embeddings\n",
    "https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "**About GloVe**: GloVe is an unsupervised learning algorithm for obtaining vector representations for words. \n",
    "\n",
    "Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space. [https://nlp.stanford.edu/projects/glove/]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity let's get the data again\n",
    "train, val = train_test_split(df, test_size=0.8, train_size=0.2, random_state=None, shuffle=False)\n",
    "train_labels = train['labels']\n",
    "train = train.drop(['labels'], axis=1, inplace=False)\n",
    "val_labels = val['labels']\n",
    "val = val.drop(['labels'], axis=1, inplace=False)\n",
    "\n",
    "# Need to get the boilerplate for the same reasons as for the Tf-Idf case\n",
    "train_boilerplate, val_boilerplate = train_test_split(data['boilerplate'], \n",
    "                                                      test_size=0.8, train_size=0.2, random_state=None, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New imports in this section\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================================\n",
    "# glove embeddings\n",
    "# let's represent them in the form of pandas \n",
    "# dataframe for easy visualization\n",
    "#===============================================\n",
    "\n",
    "# If glove already on local machine\n",
    "#glove = pd.read_table(\"nlp/glove/glove.6B.50d.txt\",\n",
    "#                      sep = \" \", header = None, \n",
    "#                      quoting = csv.QUOTE_NONE)\n",
    "\n",
    "# If not already on local, download text file directly from Dropbox\n",
    "! wget -O /home/ubuntu/machine_learning_aws/data/glove.6B.50d.txt \"https://www.dropbox.com/s/9wlveoiprmvcfro/glove.6B.50d.txt?dl=0\"\n",
    "glove = pd.read_table(\"/home/ubuntu/machine_learning_aws/data/glove.6B.50d.txt\", sep = \" \", header = None, quoting = csv.QUOTE_NONE)\n",
    "\n",
    "\n",
    "# set column names\n",
    "glove.columns = [\"word\"] + [\"wv\" + str(x + 1) for x in range(glove.shape[1] - 1)]\n",
    "print(glove.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Uncomment this for your own visualization purposes\n",
    "# display(glove.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Uncomment this for your own visualization purposes\n",
    "# glove.loc[glove[\"word\"] == \"uruguay\", :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert data frame to dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to dictionary\n",
    "glove_dict = glove.set_index(\"word\").T.to_dict(\"list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Uncomment this for your own visualization purposes\n",
    "# glove_dict[\"uruguay\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert URL Boilerplate to dense numeric representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================================\n",
    "# prepare text to get the word vectors \n",
    "# corresponding to a URL\n",
    "#===============================================\n",
    "\n",
    "def UrlVectors(d):\n",
    "    '''\n",
    "    Expected input: A full boilerplate document, consisting of multiple words\n",
    "    Glove has an output corresponding to every word and we need to output a 50\n",
    "    dimensional embedding. So we will just average the word embeddings for all\n",
    "    words in the document in order to get a 50 dimensional output for a document\n",
    "    '''\n",
    "    d = word_tokenize(re.sub(r'[^\\w\\s]', ' ', d.lower())) \n",
    "    # above separates words that have spaces between them and \n",
    "    # separates stuff such as ***don't*** to make it ***do n't***\n",
    "    \n",
    "    url_vec = []\n",
    "    # get the corresponding vectors to the words from the glove \n",
    "    # dictionary as a list of word vectors, where each word vector\n",
    "    # is that of the word in the document\n",
    "    for word in d:\n",
    "        if word in glove_dict:\n",
    "            url_vec.append(glove_dict[word]) \n",
    "            \n",
    "    #print(f\"words found: {len(prod_vec)}\")        \n",
    "    if len(url_vec) > 0: \n",
    "        # Since we have url_vec being a list of word embeddings\n",
    "        # we need to average it along the column dimension to get\n",
    "        # the final document embedding\n",
    "        url_vec = np.array(url_vec)\n",
    "        url_vec = url_vec.mean(axis = 0)\n",
    "        return list(url_vec)\n",
    "    \n",
    "    else:\n",
    "        # in case we could not find the word, return a vector of 0's\n",
    "        url_vec = [0]*50 \n",
    "        return url_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vectors for all url boilerplate texts in the data\n",
    "url_emb_train = [UrlVectors(x) for x in list(train_boilerplate.values)]\n",
    "url_emb_val = [UrlVectors(x) for x in list(val_boilerplate.values)]\n",
    "len(url_emb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Uncomment and run for your own purpose\n",
    "# url_emb_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data frame for the embeddings of website boilerplates\n",
    "url_emb_train_df = pd.DataFrame(url_emb_train) # convert the array to a dataframe\n",
    "wv_names_train = [\"wv\" + str(x + 1) for x in range(url_emb_train_df.shape[1])] \n",
    "# above make the column heading as wv1, wv2, ..., wv50 etc where each correspond to the \n",
    "# 50 numbers in the word vector\n",
    "url_emb_train_df.columns = wv_names_train\n",
    "\n",
    "url_emb_val_df = pd.DataFrame(url_emb_val)\n",
    "wv_names_val = [\"wv\" + str(x + 1) for x in range(url_emb_val_df.shape[1])]\n",
    "url_emb_val_df.columns = wv_names_val\n",
    "\n",
    "### Uncomment below for your own purpose\n",
    "# display(url_emb_train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the word embeddings with the other numerical features of the dataset\n",
    "train_wv = pd.concat([train, url_emb_train_df], axis = 1)\n",
    "val.reset_index(val, inplace=True, drop=True) \n",
    "# Note: Above is a necessary step before merge because indexes did not start from 0 previously\n",
    "val_wv = pd.concat([val, url_emb_val_df], axis = 1)\n",
    "print(train_wv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train_wv.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression on Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is similar to Week2, Day 1: We are just doing Logistic Regression on the data \n",
    "# with the word embeddings\n",
    "model = LogisticRegression()\n",
    "model.fit(train_wv, train_labels)\n",
    "preds = model.predict_proba(val_wv)[:,1]\n",
    "score = roc_auc_score(val_labels, preds)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Material: Cosine Similarity with Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*One benefit of using word embeddings is the idea of [distributional semantics](https://aurelieherbelot.net/research/distributional-semantics-intro/)*\n",
    "Basically, you are able to see that words such as Football and Cricket are more similar than Football and Wine using a measure known as [Cosine Similarity](https://www.sciencedirect.com/topics/computer-science/cosine-similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New import in this section\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(np.array(glove_dict[\"uruguay\"]).reshape(1, -1) , \n",
    "                  np.array(glove_dict[\"brazil\"]).reshape(1, -1))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(np.array(glove_dict[\"uruguay\"]).reshape(1, -1) , \n",
    "                  np.array(glove_dict[\"argentina\"]).reshape(1, -1))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(np.array(glove_dict[\"uruguay\"]).reshape(1, -1) , \n",
    "                  np.array(glove_dict[\"japan\"]).reshape(1, -1))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(np.array(glove_dict[\"football\"]).reshape(1, -1) , \n",
    "                  np.array(glove_dict[\"cricket\"]).reshape(1, -1))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(np.array(glove_dict[\"football\"]).reshape(1, -1) , \n",
    "                  np.array(glove_dict[\"chess\"]).reshape(1, -1))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(np.array(glove_dict[\"football\"]).reshape(1, -1) , \n",
    "                  np.array(glove_dict[\"wine\"]).reshape(1, -1))[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Dimensionality Reduction Techniques: PCA and t-SNE\n",
    "\n",
    "***How to reduce overfitting?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis\n",
    "\n",
    "- n_components = number of linearly uncorrelated variables; basically the final dimensionality of your vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you want to reduce the number of features in your dataset, you will use [PCA](https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary package\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "# PCA is affected by the magnitude of your features so you need to scale the features in your data before\n",
    "# applying PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = StandardScaler().fit_transform(train_wv) # perform the scaling of your features\n",
    "\n",
    "### Uncomment below if you want to see what x looks like\n",
    "# viz_x = pd.DataFrame(x)\n",
    "# viz_x.columns = train_wv.columns\n",
    "# viz_x.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 2) # define PCA\n",
    "principalComponents = pca.fit_transform(x) # fit to the data and transform it in one line (no need to do them separately)\n",
    "principalDf = pd.DataFrame(data=principalComponents, columns=['PC1', 'PC2']) # convert to dataframe for visualization\n",
    "\n",
    "### Uncomment below if you want to see principalDf\n",
    "# principalDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDf = pd.concat([principalDf, train_labels], axis=1)\n",
    "\n",
    "### Uncomment below if you want to see what finalDf looks like (just has the label column also)\n",
    "# finalDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's plot this!**\n",
    "\n",
    "We will ask you to look into plotting with matplotlib in your own time, but we care about looking at the output of PCA in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 component PCA', fontsize = 20)\n",
    "targets = [0, 1]\n",
    "colors = ['r', 'g']\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = finalDf['labels'] == target\n",
    "    ax.scatter(finalDf.loc[indicesToKeep, 'PC1']\n",
    "               , finalDf.loc[indicesToKeep, 'PC2']\n",
    "               , c = color\n",
    "               , s = 50)\n",
    "ax.legend(['Non-Evergreen', 'Evergreen'])\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE: t-Distributed Stochastic Neighbor Embedding\n",
    "- n_components = the final dimensionality of your features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is a linear dimensionality reduction technique. In contrast, t-SNE is a non-linear technique. [This](https://towardsdatascience.com/t-sne-python-example-1ded9953f26) medium article shows how to implement the technique for those who are interested. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary python packages\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components = 2)\n",
    "tsne_obj = tsne.fit_transform(x)\n",
    "tsneDf = pd.DataFrame(data=tsne_obj, columns=['X', 'Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDf = pd.concat([tsneDf, train_labels], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's plot this!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('X', fontsize = 15)\n",
    "ax.set_ylabel('Y', fontsize = 15)\n",
    "ax.set_title('2 component t-SNE', fontsize = 20)\n",
    "targets = [0, 1]\n",
    "colors = ['r', 'g']\n",
    "for target, color in zip(targets,colors):\n",
    "    indicesToKeep = finalDf['labels'] == target\n",
    "    ax.scatter(finalDf.loc[indicesToKeep, 'X']\n",
    "               , finalDf.loc[indicesToKeep, 'Y']\n",
    "               , c = color\n",
    "               , s = 50)\n",
    "ax.legend(['Non-Evergreen', 'Evergreen'])\n",
    "ax.grid()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
