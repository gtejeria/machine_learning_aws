{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Tutorial\n",
    "This Jupyter notebook uses documentation from Keras to provide a quick, hands-on introduction to this high-level machine learning API.  For reference, we used code from the following resource for the examples below: https://keras.io/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras\n",
    "import keras\n",
    "\n",
    "# OR\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Import different layers\n",
    "from keras.layers import Dense\n",
    "\n",
    "# We also want to import an image reader to visualize data\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Data Structure in Keras: The Model\n",
    "The core data structure in Keras is the model class, which has methods for compiling with an optimizer and loss function, fitting (training), evaluation (testing), and prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward, Stacked Models from Sequential() Class\n",
    "This is the main element from the model class that we will be using in this tutorial.  Other types of models exist, and we encourage you to investigate them!  \n",
    "\n",
    "The main idea with the `Sequential()` model is that we stack layers sequentially after one another.  Each layer we add to our model is cascaded with the rest of the layers in our model.  The result we see after stacking these layers below is something similar to the picture below.\n",
    "\n",
    "![Deep NN](notebook_diagrams/deep_nn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, create the model\n",
    "model = Sequential()  # Enables for stacking of layers\n",
    "\n",
    "# Now add layers\n",
    "model.add(Dense(units=64, activation='relu', input_dim=100))  # Input/first hidden layer\n",
    "model.add(Dense(units=32, activation='relu'))  # Second layer\n",
    "model.add(Dense(units=16, activation='relu'))  # Third layer\n",
    "model.add(Dense(units=12, activation='relu'))  # Fourth layer\n",
    "model.add(Dense(units=10, activation='softmax'))  # Output layer\n",
    "\n",
    "# Now get information about the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation\n",
    "Like our other machine learning algorithms, we will use training and evaluation procedures with deep learning to create optimal models for predicting on UNSEEN data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new model and add layers\n",
    "model = Sequential()\n",
    "model.add(Dense(units=64, activation='relu', input_dim=100))\n",
    "model.add(Dense(units=10, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fit model to data (x_train and y_train in this case) - data inputs need only be numpy arrays\n",
    "from keras.datasets import cifar10  # Popular image dataset\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Cifar Data\n",
    "We'll be using the Cifar dataset for this example, which is a famous dataset that is used to train and provide a baseline evaluation for many neural network models in computer vision.\n",
    "\n",
    "Let's look at some of the data we'll be visualizing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Get random indices for showing data\n",
    "random_indices = np.random.randint(low=0, high=50000, size=3)\n",
    "\n",
    "# Show images randomly\n",
    "for index in random_indices:\n",
    "    IMG = x_train[index]\n",
    "    plt.imshow(IMG)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we are ready to fit our model!\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32)\n",
    "\n",
    "# Evaluate\n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)\n",
    "\n",
    "# Predictions\n",
    "classes = model.predict(x_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next week, we'll be diving into how our neural network models compare to some of the other models we've been analyzing in this course!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading Models\n",
    "Often times, we don't want to have to retrain models, and want to re-use them whenever we can.  We can do that by saving, loading, and sharing models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model\n",
    "model = Sequential()\n",
    "model.add(Dense(units=64, activation='relu', input_dim=100))\n",
    "model.add(Dense(units=10, activation='softmax'))\n",
    "\n",
    "# Save model to HDF5 file\n",
    "model.save_weights(filepath)\n",
    "model.load_weights(filepath, by_name=False)  # Can load for specific layers (e.g. transfer learning??) \n",
    "                                             #  by setting by_name arg to True\n",
    "\n",
    "# Save as JSON representation\n",
    "json_string = model.to_json()\n",
    "model_reloaded = model_from_json(json_string)\n",
    "\n",
    "# Save as yaml representation\n",
    "yaml_string = model.to_yaml()\n",
    "model_reloaded = model_from_yaml(yaml_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Model Specs\n",
    "We can also get model information using the specs below.  Model contains the following attributes that can be accessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_specs(model):\n",
    "    print(model.layers)  # Flattened list of tensors comprising model\n",
    "    print(model.inputs)  # List of input tensors to model\n",
    "    print(model.outputs)  # List of output tensors of model \n",
    "    print(model.summary())  # Brief summary of your Keras model\n",
    "    print(model.get_config())  # Dict containing configuration of model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get and Set Weights\n",
    "We can get information about weights from above, and can also set weights using our own weight files, or, more commonly, using pre-trained weights!  The intuition here is that some person/company/school went through a lot of trouble to find weights that generally work well for solving problems, so we should use them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_weights()  # Returns weights of model\n",
    "\n",
    "model.set_weights(weights)  # Sets weights of model to be weights arg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Loading Keras Datasets\n",
    "Next week, we will be analyzing the MNIST dataset, which consists of a series of handwritten digits that have labels ranging from 0 to 9.  We will briefly dive into how we can use deep learning with Keras to develop a classifier to predict a number given a handwritten digit - a.k.a. how to train a neural network to recognize handwritten digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters for \"Core\" Keras Layers\n",
    "\n",
    "1. units: Positive integer, dimensionality of the output space.\n",
    "2. activation: Activation function to use (see activations). If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x).\n",
    "3. use_bias: Boolean, whether the layer uses a bias vector.\n",
    "4. kernel_initializer: Initializer for the kernel weights matrix (see initializers).\n",
    "5. bias_initializer: Initializer for the bias vector (see initializers).\n",
    "6. kernel_regularizer: Regularizer function applied to the kernel weights matrix (see regularizer).\n",
    "7. bias_regularizer: Regularizer function applied to the bias vector (see regularizer).\n",
    "8. activity_regularizer: Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer).\n",
    "9. kernel_constraint: Constraint function applied to the kernel weights matrix (see constraints).\n",
    "10. bias_constraint: Constraint function applied to the bias vector (see constraints)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Keras Layers\n",
    "\n",
    "1. **Dense**: Fully connected layer\n",
    "2. **Activation**: Layer for applying an activation function to an output.  \n",
    "3. **Dropout**: Applies a dropout layer.  Probability of dropout is an argument in layer init.\n",
    "4. **Flatten**: Layer typically used for turning 2D/3D into 1D vector (e.g. Conv layers to Dense).\n",
    "5. **Conv1D**: 1D convolutional layer\n",
    "6. **Conv2D**: 2D convolutional layer\n",
    "7. **MaxPooling1D**: Layer for max pooling in 1D\n",
    "8. **MaxPooling2D**: Layer for max pooling in 2D\n",
    "9. **RNN & GRU**: Base classes for recurrent layers\n",
    "10. **BatchNormalization**: Batch normalization layer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Example - 1D CNN for Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "\n",
    "# set parameters:\n",
    "max_features = 5000\n",
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 2\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# we add a Convolution1D, which will learn filters\n",
    "# word group filters of size filter_length:\n",
    "model.add(Conv1D(filters,\n",
    "                 kernel_size,\n",
    "                 padding='valid',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "# we use max pooling:\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying the Model() Class for Fully-Customizable Models\n",
    "We can modify the `Model()` class in Keras to create flexible models with our choice of layers, actvation functions, etc.  \n",
    "\n",
    "The code below is more similar to the code we would see when building models in `pytorch` or `tensorflow`, which are machine learning packages that provide the user more flexibility with defining models and training and evaluation procedures, at the price of being somewhat more lower-level.  We will not be using these frameworks in this course, but we strongly encourage you to explore these frameworks in greater detail if you're interested.  Both `pytorch` and `tensorflow` are free and open-source, so you could start using them right now if you wanted to!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "class SimpleMLP(keras.Model):\n",
    "    \n",
    "    # This function is known as the \"constructor\" for class objects\n",
    "    def __init__(self, use_bn=False, use_dp=False, num_classes=10):\n",
    "        \n",
    "        # \"Inherits from the base keras.Model class\"\n",
    "        super(SimpleMLP, self).__init__(name='mlp')\n",
    "        self.use_bn = use_bn\n",
    "        self.use_dp = use_dp\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Define layers of neural network model\n",
    "        self.dense1 = keras.layers.Dense(32, activation='relu')\n",
    "        self.dense2 = keras.layers.Dense(num_classes, activation='softmax')\n",
    "        if self.use_dp:\n",
    "            self.dp = keras.layers.Dropout(0.5)\n",
    "        if self.use_bn:\n",
    "            self.bn = keras.layers.BatchNormalization(axis=-1)\n",
    "    \n",
    "    # Function for passing an input through the neural network\n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        if self.use_dp:\n",
    "            x = self.dp(x)\n",
    "        if self.use_bn:\n",
    "            x = self.bn(x)\n",
    "        return self.dense2(x)\n",
    "\n",
    "# Create a model, compile it with a loss function, optimizer, and metric, and then train it!\n",
    "model = SimpleMLP()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "# x_train and y_train are numpy arrays\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
