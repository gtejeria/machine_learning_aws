{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIT-GSL Uruguay \n",
    "\n",
    "## January 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson - 2: Introduction to ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StumbleUpon - Hands On Exercise\n",
    "\n",
    "**Overview**\n",
    "\n",
    "1. The following hands on exercise focuses on the application of techniques discussed during the lecture series and the hands on programming session.\n",
    "2. The exercise aims to bolster the understanding of use of algorithms for model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions\n",
    "----------------------\n",
    "\n",
    "\n",
    "1. We have the following datasets created:\n",
    "    1. TFIDF\n",
    "    2. TFIDF - with important features selected\n",
    "    3. TF (Count)\n",
    "    4. PCA on TFIDF\n",
    "    5. Base features (can be combined with any of the four above)\n",
    "2. We have the following algorithms to choose from:\n",
    "    1. GBM\n",
    "    2. RF\n",
    "    3. ERT\n",
    "    4. SGD\n",
    "    5. XGBoost (we will discuss this in detail)\n",
    "3. The task is to use pick a dataset and an algorithm, cross-validate it, and generate predictions for out-of-fold and test data\n",
    "4. These predictions will be included in the ensemble with the remaining models to generate the final probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points to note:\n",
    "-----------------------\n",
    "\n",
    "1. Don't pick a data-algo combination already used\n",
    "2. The ultimate objective is to build a better ensemble and **not** a strong individual model\n",
    "3. You can choose any other algorithm from outside the five mentioned above\n",
    "4. We are going to test the results on the separate hold out 25% sample \n",
    "5. There is no right answer to this question\n",
    "6. Your answer will depend on how you twist and turn the **given** data \n",
    "7. We will collectively discuss why we chose a particular algorithm and critically appraise each other's responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/software/sloan/local/lib/py36/cryptography/hazmat/bindings/openssl/binding.py:163: CryptographyDeprecationWarning: OpenSSL version 1.0.1 is no longer supported by the OpenSSL project, please upgrade. A future version of cryptography will drop support for it.\n",
      "  utils.CryptographyDeprecationWarning\n",
      "/home/software/sloan/local/lib/py36/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: (3697, 27)\n",
      "Validation data size: (1109, 27)\n",
      "Test data size: (740, 27)\n",
      "Fold 1 of 4\n",
      "Fold 2 of 4\n",
      "Fold 3 of 4\n",
      "Fold 4 of 4\n"
     ]
    }
   ],
   "source": [
    "# Collective code till now\n",
    "# ==============================================\n",
    "# 0. Module imports\n",
    "# ==============================================\n",
    "\n",
    "# working directory\n",
    "import os\n",
    "os.chdir(\"/pool001/madhavk/gsl-uruguay/W-01-IntroML/\")\n",
    "\n",
    "# data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "\n",
    "# plots\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "\n",
    "# classification algorithms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# dimension reduction\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# cross-validation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import model_selection\n",
    "\n",
    "# parallel processing\n",
    "from joblib import Parallel, delayed  \n",
    "import multiprocessing\n",
    "\n",
    "# model evaluation\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble.partial_dependence import plot_partial_dependence\n",
    "from sklearn.ensemble.partial_dependence import partial_dependence\n",
    "\n",
    "# text mining\n",
    "import re\n",
    "from nltk import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "# ==============================================\n",
    "# 1. Data import and explore\n",
    "# ==============================================\n",
    "\n",
    "train = pd.read_table(\"stumble-data/stumbleupon/train.tsv\", sep= \"\\t\")\n",
    "train = train.replace(\"?\", np.nan)\n",
    "train[[\"alchemy_category_score\", \"is_news\", \"news_front_page\"]] = train[[\"alchemy_category_score\",\n",
    "                                                                         \"is_news\", \"news_front_page\"]].astype(float)\n",
    "\n",
    "# ==============================================\n",
    "# 3. Prep for modeling\n",
    "# ==============================================\n",
    "\n",
    "# =============================\n",
    "# Train-val-test split\n",
    "# =============================\n",
    "\n",
    "train, val = train_test_split(train, test_size= 0.5, train_size= 0.5, random_state= 234)\n",
    "val, test = train_test_split(val, test_size= 0.2, train_size= 0.3, random_state= 675)\n",
    "print(\"Train data size: \" + str(train.shape))\n",
    "print(\"Validation data size: \" + str(val.shape))\n",
    "print(\"Test data size: \" + str(test.shape))\n",
    "\n",
    "# ==============================================\n",
    "# 4. Explore\n",
    "# ==============================================\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 5. Feature engineering\n",
    "# ==========================================\n",
    "\n",
    "# =============================\n",
    "# 5.1 Variables with missing \n",
    "#     values\n",
    "# =============================\n",
    "\n",
    "train[\"alchemy_category_score\"] = train[\"alchemy_category_score\"].fillna(np.mean(train[\"alchemy_category_score\"]))\n",
    "val[\"alchemy_category_score\"] = val[\"alchemy_category_score\"].fillna(np.mean(train[\"alchemy_category_score\"]))\n",
    "test[\"alchemy_category_score\"] = test[\"alchemy_category_score\"].fillna(np.mean(train[\"alchemy_category_score\"]))\n",
    "train[\"is_news\"] = train[\"is_news\"].fillna(0)\n",
    "train[\"news_front_page\"] = train[\"news_front_page\"].fillna(2)\n",
    "\n",
    "# =============================\n",
    "# 5.2.1 Categorical variables\n",
    "# =============================\n",
    "\n",
    "# impute missing values as separate category\n",
    "train[\"alchemy_category\"] = train[\"alchemy_category\"].fillna(\"_M\")\n",
    "val[\"alchemy_category\"] = val[\"alchemy_category\"].fillna(\"_M\")\n",
    "test[\"alchemy_category\"] = test[\"alchemy_category\"].fillna(\"_M\")\n",
    "\n",
    "# dummy variables for all categories\n",
    "alch_train = pd.get_dummies(train[\"alchemy_category\"], prefix= \"al_cat\")\n",
    "alch_val = pd.get_dummies(val[\"alchemy_category\"], prefix= \"al_cat\")\n",
    "alch_test = pd.get_dummies(test[\"alchemy_category\"], prefix= \"al_cat\")\n",
    "train = train.join(alch_train)\n",
    "val = val.join(alch_val)\n",
    "test= test.join(alch_test)\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 5.2.2 Categorical variables\n",
    "# =============================\n",
    "\n",
    "# Function to create n cross validation folds\n",
    "def createFolds(df, nfolds):\n",
    "    rows = df.shape[0]\n",
    "    folds = list(range(0, nfolds))*int(np.ceil(float(rows)/nfolds))\n",
    "    folds = folds[0:rows]\n",
    "    np.random.shuffle(folds)\n",
    "    folds = pd.Series(folds)\n",
    "    return folds\n",
    "\n",
    "\n",
    "# cross-fold category average\n",
    "def cat_avg_cv(df, target, var, var_out, idcol, nfolds= 4, r1= 0.6, r2= 0.4):\n",
    "    df = df[[idcol, target, var]]\n",
    "    df_out = pd.DataFrame({idcol:[], var_out: []})\n",
    "    folds = createFolds(df= df, nfolds= nfolds)\n",
    "    for f in range(0, nfolds):\n",
    "        print(\"Fold\", f+1, \"of\", nfolds)\n",
    "        tr = df.ix[folds.values == f, [idcol, var]]\n",
    "        va = df.ix[folds.values != f, [var, target]]\n",
    "        fold_mean = va[target].mean()\n",
    "        va = va.groupby(var).agg({target: np.mean})\n",
    "        va = pd.DataFrame({var: va.index, var_out: va[target]})\n",
    "        va[var_out] = r1*va[var_out] + r2*fold_mean\n",
    "        tr = pd.merge(tr, va, on= var, how= \"left\")\n",
    "        tr = tr[[idcol, var_out]]\n",
    "        df_out = df_out.append(tr)\n",
    "\n",
    "    df_out = df_out.fillna(df[target].mean())\n",
    "    return df_out\n",
    "\n",
    "# for training data\n",
    "alch_cat_cv = cat_avg_cv(train, \"label\", \"alchemy_category\", \"alch_avg\", \n",
    "                         idcol= \"urlid\", nfolds= 4, r1= 0.6, r2= 0.4)\n",
    "# use direct means for validation and testing data\n",
    "alch_avg = train.groupby(\"alchemy_category\").label.mean()\n",
    "alch_avg = pd.DataFrame({\"alchemy_category\": alch_avg.index,\n",
    "                         \"alch_avg\": alch_avg.values})\n",
    "# merge with train and val\n",
    "train = pd.merge(train, alch_cat_cv, on= \"urlid\", how= \"left\")\n",
    "val = pd.merge(val, alch_avg, on= \"alchemy_category\", how= \"left\")\n",
    "test = pd.merge(test, alch_avg, on= \"alchemy_category\", how= \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# =============================\n",
    "# 5.3.1 Text features\n",
    "# =============================\n",
    "\n",
    "# TF - COUNT\n",
    "count_dtm = CountVectorizer(min_df= 10,  max_features= 10000, strip_accents= 'unicode',\n",
    "                            analyzer= \"word\", token_pattern= r\"\\w{1,}\", ngram_range=(1, 2), \n",
    "                            binary= True)\n",
    "count_dtm.fit(train[\"boilerplate\"])\n",
    "train_cnt_dtm = count_dtm.transform(train[\"boilerplate\"])\n",
    "val_cnt_dtm = count_dtm.transform(val[\"boilerplate\"])\n",
    "test_cnt_dtm = count_dtm.transform(test[\"boilerplate\"])\n",
    "\n",
    "# TFIDF\n",
    "idf_dtm = TfidfVectorizer(min_df= 10,  max_features= None, strip_accents= \"unicode\",\n",
    "                          analyzer= \"word\", token_pattern= r\"\\w{1,}\", ngram_range=(1, 2), \n",
    "                          use_idf= 1, smooth_idf= 1, sublinear_tf= 1)\n",
    "idf_dtm.fit(train[\"boilerplate\"])\n",
    "train_idf_dtm = idf_dtm.transform(train[\"boilerplate\"])\n",
    "val_idf_dtm = idf_dtm.transform(val[\"boilerplate\"])\n",
    "test_idf_dtm = idf_dtm.transform(test[\"boilerplate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 of 2\n",
      "Fold 1 of 4\n",
      "Fold 2 of 4\n",
      "Fold 3 of 4\n",
      "Fold 4 of 4\n",
      "Run 2 of 2\n",
      "Fold 1 of 4\n",
      "Fold 2 of 4\n",
      "Fold 3 of 4\n",
      "Fold 4 of 4\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# 5.4 Feature selection\n",
    "# =============================\n",
    "\n",
    "def textFeatureSelect(dtm, target, model, tf_object, nfolds= 4, nreps= 2):\n",
    "    feat_imp = pd.DataFrame({\"feat\":[], \"imp\": []})\n",
    "    for r in range(0, nreps):\n",
    "        print(\"Run\", r+1, \"of\", nreps)\n",
    "        folds = createFolds(df= dtm, nfolds= nfolds)\n",
    "        for f in range(0, nfolds):\n",
    "            print(\"Fold\", f+1, \"of\", nfolds)\n",
    "            tr = np.where(folds.values != f)[0]\n",
    "            va = np.where(folds.values == f)[0]\n",
    "            model.fit(dtm[tr,:], target[tr])\n",
    "            imp = pd.DataFrame({\"feat\": tf_object.get_feature_names(), \"imp\": model.coef_[0,:]})\n",
    "            imp[\"imp\"] = imp[\"imp\"].abs()\n",
    "            feat_imp = feat_imp.append(imp)\n",
    "\n",
    "    feat_imp = feat_imp.groupby(\"feat\")\n",
    "    feat_imp = feat_imp.agg({\"imp\": np.mean})\n",
    "    feat_imp = pd.DataFrame({\"feat\": feat_imp.index, \"imp\": feat_imp.imp})\n",
    "    feat_imp = feat_imp.sort_values(by = [\"imp\"], ascending= False)\n",
    "    \n",
    "    return feat_imp\n",
    "\n",
    "# Select best features from text data using logistic regression\n",
    "model = LogisticRegression()\n",
    "imp_text_feat = textFeatureSelect(train_idf_dtm, target= train[\"label\"].values, model= model, \n",
    "                                  tf_object= idf_dtm, nfolds= 4, nreps= 2)\n",
    "\n",
    "# keep the top 1000 ones\n",
    "text_feat = imp_text_feat.ix[0:1000, :]\n",
    "idf_dtm_words = pd.DataFrame({\"feat\": idf_dtm.get_feature_names()})\n",
    "idf_dtm_words = pd.merge(idf_dtm_words, text_feat, on= \"feat\", how= \"inner\", left_index= False, right_index= True)\n",
    "\n",
    "# subset dtm with important features only\n",
    "train_idf_dtm_sub = train_idf_dtm[:, idf_dtm_words.index]\n",
    "val_idf_dtm_sub = val_idf_dtm[:, idf_dtm_words.index]\n",
    "test_idf_dtm_sub = test_idf_dtm[:, idf_dtm_words.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# 5.5 Dimension reduction -- PCA\n",
    "# =============================\n",
    "\n",
    "# fit with 100 components\n",
    "rpca = PCA(n_components = 100, random_state = 8795, svd_solver = \"randomized\")\n",
    "rpca.fit(train_idf_dtm.toarray())\n",
    "\n",
    "# tranform dtm\n",
    "train_idf_dtm_pca = rpca.transform(train_idf_dtm.toarray())\n",
    "val_idf_dtm_pca = rpca.transform(val_idf_dtm.toarray())\n",
    "test_idf_dtm_pca = rpca.transform(test_idf_dtm.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 6. Cross-validation\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 6.1 CV function - 1 \n",
    "# =============================\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "def cross_val_model(train, target, model, nfolds= 4, classify= True):\n",
    "        folds = KFold(n_splits = nfolds, shuffle= True, random_state = np.random.randint(1e4))\n",
    "        if classify:\n",
    "            preds = [np.array([model.fit(train[train_indices], \n",
    "                                         target[train_indices]).predict_proba(train[test_indices])[:,1], \n",
    "                               target[test_indices], test_indices])\n",
    "                     for train_indices, test_indices in folds.split(train)]\n",
    "        else:\n",
    "            preds = [np.array([model.fit(train[train_indices], target[train_indices]).predict(train[test_indices]), \n",
    "                               target[test_indices], test_indices])\n",
    "                     for train_indices, test_indices in folds.split(train)]\n",
    "        \n",
    "        preds = [preds[x].transpose() for x in range(len(preds))]\n",
    "        preds = np.concatenate(preds, axis= 0)\n",
    "        preds = pd.DataFrame(preds, columns= [\"pred\", \"target\", \"index\"])\n",
    "        \n",
    "        return preds\n",
    "\n",
    "    \n",
    "# serial version\n",
    "# nreps = 4\n",
    "# preds = [cross_val_model(tr_dtm, target, model= model, nfolds= 10, classify= True) for r in range(nreps)]\n",
    "\n",
    "def cross_val_model_nrep(train, target, model, nfolds, nreps= 4, classify= True,\n",
    "                         num_cores= 2, parallel= False, verbose= 0):\n",
    "    if parallel:\n",
    "        preds = Parallel(n_jobs= num_cores, \n",
    "                         verbose= verbose)(delayed(cross_val_model)(train, \n",
    "                                                                    target, model, nfolds= 10,\n",
    "                                                                    classify= True) for r in range(nreps))\n",
    "    else:\n",
    "        preds = [cross_val_model(train, target, model= model, nfolds= 10, classify= True) for r in range(nreps)]\n",
    "    preds = pd.DataFrame(np.concatenate(preds, axis= 0), columns= [\"pred\", \"target\", \"index\"])\n",
    "    preds = preds.groupby([\"index\", \"target\"]).agg({\"pred\": np.mean})\n",
    "    preds = preds.reset_index().sort_values(by= [\"index\"])\n",
    "    return preds\n",
    "\n",
    "\n",
    "# =============================\n",
    "# 6.3 CV function - 2\n",
    "# =============================\n",
    "\n",
    "def cvModel(train, test, target, feat, model, idcol, nfolds= 2, nreps= 2, classify= True):\n",
    "    ''' Train a model using k-fold cross validation\n",
    "        and return cross-validated predictions on \n",
    "        training and test data sets\n",
    "    '''\n",
    "    val_pred = pd.DataFrame({idcol: [], \n",
    "                             'target': [],\n",
    "                            'repeat':[], \n",
    "                            'fold': [], \n",
    "                            'pred': []})\n",
    "    test_pred = pd.DataFrame(test[idcol])\n",
    "    \n",
    "    for r in range(0, nreps):\n",
    "        folds = createFolds(train, nfolds)\n",
    "        print(\"Run\", r+1, \"of\", nreps)\n",
    "        for f in range(0, nfolds):\n",
    "            print(\"Fold\", f+1, \"of\", nfolds)\n",
    "            tr = folds.values != f\n",
    "            va = folds.values == f\n",
    "            model.fit(train.ix[tr, feat], target[tr])\n",
    "            scored = pd.DataFrame({idcol: train.ix[va, idcol], 'target': target[va],\n",
    "                                   'repeat': r, 'fold': f})\n",
    "            tmp = 'pred' + str(r) + str(f)                    \n",
    "            if classify:\n",
    "                scored[\"pred\"] = model.predict_proba(train.ix[va, feat])[:,1]\n",
    "                test_pred[tmp] = model.predict_proba(test[feat])[:,1]\n",
    "            else:\n",
    "                scored[\"pred\"] = model.predict(train.ix[va, feat])\n",
    "                test_pred[tmp] = model.predict(test[feat])\n",
    "            val_pred = val_pred.append(scored)\n",
    "            \n",
    "    val_pred = val_pred.groupby(idcol)\n",
    "    val_pred = val_pred.agg({'target': np.mean,\n",
    "                             'pred': np.mean})\n",
    "    val_pred = val_pred.reset_index()\n",
    "    val_pred = val_pred.sort_values(by= [idcol])\n",
    "    test_pred[\"pred\"] = test_pred.ix[:,1:(test_pred.shape[1]+1)].mean(axis= 1)\n",
    "    test_pred = test_pred[[idcol, \"pred\"]]\n",
    "    return val_pred, test_pred\n",
    "\n",
    "\n",
    "# merge pca components with main\n",
    "train_idf_dtm_pca = pd.DataFrame(train_idf_dtm_pca)\n",
    "val_idf_dtm_pca = pd.DataFrame(val_idf_dtm_pca)\n",
    "test_idf_dtm_pca = pd.DataFrame(test_idf_dtm_pca)\n",
    "\n",
    "# give string names to columns\n",
    "train_idf_dtm_pca.columns = [\"pca\" + str(i) for i in train_idf_dtm_pca.columns]\n",
    "val_idf_dtm_pca.columns = [\"pca\" + str(i) for i in val_idf_dtm_pca.columns]\n",
    "test_idf_dtm_pca.columns = [\"pca\" + str(i) for i in test_idf_dtm_pca.columns]\n",
    "\n",
    "# generate index to merge on\n",
    "train_idf_dtm_pca.index = train.index\n",
    "val_idf_dtm_pca.index = val.index\n",
    "test_idf_dtm_pca.index = test.index\n",
    "\n",
    "# join\n",
    "train_pca = train.join(train_idf_dtm_pca)\n",
    "val_pca = val.join(val_idf_dtm_pca)\n",
    "test_pca = test.join(test_idf_dtm_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    2.4s remaining:    2.4s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    2.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    2.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 of 2\n",
      "Fold 1 of 4\n",
      "Fold 2 of 4\n",
      "Fold 3 of 4\n",
      "Fold 4 of 4\n",
      "Run 2 of 2\n",
      "Fold 1 of 4\n",
      "Fold 2 of 4\n",
      "Fold 3 of 4\n",
      "Fold 4 of 4\n",
      "Run 1 of 2\n",
      "Fold 1 of 4\n",
      "Fold 2 of 4\n",
      "Fold 3 of 4\n",
      "Fold 4 of 4\n",
      "Run 2 of 2\n",
      "Fold 1 of 4\n",
      "Fold 2 of 4\n",
      "Fold 3 of 4\n",
      "Fold 4 of 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   3 out of   8 | elapsed:    5.1s remaining:    8.5s\n",
      "[Parallel(n_jobs=4)]: Done   5 out of   8 | elapsed:   10.1s remaining:    6.1s\n",
      "[Parallel(n_jobs=4)]: Done   8 out of   8 | elapsed:   10.1s finished\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 6.x Cross-validated models\n",
    "# ==========================================\n",
    "\n",
    "# Logistic regression\n",
    "logReg7_cv = LogisticRegression()\n",
    "logReg7_train = cross_val_model_nrep(train= train_idf_dtm, target= train.label.values, model= logReg7_cv, \n",
    "                                     nfolds= 10, nreps= 4, classify= True, num_cores= 4, parallel= True,\n",
    "                                     verbose= 5)\n",
    "logReg7_val= pd.Series(logReg7_cv.fit(train_idf_dtm, train.label.values).predict_proba(test_idf_dtm)[:,1])\n",
    "\n",
    "\n",
    "# gbm\n",
    "feat = list(train_idf_dtm_pca.columns.values)\n",
    "gbm4 = GradientBoostingClassifier(loss = \"deviance\", n_estimators= 100,\n",
    "                                  max_depth= 2, min_samples_split= 10, min_samples_leaf= 10,\n",
    "                                  subsample= 0.75, max_features= None, verbose= 0)\n",
    "gbm4_train, gbm4_val = cvModel(train_pca, test_pca, target= train_pca[\"label\"], model= gbm4, feat= feat, \n",
    "                               idcol= \"urlid\", nfolds= 4, nreps= 2, classify= True)\n",
    "\n",
    "\n",
    "# random forests\n",
    "feat = [\"avglinksize\", \"commonlinkratio_1\", \"commonlinkratio_2\", \"commonlinkratio_3\", \"commonlinkratio_4\", \n",
    "        \"compression_ratio\", \"embed_ratio\", \"framebased\", \"frameTagRatio\", \"hasDomainLink\", \"html_ratio\",\n",
    "        \"image_ratio\", \"lengthyLinkDomain\", \"linkwordscore\", \"non_markup_alphanum_characters\", \"numberOfLinks\", \n",
    "        \"numwords_in_url\", \"parametrizedLinkRatio\",\"spelling_errors_ratio\", \"alchemy_category_score\", \n",
    "        \"alch_avg\"]\n",
    "feat.extend(list(train_idf_dtm_pca.columns.values))\n",
    "\n",
    "rf2 = RandomForestClassifier(n_estimators= 100, min_samples_split= 5, random_state= 9876,\n",
    "                             max_features= 15, verbose= 0)\n",
    "rf2_train, rf2_val = cvModel(train_pca, test_pca, target= train_pca[\"label\"], model= rf2, feat= feat, \n",
    "                               idcol= \"urlid\", nfolds= 4, nreps= 2, classify= True)\n",
    "\n",
    "# ERT\n",
    "ert1_cv = ExtraTreesClassifier(n_estimators= 25, min_samples_split= 10, random_state= 134,\n",
    "                               max_features= 55, min_samples_leaf= 5, verbose= 0)\n",
    "\n",
    "ert1_train = cross_val_model_nrep(train= train_idf_dtm_sub, target= train.label.values, model= ert1_cv, \n",
    "                                     nfolds= 3, nreps= 8, classify= True, num_cores= 4, parallel= True,\n",
    "                                     verbose= 5)\n",
    "# fit on entire training data to check on validation data\n",
    "ert1_val= pd.Series(ert1_cv.fit(train_idf_dtm_sub, train.label.values).predict_proba(test_idf_dtm_sub)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic cv score:0.87528\n",
      "GBM cv score:0.86904\n",
      "RF cv score: 0.86915\n",
      "ERT cv score: 0.87807\n",
      " --- \n",
      "Logistic val score:0.88871\n",
      "GBM val score:0.89272\n",
      "RF val score: 0.89157\n",
      "ERT val score: 0.88481\n"
     ]
    }
   ],
   "source": [
    "# Current Scores\n",
    "# CV scores\n",
    "print(\"Logistic cv score:\" + str(np.round(roc_auc_score(train[\"label\"], logReg7_train.pred.values), 5)))\n",
    "print(\"GBM cv score:\" + str(np.round(roc_auc_score(gbm4_train.target.values, gbm4_train.pred.values), 5)))\n",
    "print(\"RF cv score: \" + str(np.round(roc_auc_score(rf2_train.target.values, rf2_train.pred.values), 5)))\n",
    "print(\"ERT cv score: \" + str(np.round(roc_auc_score(train[\"label\"], ert1_train.pred.values), 5)))\n",
    "\n",
    "print(\" --- \")\n",
    "\n",
    "# Test scores\n",
    "print(\"Logistic val score:\"  + str(np.round(roc_auc_score(test[\"label\"], logReg7_val.values), 5)))\n",
    "print(\"GBM val score:\" + str(np.round(roc_auc_score(test_pca[\"label\"], gbm4_val.pred.values), 5)))\n",
    "print(\"RF val score: \" + str(np.round(roc_auc_score(test_pca[\"label\"], rf2_val.pred.values), 5)))\n",
    "print(\"ERT val score: \"  + str(np.round(roc_auc_score(test[\"label\"], ert1_val.values), 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble\n",
    "train[\"sort_col\"] = range(0, train.shape[0])\n",
    "gbm4_train = pd.merge(gbm4_train, train[[\"urlid\", \"sort_col\"]], on= \"urlid\", how= 'left').sort_values(by= [\"sort_col\"])\n",
    "rf2_train = pd.merge(rf2_train, train[[\"urlid\", \"sort_col\"]], on= \"urlid\", how= 'left').sort_values(by= [\"sort_col\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 of 4\n",
      "Fold 1 of 4\n",
      "Fold 2 of 4\n",
      "Fold 3 of 4\n",
      "Fold 4 of 4\n",
      "Run 2 of 4\n",
      "Fold 1 of 4\n",
      "Fold 2 of 4\n",
      "Fold 3 of 4\n",
      "Fold 4 of 4\n",
      "Run 3 of 4\n",
      "Fold 1 of 4\n",
      "Fold 2 of 4\n",
      "Fold 3 of 4\n",
      "Fold 4 of 4\n",
      "Run 4 of 4\n",
      "Fold 1 of 4\n",
      "Fold 2 of 4\n",
      "Fold 3 of 4\n",
      "Fold 4 of 4\n",
      " --- \n",
      "Ensemble cv score: 0.88024\n",
      "Ensemble val score: 0.89459\n"
     ]
    }
   ],
   "source": [
    "# Ensemble\n",
    "train_ens = pd.DataFrame({\"urlid\": gbm4_train.urlid.values, \"label\": gbm4_train.target.values, \n",
    "                          \"logReg7\": logReg7_train.pred.values,\n",
    "                          \"gbm4\": gbm4_train.pred.values, \"rf2\": rf2_train.pred.values,\n",
    "                         \"ert1\": ert1_train.pred.values})\n",
    "val_ens = pd.DataFrame({\"urlid\": gbm4_val.urlid.values, \"gbm4\": gbm4_val.pred.values,\n",
    "                        \"logReg7\": logReg7_val.values, \"rf2\": rf2_val.pred.values,\n",
    "                       \"ert1\": ert1_val.values})\n",
    "val_ens = pd.merge(val_ens, test[[\"urlid\", \"label\"]], on= \"urlid\")\n",
    "\n",
    "\n",
    "# logistic regression\n",
    "logReg8_ens = LogisticRegression()\n",
    "feat = [\"gbm4\", \"logReg7\", \"rf2\", \"ert1\"]\n",
    "logReg8_ens_cv, logReg8_ens_val = cvModel(train_ens, val_ens, target= train_ens[\"label\"], model= logReg8_ens, \n",
    "                                          feat= feat, idcol= \"urlid\", nfolds= 4, nreps= 4, classify= True)\n",
    "\n",
    "# auc score\n",
    "print(\" --- \")\n",
    "print(\"Ensemble cv score: \" + str(np.round(roc_auc_score(logReg8_ens_cv.target.values, logReg8_ens_cv.pred.values), 5)))\n",
    "print(\"Ensemble val score: \" + str(np.round(roc_auc_score(val_ens[\"label\"], logReg8_ens_val.pred.values), 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple avg val: 0.89574\n"
     ]
    }
   ],
   "source": [
    "simple_avg_val = np.mean([val_ens.gbm4.values, val_ens.rf2.values, val_ens.logReg7.values,\n",
    "                         val_ens.ert1.values], axis= 0)\n",
    "print(\"Simple avg val: \" + str(np.round(roc_auc_score(val_ens[\"label\"], simple_avg_val), 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD cv score:0.90495\n",
      "SGD val score:0.8864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:    0.4s remaining:    0.4s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:    0.4s finished\n"
     ]
    }
   ],
   "source": [
    "# Exercise\n",
    "\n",
    "# sgd\n",
    "sgd1_cv = SGDClassifier(loss= \"log\", penalty= \"l2\", n_iter= 30, random_state= 34)\n",
    "sgd1_cv_train = cross_val_model_nrep(train= train_idf_dtm_sub, target= train.label.values, model= sgd1_cv, \n",
    "                                     nfolds= 10, nreps= 4, classify= True, num_cores= 4, parallel= True,\n",
    "                                     verbose= 5)\n",
    "sgd1_cv_val= pd.Series(logReg7_cv.fit(train_idf_dtm_sub, train.label.values).predict_proba(test_idf_dtm_sub)[:,1])\n",
    "\n",
    "print(\"SGD cv score:\" + str(np.round(roc_auc_score(train[\"label\"], sgd1_cv_train.pred.values), 5)))\n",
    "print(\"SGD val score:\"  + str(np.round(roc_auc_score(test[\"label\"], sgd1_cv_val.values), 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 of 4\n",
      "Fold 1 of 4\n",
      "Fold 2 of 4\n",
      "Fold 3 of 4\n",
      "Fold 4 of 4\n",
      "Run 2 of 4\n",
      "Fold 1 of 4\n",
      "Fold 2 of 4\n",
      "Fold 3 of 4\n",
      "Fold 4 of 4\n",
      "Run 3 of 4\n",
      "Fold 1 of 4\n",
      "Fold 2 of 4\n",
      "Fold 3 of 4\n",
      "Fold 4 of 4\n",
      "Run 4 of 4\n",
      "Fold 1 of 4\n",
      "Fold 2 of 4\n",
      "Fold 3 of 4\n",
      "Fold 4 of 4\n",
      " --- \n",
      "Ensemble cv score: 0.91225\n",
      "Ensemble val score: 0.88463\n"
     ]
    }
   ],
   "source": [
    "# Ensemble\n",
    "train_ens = pd.DataFrame({\"urlid\": gbm4_train.urlid.values, \"label\": gbm4_train.target.values, \n",
    "                          \"logReg7\": logReg7_train.pred.values,\n",
    "                          \"gbm4\": gbm4_train.pred.values, \"rf2\": rf2_train.pred.values,\n",
    "                         \"ert1\": ert1_train.pred.values, \"x1\":sgd1_cv_train.pred.values})\n",
    "val_ens = pd.DataFrame({\"urlid\": gbm4_val.urlid.values, \"gbm4\": gbm4_val.pred.values,\n",
    "                        \"logReg7\": logReg7_val.values, \"rf2\": rf2_val.pred.values,\n",
    "                       \"ert1\": ert1_val.values, \"x1\":sgd1_cv_val.values})\n",
    "val_ens = pd.merge(val_ens, test[[\"urlid\", \"label\"]], on= \"urlid\")\n",
    "\n",
    "\n",
    "# logistic regression\n",
    "logReg8_ens = LogisticRegression()\n",
    "feat = [\"gbm4\", \"logReg7\", \"rf2\", \"ert1\", \"x1\"]\n",
    "logReg8_ens_cv, logReg8_ens_val = cvModel(train_ens, val_ens, target= train_ens[\"label\"], model= logReg8_ens, \n",
    "                                          feat= feat, idcol= \"urlid\", nfolds= 4, nreps= 4, classify= True)\n",
    "\n",
    "# auc score\n",
    "print(\" --- \")\n",
    "print(\"Ensemble cv score: \" + str(np.round(roc_auc_score(logReg8_ens_cv.target.values, logReg8_ens_cv.pred.values), 5)))\n",
    "print(\"Ensemble val score: \" + str(np.round(roc_auc_score(val_ens[\"label\"], logReg8_ens_val.pred.values), 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple avg val: 0.89507\n"
     ]
    }
   ],
   "source": [
    "simple_avg_val = np.mean([val_ens.gbm4.values, val_ens.rf2.values, val_ens.logReg7.values,\n",
    "                         val_ens.ert1.values, val_ens.x1.values], axis= 0)\n",
    "print(\"Simple avg val: \" + str(np.round(roc_auc_score(val_ens[\"label\"], simple_avg_val), 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
