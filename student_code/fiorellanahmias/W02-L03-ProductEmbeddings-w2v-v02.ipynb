{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIT-GSL Uruguay \n",
    "\n",
    "## January 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week - 2 | Lesson - 03 \n",
    "# NLP: Product embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. After introducing the concept of embeddings through NLP, we extend the notion of embeddings to other settings\n",
    "2. Note that embeddings are dense continuous representations for discrete, sparse tokens - this makes embeddings widely applicable\n",
    "3. We will use the concept of embeddings to understand the world of e-commerce better "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec and its Applications to Market-Basket Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instacart Grocery Dataset\n",
    "\n",
    "### Source: https://www.instacart.com/datasets/grocery-shopping-2017\n",
    "\n",
    "1. Instacart is an online grocery delivery service\n",
    "2. They have made available 3M grocery orders for over 200K users\n",
    "3. They provide between 4 to 100 orders for each user and each order contains the sequence of products purchased\n",
    "4. We also have a brief description of the products\n",
    "\n",
    "### Overview:\n",
    "1. We will use this data to build an understanding of word embeddings and investigate their application to downstream tasks\n",
    "2. For this purpose, we will consider each purchase basket to be a sentence with an unordered sequence of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/pool001/madhavk/gsl-uruguay/W-02-NLP/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fa8f3ca2b504>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/pool001/madhavk/gsl-uruguay/W-02-NLP/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/pool001/madhavk/gsl-uruguay/W-02-NLP/'"
     ]
    }
   ],
   "source": [
    "# ==============================================\n",
    "# 0. Module imports\n",
    "# ==============================================\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 100\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "import csv\n",
    "\n",
    "# w2v\n",
    "import gensim\n",
    "\n",
    "# text processing\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import ToktokTokenizer\n",
    "import string\n",
    "import re # regular expressions\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# T-Sne\n",
    "#import umap\n",
    "#from openTSNE import TSNE, TSNEEmbedding, affinity, initialization\n",
    "#from openTSNE import initialization\n",
    "#from openTSNE.callbacks import ErrorLogger\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "# utils\n",
    "from sklearn import utils\n",
    "\n",
    "\n",
    "# parallel processing\n",
    "import multiprocessing\n",
    "from joblib import delayed, Parallel\n",
    "\n",
    "# time code\n",
    "import time\n",
    "\n",
    "# 2-d visualiztion\n",
    "%matplotlib inline\n",
    "from ggplot import *\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "os.chdir(\"/pool001/madhavk/gsl-uruguay/W-02-NLP/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# set directories\n",
    "# =========================================================\n",
    "\n",
    "import os\n",
    "EC2 = True  # If using EC2 (for data directory/paths)\n",
    "\n",
    "# Select path based off of local or remote\n",
    "if not EC2:\n",
    "    wd = \"/pool001/madhavk/gsl-uruguay/W-02-NLP/\"\n",
    "else:\n",
    "    wd = \"/home/ubuntu/machine_learning_aws/\"\n",
    "os.chdir(wd)\n",
    "\n",
    "EC2 = True  # If using EC2 (for data directory/paths)\n",
    "if not EC2:\n",
    "    # raw data\n",
    "    raw_data_dir = \"nlp-data/in-grocery/instacart_2017_05_01/\"\n",
    "    # processed data\n",
    "    process_dir = \"nlp-data/in-grocery/prepared-data/\"  \n",
    "else:\n",
    "    # raw data\n",
    "    raw_data_dir = \"data/in-grocery/instacart_2017_05_01\"\n",
    "    # processed data\n",
    "    process_dir = \"data/in-grocery/prepared-data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# processed files\n",
    "# =========================================================\n",
    "\n",
    "os.listdir(process_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# global parameters\n",
    "# =========================================================\n",
    "\n",
    "# show entire value of cell in pandas\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "# number of cpus\n",
    "cpus = multiprocessing.cpu_count()\n",
    "f\"Number of CPUs: {cpus}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import grocery data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Order level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# order-level data\n",
    "# =========================================================\n",
    "\n",
    "orders_wide = pd.read_csv(process_dir + \"all-orders-wide-v1.csv\")\n",
    "# This data set has one row per order with the products ordered in the product_id column. \n",
    "# Products are separated by space.\n",
    "print(orders_wide.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(orders_wide.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# orders meta-data\n",
    "# =========================================================\n",
    "\n",
    "orders_meta = pd.read_csv(process_dir + \"orders-split-v1.csv\")\n",
    "# This dataset includes the meta data for each order, i.e., the user who ordered it, order day of the week, order time\n",
    "print(orders_meta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(orders_meta.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Merge train-val-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# merge orders-wide and orders-meta\n",
    "# =========================================================\n",
    "\n",
    "orders_wide.drop(labels = \"eval\", axis = 1, inplace = True)\n",
    "orders_wide = pd.merge(orders_wide, # data - 1\n",
    "                       orders_meta[[\"order_id\", \"user_id\", \"eval\"]], # data - 2\n",
    "                       on = \"order_id\", # merge key\n",
    "                       how = \"left\") # left join\n",
    "print(orders_wide.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(orders_wide.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# eval-set distribution\n",
    "# =========================================================\n",
    "\n",
    "orders_wide[\"eval\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear some space\n",
    "del orders_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Import product info data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = pd.read_csv(process_dir + \"products-merged-v1.csv\")\n",
    "print(products.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(products.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================================\n",
    "# top departments\n",
    "#===============================================\n",
    "products[\"department\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-class exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#===============================================\n",
    "# top aisles\n",
    "#===============================================\n",
    "# can you figure out which aisles host the most number of products?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# long-form of orders\n",
    "orders_long = pd.read_csv(process_dir + \"all-orders-long-v1.csv\")\n",
    "print(orders_long.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top-20 observations in the data frame\n",
    "display(orders_long.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Most frequently purchased products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most frequently bought products\n",
    "most_freq_purchased = pd.DataFrame(orders_long[\"product_id\"].value_counts()) # count the number times each product-id appears in data frame\n",
    "most_freq_purchased.reset_index(drop = False, inplace = True) # complying with pandas indexing \n",
    "most_freq_purchased.columns = [\"product_id\", \"freq\"] # assign column names\n",
    "display(most_freq_purchased.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with product info from the meta dataset\n",
    "most_freq_purchased = pd.merge(most_freq_purchased, \n",
    "                               products, \n",
    "                               on = \"product_id\", \n",
    "                               how = \"left\")\n",
    "display(most_freq_purchased.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-class exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can you figure out the least popular products?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# most popular aisle\n",
    "aisle = most_freq_purchased.groupby(\"aisle\").agg({\"freq\": \"sum\"})\n",
    "aisle = aisle.reset_index(drop = False, inplace = False).sort_values(\"freq\", ascending = False)\n",
    "display(aisle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-class exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can you figure out the most popular department, i.e., the department from where most products are purchased?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# most purchased department"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Co-purchased products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# Generate co-purchase matrix\n",
    "# ===========================================\n",
    "\n",
    "def CoPurchaseMatrix(orders_wide, product_info = True):\n",
    "    '''\n",
    "    orders_wide is order-level data with one row per order\n",
    "    '''\n",
    "    count_vec = CountVectorizer(ngram_range = (1,1), binary = True, \n",
    "                            token_pattern = \"\\\\b\\\\w+\\\\b\") # sku counts\n",
    "    pur_mat = count_vec.fit_transform(orders_wide[\"product_id\"])\n",
    "    co_pur_mat = (pur_mat.T * pur_mat) #copurchase matrix\n",
    "    co_pur_mat.setdiag(0) # set diagonal to 0\n",
    "    co_pur_mat_df = pd.DataFrame(co_pur_mat.todense()) # convert to data frame\n",
    "    co_pur_mat_df.index = count_vec.vocabulary_ # row names from sku-ids\n",
    "    co_pur_mat_df.columns = count_vec.vocabulary_ # column names from sku-ids\n",
    "    co_pur_mat_df = co_pur_mat_df.where(np.triu(np.ones(co_pur_mat_df.shape)).astype(np.bool)) # consider the uppre tri\n",
    "    co_pur_mat_df = co_pur_mat_df.stack().reset_index() # melt to sku-1 and sku-2 per row\n",
    "    co_pur_mat_df.columns = [\"product_id_1\", \"product_id_2\", \"copur\"]\n",
    "    co_pur_mat_df = co_pur_mat_df.loc[co_pur_mat_df[\"copur\"] > 0, :] # subset for copur > 0\n",
    "    co_pur_mat_df_top = co_pur_mat_df.sort_values([\"product_id_1\", \"copur\"], ascending = False)\n",
    "    co_pur_mat_df_top = co_pur_mat_df_top.drop_duplicates([\"product_id_1\"], keep = \"first\")\n",
    "    co_pur_mat_df_top[\"product_id_1\"] = co_pur_mat_df_top[\"product_id_1\"].astype(int) # fix data types\n",
    "    co_pur_mat_df_top[\"product_id_2\"] = co_pur_mat_df_top[\"product_id_2\"].astype(int) # fix data types\n",
    "    co_pur_mat_df_top = co_pur_mat_df_top.sort_values([\"copur\"], ascending = False).reset_index(drop = True)\n",
    "    if product_info:\n",
    "        co_pur_mat_df_top = pd.merge(co_pur_mat_df_top, products, how = \"left\", \n",
    "                                     left_on = \"product_id_1\", right_on = \"product_id\")\n",
    "        co_pur_mat_df_top.drop(\"product_id\", axis = 1, inplace = True)\n",
    "        co_pur_mat_df_top = pd.merge(co_pur_mat_df_top, products, how = \"left\",\n",
    "                                     left_on = \"product_id_2\", right_on = \"product_id\", \n",
    "                                     suffixes = [\"_1\", \"_2\"])\n",
    "        co_pur_mat_df_top.drop(\"product_id\", axis = 1, inplace = True)\n",
    "        col_order = ['product_id_1', 'product_id_2', 'copur', 'product_name_1', 'product_name_2',\n",
    "                             'aisle_1', 'aisle_2', 'department_1', 'department_2', \n",
    "                             'aisle_id_1', 'aisle_id_2', 'department_id_1', 'department_id_2']\n",
    "        co_pur_mat_df_top = co_pur_mat_df_top[col_order]\n",
    "    return(co_pur_mat_df_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate copurchases\n",
    "copur = CoPurchaseMatrix(orders_wide = orders_wide, product_info = True)\n",
    "print(copur.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(copur[[\"product_name_1\", \"product_name_2\", \"copur\", \"aisle_1\", \"aisle_2\"]].head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Split train-val-test datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================================\n",
    "# split train-val-test\n",
    "#===============================================\n",
    "\n",
    "train = orders_wide.loc[orders_wide[\"eval\"].isin([\"prior\", \"train\"]), :]\n",
    "val = orders_wide.loc[orders_wide[\"eval\"] == \"val\", :]\n",
    "test = orders_wide.loc[orders_wide[\"eval\"] == \"test\", :]\n",
    "print(\"train size:\", train.shape)\n",
    "print(\"val size:\", val.shape)\n",
    "print(\"test size:\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear more space\n",
    "# del orders_wide, orders_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Random sample for faster processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================================\n",
    "# randomly sample training data\n",
    "#===============================================\n",
    "\n",
    "sample_size = 1000000\n",
    "train = train.sample(n = sample_size)\n",
    "train = train.reset_index(drop = True)\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Tokenize sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start pool process for parallel procressing\n",
    "pool = multiprocessing.Pool(processes = cpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text processing\n",
    "from nltk import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training sentences\n",
    "t0 = time.time()\n",
    "train_orders = pool.map(word_tokenize, train[\"product_id\"])\n",
    "t1 = time.time()\n",
    "print(len(train_orders))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# time taken\n",
    "print(f\"Time Taken: {t1 - t0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training orders\n",
    "train_orders[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation sentences\n",
    "t0 = time.time()\n",
    "val_orders = pool.map(word_tokenize, val[\"product_id\"])\n",
    "t1 = time.time()\n",
    "print(len(val_orders))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-class exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what do the first three validation orders look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can you similarly tokenize the test sentences?\n",
    "#print(len(test_orders))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Word2Vec sample model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Define and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================================\n",
    "# define and train model\n",
    "#===============================================\n",
    "\n",
    "w2v_1 = gensim.models.Word2Vec(sentences = train_orders,\n",
    "                               workers = multiprocessing.cpu_count(),\n",
    "                               seed = 1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================================\n",
    "# vocabulary\n",
    "#===============================================\n",
    "\n",
    "# vocabulary length\n",
    "f\"Vocab length: {len(w2v_1.wv.vocab)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample vocabulary\n",
    "list(w2v_1.wv.vocab.keys())[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of corpus\n",
    "print(w2v_1.corpus_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of iterations\n",
    "w2v_1.iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================================\n",
    "# product vectors\n",
    "#===============================================\n",
    "\n",
    "# enter product-id\n",
    "prod_id = \"1\"\n",
    "print(w2v_1[prod_id].shape)\n",
    "print(\"------------\")\n",
    "print(\"------------\")\n",
    "print(w2v_1[prod_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Update embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================================\n",
    "# update model weights\n",
    "#===============================================\n",
    "w2v_1.train(sentences = train_orders, total_examples = w2v_1.corpus_count, epochs = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Inspect model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================================\n",
    "# upadted product vectors\n",
    "#===============================================\n",
    "\n",
    "# enter product-id\n",
    "prod_id = \"1\"\n",
    "print(w2v_1[prod_id].shape)\n",
    "print(\"------------\")\n",
    "print(\"------------\")\n",
    "print(w2v_1[prod_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Improve W2V model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================================\n",
    "# model parameters\n",
    "#===============================================\n",
    "\n",
    "# size of embedding matrix\n",
    "emb_size = 100\n",
    "\n",
    "# context window size\n",
    "cxt_window = 10\n",
    "\n",
    "# batch size for gradient update\n",
    "batch_size = 10000\n",
    "\n",
    "# down-sample high frequency words\n",
    "hfs = 0.001\n",
    "\n",
    "# learning rate\n",
    "lr = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================================\n",
    "# define model\n",
    "#===============================================\n",
    "\n",
    "w2v_1 = gensim.models.Word2Vec(sentences = train_orders,\n",
    "                               size = emb_size, # number of columns in embedding matrix\n",
    "                               hs = 1, # hierarchical softmax\n",
    "                               negative = 0, # negative sampling\n",
    "                               window = cxt_window, # context window\n",
    "                               min_count = 1, # minimum frequency count\n",
    "                               batch_words = batch_size, # batch size for update\n",
    "                               alpha = lr, # learning rate\n",
    "                               sample = hfs, # down sample high frequency words \n",
    "                               workers = cpus,\n",
    "                               seed = 1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Score on validation and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================================\n",
    "# Score model to get log-likelihood\n",
    "#===============================================\n",
    "\n",
    "def ScoreW2V(test_sent, model, normalize = True, avg_over_sent = True):\n",
    "    test_score = model.score(test_sent, total_sentences = len(test_sent), \n",
    "                             chunksize = 100,\n",
    "                             queue_factor = 2,\n",
    "                             report_delay = 1)\n",
    "    if normalize:\n",
    "        test_score = [test_score[x]/len(test_sent[x]) for x in range(len(test_sent))]\n",
    "    else:\n",
    "        test_score = list(test_score)\n",
    "    if avg_over_sent:\n",
    "        test_score = np.mean(test_score)\n",
    "    return test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_1_val = ScoreW2V(test_sent = val_orders, model = w2v_1, normalize = True, avg_over_sent = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w2v_1_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Similar products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================================\n",
    "# similar products\n",
    "#===============================================\n",
    "\n",
    "# sample product\n",
    "prod_id = \"10\"\n",
    "\n",
    "# product info for sample product\n",
    "display(products.loc[products[\"product_id\"].isin([prod_id]), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================================\n",
    "# similarity from model\n",
    "#===============================================\n",
    "\n",
    "w2v_1.wv.most_similar(prod_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#===============================================\n",
    "# lookup product info\n",
    "#===============================================\n",
    "most_similar_prods = [y[0] for y in w2v_1.wv.most_similar(positive = prod_id)]\n",
    "most_similar_prods = products.loc[products[\"product_id\"].isin(most_similar_prods), :]\n",
    "display(most_similar_prods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================================\n",
    "# most dissimilar products\n",
    "#===============================================\n",
    "most_dissimilar_prods = [y[0] for y in w2v_1.wv.most_similar(negative = [prod_id])]\n",
    "most_dissimilar_prods = products.loc[products[\"product_id\"].isin(most_dissimilar_prods), :]\n",
    "display(most_dissimilar_prods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-class exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the most similar products to \n",
    "prod_id = \"100\"\n",
    "\n",
    "# product info for sample product\n",
    "display(products.loc[products[\"product_id\"].isin([prod_id]), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which products are most similar to peanut better and strawberry jam sandwich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what about the most dissimilar products?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Product maps using t-sne "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Extract all product vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================================\n",
    "# product vectors\n",
    "#===============================================\n",
    "\n",
    "items = [key for key in w2v_1.wv.vocab.keys()] # all product-ids\n",
    "word_vec = [list(w2v_1[item]) for item in items] # \"word\" vector for each product id\n",
    "word_vec = np.array(word_vec) # convert to array\n",
    "word_vec_df = pd.DataFrame(word_vec) # convert to data frame\n",
    "w2v_vec_names = [\"wv\" + str(x + 1) for x in range(word_vec_df.shape[1])] # column names\n",
    "word_vec_df.columns = w2v_vec_names # assign column names\n",
    "word_vec_df[\"product_id\"] = items # include product id in data frame\n",
    "word_vec_df[\"product_id\"] = word_vec_df[\"product_id\"].astype(int) # convert to type integer for later merge\n",
    "word_vec_df = word_vec_df[[\"product_id\"] + w2v_vec_names] # re-order columns\n",
    "print(word_vec_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(word_vec_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge product vectors with product info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================================\n",
    "# merge relevant columns\n",
    "#===============================================\n",
    "\n",
    "# relevant columns from product info\n",
    "prod_info_cols = [\"product_id\", \"product_name\", \"department\", \"aisle\"]\n",
    "\n",
    "# merge\n",
    "word_vec_df = pd.merge(products[prod_info_cols], word_vec_df, on = \"product_id\", how = \"inner\")\n",
    "print(word_vec_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(word_vec_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Fit t-sne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================\n",
    "# fit t-sne\n",
    "# =================================================\n",
    "tsne = TSNE(n_components = 2, verbose = 1, perplexity = 35, n_iter = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit\n",
    "t0 = time.time()\n",
    "tsne_fit = tsne.fit_transform(word_vec_df[w2v_vec_names])\n",
    "t1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time taken\n",
    "f\"Time Taken: {t1 - t0}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-sne component data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================================\n",
    "# create t-sne data frame\n",
    "#===============================================\n",
    "\n",
    "tsne_df = word_vec_df[[\"product_name\", \"department\", \"aisle\"]]\n",
    "\n",
    "# extract t-sne dimensions\n",
    "tsne_df[\"x_tsne\"] = tsne_fit[:,0]\n",
    "tsne_df[\"y_tsne\"] = tsne_fit[:,1]\n",
    "print(tsne_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot with ggplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================================\n",
    "# subset data for plot\n",
    "#===============================================\n",
    "\n",
    "# select only top departments\n",
    "select_dept = [\"produce\", \"babies\", \"beverages\"]\n",
    "tsne_plot_df = tsne_df.loc[tsne_df[\"department\"].isin(select_dept), :]\n",
    "print(tsne_plot_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_plot = ggplot(tsne_plot_df, aes(x = \"x_tsne\", y = \"y_tsne\", color = \"department\") ) \\\n",
    "        + geom_point(size = 70, alpha = 0.5) \\\n",
    "        + ggtitle(\"T-sne on product vectors\") \\\n",
    "        + xlab(\" \") + ylab(\" \")\n",
    "tsne_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-class exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning embeddings using Skip-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We will now use another method to train embeddings called skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================================\n",
    "# model parameters\n",
    "#===============================================\n",
    "\n",
    "# size of embedding matrix\n",
    "emb_size  # set between 50-100\n",
    "\n",
    "# context window size\n",
    "cxt_window # set between 2-10 \n",
    "\n",
    "# batch size for gradient update\n",
    "batch_size # set between 2000 to 10000\n",
    "\n",
    "# learning rate\n",
    "lr # set between 0.001 to 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================================\n",
    "# define and train model\n",
    "#===============================================\n",
    "\n",
    "t0 = time.time()\n",
    "### Write model code here\n",
    "t1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# time taken\n",
    "f\"Time Taken: {t1 - t0}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the most similar products to \n",
    "prod_id = \"1000\"\n",
    "\n",
    "# product info for sample product\n",
    "display(products.loc[products[\"product_id\"].isin([prod_id]), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
