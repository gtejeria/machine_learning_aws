{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification: Now with Text :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle StumbleUpon Competition\n",
    "\n",
    "https://www.kaggle.com/c/stumbleupon\n",
    "\n",
    "** Competition **: \n",
    "1. Some web pages, such as news articles or seasonal recipes, are only relevant for a short period of time. Others continue to be important for a long time.\n",
    "2. The goal is to identify pages which pages will be relevant for a short span of time, and which will be relevant for a long span on time and are thus considered \"evergreen\". \n",
    "\n",
    "** Evaluation **: Area under the curve (AUC) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Python Modules \n",
    "================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick hack to fix import path\n",
    "# import sys; sys.path.append('/Users/julianalverio/code/conda/envs/sac/lib/python3.6/site-packages/')\n",
    "\n",
    "# data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# plots\n",
    "%matplotlib inline\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "\n",
    "# classification algorithms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "# dimensionality reduction\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# cross-validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection\n",
    "\n",
    "# text features\n",
    "import re\n",
    "from nltk import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# model evaluation\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "os.chdir(os.path.join(\"..\", \"data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/machine_learning_aws/data'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Import\n",
    "We will download this data directly from Dropbox, and then read our table using the **pandas** library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-01-15 20:54:25--  https://www.dropbox.com/s/10ch2yhfk8tyfri/train.tsv?dl=0\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.6.1, 2620:100:601c:1::a27d:601\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.6.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: /s/raw/10ch2yhfk8tyfri/train.tsv [following]\n",
      "--2020-01-15 20:54:30--  https://www.dropbox.com/s/raw/10ch2yhfk8tyfri/train.tsv\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://uc04eedb607c2e4d546760e0fad7.dl.dropboxusercontent.com/cd/0/inline/AwNEwcbnuHVNizCg3fa78oZ_2uNcbAxrkfWpBSCQPgTa8aFDykj3YOZURZRSVgvccG9bw_05o2aRiU1gWtFoy9LyfV_OtFzo9Oga4Fq_fzm3rCdfJAPkdqj4spC7zrEWNXU/file# [following]\n",
      "--2020-01-15 20:54:31--  https://uc04eedb607c2e4d546760e0fad7.dl.dropboxusercontent.com/cd/0/inline/AwNEwcbnuHVNizCg3fa78oZ_2uNcbAxrkfWpBSCQPgTa8aFDykj3YOZURZRSVgvccG9bw_05o2aRiU1gWtFoy9LyfV_OtFzo9Oga4Fq_fzm3rCdfJAPkdqj4spC7zrEWNXU/file\n",
      "Resolving uc04eedb607c2e4d546760e0fad7.dl.dropboxusercontent.com (uc04eedb607c2e4d546760e0fad7.dl.dropboxusercontent.com)... 162.125.6.6, 2620:100:601c:6::a27d:606\n",
      "Connecting to uc04eedb607c2e4d546760e0fad7.dl.dropboxusercontent.com (uc04eedb607c2e4d546760e0fad7.dl.dropboxusercontent.com)|162.125.6.6|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 21972916 (21M) [text/plain]\n",
      "Saving to: ‘/home/ubuntu/machine_learning_aws/data/train.tsv’\n",
      "\n",
      "/home/ubuntu/machin 100%[===================>]  20.95M  88.2MB/s    in 0.2s    \n",
      "\n",
      "2020-01-15 20:54:31 (88.2 MB/s) - ‘/home/ubuntu/machine_learning_aws/data/train.tsv’ saved [21972916/21972916]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get stumbleupon data from dropbox\n",
    "! wget -O /home/ubuntu/machine_learning_aws/data/train.tsv \"https://www.dropbox.com/s/10ch2yhfk8tyfri/train.tsv?dl=0\"\n",
    "\n",
    "# Read stumbleupon data using pandas\n",
    "data = pd.read_table(\"/home/ubuntu/machine_learning_aws/data/train.tsv\", sep= \"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Numerical Features (same as last week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alchemy category, converting to one-hots\n",
    "df = data['alchemy_category']   # 2K ? values\n",
    "one_hots = pd.get_dummies(data['alchemy_category'])\n",
    "df = one_hots\n",
    "rename_dict = {'?': 'alchemy_cat_?'}\n",
    "df = df.rename(columns=rename_dict)\n",
    "\n",
    "# FrameTagRatio, leaving as continuous number\n",
    "df_var = data['frameTagRatio']\n",
    "df['frame_tag_ratio'] = df_var\n",
    "\n",
    "\n",
    "# link word score, 0-100 gaussian, keeping continuous\n",
    "df['link_word_score'] = data['linkwordscore']\n",
    "\n",
    "\n",
    "# alchemy category score, with replacing missing values with random\n",
    "df_var = data['alchemy_category_score']\n",
    "df_var_temp = df_var.apply(lambda x: np.random.random() if x == '?' else float(x)).astype('float32')\n",
    "df['alchemy_category_score'] = df_var_temp\n",
    "\n",
    "\n",
    "# num word in url -- discrete 0-25 to custom binning from looking at the histogram\n",
    "df_var = data['numwords_in_url']\n",
    "bins = [0, 6, 8, 13, 25]\n",
    "df_var_temp = pd.cut(x=df_var, bins=bins, right=True, labels=['num_words_url_bin_0', 'num_words_url_bin_1', 'num_words_url_bin_2', 'num_words_url_bin_3'])\n",
    "dummies = pd.get_dummies(df_var_temp)\n",
    "df = pd.concat([df, dummies], axis=1)\n",
    "\n",
    "\n",
    "# parameterized_link_ratio -- leaving as continuous, right-half gaussian\n",
    "df['parameterized_link_ratio'] = data['parametrizedLinkRatio']\n",
    "\n",
    "# spelling errors ratio -- leaving as continuous\n",
    "df['spelling_errors_ratio'] = data['spelling_errors_ratio']\n",
    "\n",
    "\n",
    "# embed_ratio -- bimodal continuous binned into 2 bins\n",
    "df_var = pd.DataFrame(data['embed_ratio'])\n",
    "df_var = df_var['embed_ratio'].apply(lambda x: 1 if x > -1 else 0)\n",
    "dummies = pd.get_dummies(df_var)\n",
    "rename = {0: 'embed_ratio_0', 1: 'embed_ratio_1'}\n",
    "dummies = dummies.rename(columns=rename)\n",
    "df = pd.concat([df, dummies], axis=1)\n",
    "\n",
    "\n",
    "# html_ratio -- leaving continuous\n",
    "df['html_ratio'] = data['html_ratio']\n",
    "\n",
    "# lengthy_link_domain\n",
    "df_var = pd.get_dummies(data['lengthyLinkDomain'])\n",
    "rename = {0: 'lengthy_link_domain_0', 1: 'lengthy_link_domain_1'}\n",
    "df_var = df_var.rename(columns=rename)\n",
    "df = pd.concat([df, df_var], axis=1)\n",
    "\n",
    "df['labels'] = data['label']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training and Testing Data Splits\n",
    "Whenever we create and implement a **supervised** machine learning algorithm, we want to split our data into **training (known as in-sample)** and **testing (known as out-of-sample)** datasets.  The ratio for this can vary, but some common examples for train: test are 0.5: 0.5 and 0.8: 0.2.  The **testing** dataset can also be split into **validation** and **test** datasets - the difference between these two **testing** subsets are given below, courtesy of [machinelearningmastery.com](https://machinelearningmastery.com/difference-test-validation-datasets/).\n",
    "\n",
    "**Validation Dataset**: The sample of data used to provide an unbiased evaluation of a model fit on the training dataset while tuning model hyperparameters. The evaluation becomes more biased as skill on the validation dataset is incorporated into the model configuration.\n",
    "\n",
    "**Test Dataset**: The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing\n",
    "train, val = train_test_split(df, test_size=0.5, train_size=0.5, random_state=234)\n",
    "\n",
    "# Split testing into validation and test\n",
    "val, test = train_test_split(val, test_size=0.5, train_size=0.5, random_state= 675)\n",
    "\n",
    "# Get labels for training dataset\n",
    "train_labels = train['labels']\n",
    "train = train.drop(['labels'], axis=1, inplace=False)\n",
    "\n",
    "# Get labels for validation dataset\n",
    "val_labels = val['labels']\n",
    "val = val.drop(['labels'], axis=1, inplace=False)\n",
    "\n",
    "# Get labels for testing dataset\n",
    "test_labels = test['labels']\n",
    "test = test.drop(['labels'], axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last Time : logistic regression with numerical features : AUC=0.71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create logistic regression model with sklearn\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Fit model, generate predictions, and evaluate it\n",
    "model.fit(train, train_labels)\n",
    "preds = model.predict_proba(val)[:,1]\n",
    "score = roc_auc_score(val_labels, preds)\n",
    "print(\"Score is %s\" % (score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 : Textual feature with Count Vectorizer (Bag of Words)\n",
    "\n",
    "- min_df = minimum freuencey cut-off\n",
    "- max_features = take the top 1000 most common feature\n",
    "- strip_accents = to handle non english letters\n",
    "- ngram_range = we are doing bag of word features here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF add in the text features with tfidf\n",
    "unigram_dtm = CountVectorizer(min_df= 10,  max_features= 1000, strip_accents= \"unicode\",\n",
    "                          ngram_range=(1, 1))\n",
    "unigram_dtm.fit(data[\"boilerplate\"])\n",
    "data_text = unigram_dtm.transform(data[\"boilerplate\"])\n",
    "train_text, val_text = train_test_split(data_text, test_size=0.5, train_size=0.5, random_state=234)\n",
    "val_text, test_text = train_test_split(val_text, test_size=0.5, train_size=0.5, random_state= 675)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly choose features\n",
    "np.random.choice(unigram_dtm.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = train_text.toarray()\n",
    "print (xx.shape)\n",
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print dimensionality of training data\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to array and print dimensions\n",
    "train_text.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and validation datasets with text with concatenation\n",
    "train_with_text = pd.concat([train.reset_index(drop = True), pd.DataFrame(train_text.toarray())], axis=1)\n",
    "val_with_text = pd.concat([val.reset_index(drop = True), pd.DataFrame(val_text.toarray())], axis=1)\n",
    "train_with_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create logistic regression model with sklearn\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Fit, generate predictions, and evaluate model\n",
    "model.fit(train_with_text, train_labels)\n",
    "preds = model.predict_proba(val_with_text)[:,1]\n",
    "score = roc_auc_score(val_labels, preds)\n",
    "print(\"Score is %\" % (score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Textual Features with Count Vectorizer (Bi-Gram)\n",
    "- ngram_range = (2,2) now so we only bi-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF add in the text features with tfidf\n",
    "bigram_dtm = CountVectorizer(min_df= 10,  max_features= 1000, strip_accents= \"unicode\",\n",
    "                          ngram_range=(2, 2))\n",
    "bigram_dtm.fit(data[\"boilerplate\"])\n",
    "data_text = bigram_dtm.transform(data[\"boilerplate\"])\n",
    "train_text, val_text = train_test_split(data_text, test_size=0.5, train_size=0.5, random_state=234)\n",
    "val_text, test_text = train_test_split(val_text, test_size=0.5, train_size=0.5, random_state= 675)\n",
    "\n",
    "train_with_text = pd.concat([train.reset_index(drop = True), pd.DataFrame(train_text.toarray())], axis=1)\n",
    "val_with_text = pd.concat([val.reset_index(drop = True), pd.DataFrame(val_text.toarray())], axis=1)\n",
    "train_with_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features randomly\n",
    "np.random.choice(bigram_dtm.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create logistic regression model with sklearn\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Fit, generate predictions, and evaluate model\n",
    "model.fit(train_with_text, train_labels)\n",
    "preds = model.predict_proba(val_with_text)[:,1]\n",
    "score = roc_auc_score(val_labels, preds)\n",
    "print(\"Score is %\" % (score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Textual Features with term frequency–inverse document frequency (tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF add in the text features with tfidf\n",
    "idf_dtm = TfidfVectorizer(min_df= 10,  max_features= 1000, strip_accents= \"unicode\", ngram_range=(1, 2))\n",
    "idf_dtm.fit(data[\"boilerplate\"])\n",
    "data_text = idf_dtm.transform(data[\"boilerplate\"])\n",
    "\n",
    "# Split into training and testing\n",
    "train_text, val_text = train_test_split(data_text, test_size=0.5, train_size=0.5, random_state=234)\n",
    "\n",
    "# \n",
    "val_text, test_text = train_test_split(val_text, test_size=0.5, train_size=0.5, random_state= 675)\n",
    "\n",
    "train_with_text = pd.concat([train.reset_index(drop = True), pd.DataFrame(train_text.toarray())], axis=1)\n",
    "val_with_text = pd.concat([val.reset_index(drop = True), pd.DataFrame(val_text.toarray())], axis=1)\n",
    "train_with_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features randomly\n",
    "np.random.choice(idf_dtm.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create logistic regression model with sklearn\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Fit, generate predictions, and evaluate model\n",
    "model.fit(train_with_text, train_labels)\n",
    "preds = model.predict_proba(val_with_text)[:,1]\n",
    "score = roc_auc_score(val_labels, preds)\n",
    "print(\"Score is %\" % (score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
