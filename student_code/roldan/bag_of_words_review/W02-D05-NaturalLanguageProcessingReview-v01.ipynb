{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reviewing Bag of Words and TD-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this document, we use a few simple techniques to try and complete the task set out by the Kaggle StumbleUpon Competition listed below.\n",
    "\n",
    "https://www.kaggle.com/c/stumbleupon\n",
    "\n",
    "**Competition**: Some web pages, such as news articles or seasonal recipes, are only relevant for a short period of time. Others continue to be important for a long time.\n",
    "\n",
    "**Goal**: The goal is to identify pages which pages will be relevant for a short span of time, and which will be relevant for a long span on time and are thus considered \"evergreen\".\n",
    "\n",
    "**Evaluation**: Area under the curve (AUC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initial Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import Python Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick hack to fix import path\n",
    "# import sys; sys.path.append('/Users/julianalverio/code/conda/envs/sac/lib/python3.6/site-packages/')\n",
    "\n",
    "# data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#  mjujuuj\n",
    "# plots\n",
    "%matplotlib inline\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "\n",
    "# classification algorithms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "# dimensionality reduction\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# cross-validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection\n",
    "\n",
    "# text features\n",
    "import re\n",
    "from nltk import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# model evaluation\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "os.chdir(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>boilerplate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{\"title\":\"IBM Sees Holographic Calls Air Breat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{\"title\":\"The Fully Electronic Futuristic Star...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{\"title\":\"Fruits that Fight the Flu fruits tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{\"title\":\"10 Foolproof Tips for Better Sleep \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{\"title\":\"The 50 Coolest Jerseys You Didn t Kn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{\"url\":\"conveniencemedical genital herpes home...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{\"title\":\"fashion lane American Wild Child \",\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{\"url\":\"insidershealth article racing for reco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{\"title\":\"Valet The Handbook 31 Days 31 days\",...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{\"url\":\"howsweeteats 2010 03 24 cookies and cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>{\"title\":\"Business Financial News Breaking US ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>{\"title\":\"A Tip of the Cap to The Greatest Iro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>{\"title\":\"9 Foods That Trash Your Teeth \",\"bod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>{\"title\":\" \",\"body\":\" \",\"url\":\"thedailygreen p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>{\"url\":\"phillyburbs blogs type a kitchen frenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>{\"title\":\"Izabel Goulart Swimsuit by Kikidoll ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>{\"title\":\"Liquid Mountaineering The Awesomer \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>{\"title\":null,\"body\":\"The annual Chap Olympiad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>{\"title\":\"Grilled Peaches Sugarcrafter \",\"body...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>{\"title\":\"How to Make Your Home Healthier A Ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>{\"url\":\"tmz 2012 12 20 olympic soccer alex mor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>{\"title\":\"BBC Food Recipes Blueberry and lemon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>{\"title\":\"Quit Smoking Counter Online counter ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>{\"title\":\"Original techniques to tie your shoe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>{\"title\":\" \",\"body\":\" \",\"url\":\"blogs babble fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>{\"title\":\"Best College Football Fan Sign Ever ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>{\"title\":\"Breaking News Blog \",\"body\":\"Contact...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>{\"title\":\"The Alton Brown Flower Pot Smoker \",...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>{\"title\":\"Supermodels show off their bikini bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>{\"title\":\"Genevieve Morton Swimsuit by Tyler R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7365</th>\n",
       "      <td>{\"url\":\"cheezburger 6631333376\",\"title\":\"Choco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7366</th>\n",
       "      <td>{\"title\":\"Money Happiness Newsweek money &amp; hap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7367</th>\n",
       "      <td>{\"title\":\" \",\"body\":\" \",\"url\":\"ca askmen sport...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7368</th>\n",
       "      <td>{\"title\":\"Sourdough Sour Cherry Bread Pudding ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7369</th>\n",
       "      <td>{\"title\":\"Noble Pig Mini Peanut Butter Finger ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7370</th>\n",
       "      <td>{\"title\":\"Fancy Knot Dress in Clothes at Nasty...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7371</th>\n",
       "      <td>{\"url\":\"betterinbulk 2012 10 perfect pumpkin b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7372</th>\n",
       "      <td>{\"title\":\"Get the Lead Out Cartridge Free Eras...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7373</th>\n",
       "      <td>{\"title\":\"blog khymos org dedicated to molecul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7374</th>\n",
       "      <td>{\"title\":\"Versace launches haute couture for a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7375</th>\n",
       "      <td>{\"url\":\"whatmegansmaking\",\"title\":\"What Megan ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7376</th>\n",
       "      <td>{\"title\":\"Chinese Food Recipes Cooking Chinese...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7377</th>\n",
       "      <td>{\"title\":\"Google Gives iPhone Users Browser Ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7378</th>\n",
       "      <td>{\"title\":\"12 Smoothies Recipes and Cooking Foo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7379</th>\n",
       "      <td>{\"title\":\"Toasted Sesame Ginger Salmon TechMun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7380</th>\n",
       "      <td>{\"url\":\"m ivillage wicked good chocolate peanu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7381</th>\n",
       "      <td>{\"title\":\"Caravan Style \",\"body\":\" fashion, in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7382</th>\n",
       "      <td>{\"title\":\"Mountain Dew Cupcakes All Things Cup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7383</th>\n",
       "      <td>{\"title\":\"80s kids Random RR \",\"body\":\" \",\"url...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7384</th>\n",
       "      <td>{\"title\":\"Homemade Cake Mix i am baker \",\"body...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7385</th>\n",
       "      <td>{\"url\":\"shopmarkethq collections brands minkpi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7386</th>\n",
       "      <td>{\"title\":\"How to Make No Knead Pizza Dough No ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7387</th>\n",
       "      <td>{\"title\":\"Vintage Funk A Chic Direction \",\"bod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7388</th>\n",
       "      <td>{\"url\":\"villagevoice bestof 2009 award best pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7389</th>\n",
       "      <td>{\"title\":\"Gummy Snake Worth1000 Contests \",\"bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7390</th>\n",
       "      <td>{\"title\":\"Kno Raises 46 Million More To Build ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7391</th>\n",
       "      <td>{\"title\":\"Why I Miss College \",\"body\":\"Mar 30 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7392</th>\n",
       "      <td>{\"title\":\"Sweet Potatoes Eat This Not That  i'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7393</th>\n",
       "      <td>{\"title\":\"Naturally Ella \",\"body\":\" \",\"url\":\"n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7394</th>\n",
       "      <td>{\"title\":\"Esti Ginzburg Swimsuit by Letarte by...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7395 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            boilerplate\n",
       "0     {\"title\":\"IBM Sees Holographic Calls Air Breat...\n",
       "1     {\"title\":\"The Fully Electronic Futuristic Star...\n",
       "2     {\"title\":\"Fruits that Fight the Flu fruits tha...\n",
       "3     {\"title\":\"10 Foolproof Tips for Better Sleep \"...\n",
       "4     {\"title\":\"The 50 Coolest Jerseys You Didn t Kn...\n",
       "5     {\"url\":\"conveniencemedical genital herpes home...\n",
       "6     {\"title\":\"fashion lane American Wild Child \",\"...\n",
       "7     {\"url\":\"insidershealth article racing for reco...\n",
       "8     {\"title\":\"Valet The Handbook 31 Days 31 days\",...\n",
       "9     {\"url\":\"howsweeteats 2010 03 24 cookies and cr...\n",
       "10    {\"title\":\"Business Financial News Breaking US ...\n",
       "11    {\"title\":\"A Tip of the Cap to The Greatest Iro...\n",
       "12    {\"title\":\"9 Foods That Trash Your Teeth \",\"bod...\n",
       "13    {\"title\":\" \",\"body\":\" \",\"url\":\"thedailygreen p...\n",
       "14    {\"url\":\"phillyburbs blogs type a kitchen frenc...\n",
       "15    {\"title\":\"Izabel Goulart Swimsuit by Kikidoll ...\n",
       "16    {\"title\":\"Liquid Mountaineering The Awesomer \"...\n",
       "17    {\"title\":null,\"body\":\"The annual Chap Olympiad...\n",
       "18    {\"title\":\"Grilled Peaches Sugarcrafter \",\"body...\n",
       "19    {\"title\":\"How to Make Your Home Healthier A Ro...\n",
       "20    {\"url\":\"tmz 2012 12 20 olympic soccer alex mor...\n",
       "21    {\"title\":\"BBC Food Recipes Blueberry and lemon...\n",
       "22    {\"title\":\"Quit Smoking Counter Online counter ...\n",
       "23    {\"title\":\"Original techniques to tie your shoe...\n",
       "24    {\"title\":\" \",\"body\":\" \",\"url\":\"blogs babble fa...\n",
       "25    {\"title\":\"Best College Football Fan Sign Ever ...\n",
       "26    {\"title\":\"Breaking News Blog \",\"body\":\"Contact...\n",
       "27    {\"title\":\"The Alton Brown Flower Pot Smoker \",...\n",
       "28    {\"title\":\"Supermodels show off their bikini bo...\n",
       "29    {\"title\":\"Genevieve Morton Swimsuit by Tyler R...\n",
       "...                                                 ...\n",
       "7365  {\"url\":\"cheezburger 6631333376\",\"title\":\"Choco...\n",
       "7366  {\"title\":\"Money Happiness Newsweek money & hap...\n",
       "7367  {\"title\":\" \",\"body\":\" \",\"url\":\"ca askmen sport...\n",
       "7368  {\"title\":\"Sourdough Sour Cherry Bread Pudding ...\n",
       "7369  {\"title\":\"Noble Pig Mini Peanut Butter Finger ...\n",
       "7370  {\"title\":\"Fancy Knot Dress in Clothes at Nasty...\n",
       "7371  {\"url\":\"betterinbulk 2012 10 perfect pumpkin b...\n",
       "7372  {\"title\":\"Get the Lead Out Cartridge Free Eras...\n",
       "7373  {\"title\":\"blog khymos org dedicated to molecul...\n",
       "7374  {\"title\":\"Versace launches haute couture for a...\n",
       "7375  {\"url\":\"whatmegansmaking\",\"title\":\"What Megan ...\n",
       "7376  {\"title\":\"Chinese Food Recipes Cooking Chinese...\n",
       "7377  {\"title\":\"Google Gives iPhone Users Browser Ch...\n",
       "7378  {\"title\":\"12 Smoothies Recipes and Cooking Foo...\n",
       "7379  {\"title\":\"Toasted Sesame Ginger Salmon TechMun...\n",
       "7380  {\"url\":\"m ivillage wicked good chocolate peanu...\n",
       "7381  {\"title\":\"Caravan Style \",\"body\":\" fashion, in...\n",
       "7382  {\"title\":\"Mountain Dew Cupcakes All Things Cup...\n",
       "7383  {\"title\":\"80s kids Random RR \",\"body\":\" \",\"url...\n",
       "7384  {\"title\":\"Homemade Cake Mix i am baker \",\"body...\n",
       "7385  {\"url\":\"shopmarkethq collections brands minkpi...\n",
       "7386  {\"title\":\"How to Make No Knead Pizza Dough No ...\n",
       "7387  {\"title\":\"Vintage Funk A Chic Direction \",\"bod...\n",
       "7388  {\"url\":\"villagevoice bestof 2009 award best pl...\n",
       "7389  {\"title\":\"Gummy Snake Worth1000 Contests \",\"bo...\n",
       "7390  {\"title\":\"Kno Raises 46 Million More To Build ...\n",
       "7391  {\"title\":\"Why I Miss College \",\"body\":\"Mar 30 ...\n",
       "7392  {\"title\":\"Sweet Potatoes Eat This Not That  i'...\n",
       "7393  {\"title\":\"Naturally Ella \",\"body\":\" \",\"url\":\"n...\n",
       "7394  {\"title\":\"Esti Ginzburg Swimsuit by Letarte by...\n",
       "\n",
       "[7395 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read stumbleupon data using pandas\n",
    "data = pd.read_table(\"train.tsv\", sep= \"\\t\")\n",
    "\n",
    "# Look at data\n",
    "df = pd.DataFrame(data[\"boilerplate\"]) # extract data\n",
    "df # print out dataframe to look at it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Using Numerical Features (same as last week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alchemy category, converting to one-hots\n",
    "df = data['alchemy_category']   # 2K ? values\n",
    "one_hots = pd.get_dummies(data['alchemy_category'])\n",
    "df = one_hots\n",
    "rename_dict = {'?': 'alchemy_cat_?'}\n",
    "df = df.rename(columns=rename_dict)\n",
    "\n",
    "# FrameTagRatio, leaving as continuous number\n",
    "df_var = data['frameTagRatio']\n",
    "df['frame_tag_ratio'] = df_var\n",
    "\n",
    "# link word score, 0-100 gaussian, keeping continuous\n",
    "df['link_word_score'] = data['linkwordscore']\n",
    "\n",
    "# alchemy category score, with replacing missing values with random\n",
    "df_var = data['alchemy_category_score']\n",
    "df_var_temp = df_var.apply(lambda x: np.random.random() if x == '?' else float(x)).astype('float32')\n",
    "df['alchemy_category_score'] = df_var_temp\n",
    "\n",
    "# num word in url -- discrete 0-25 to custom binning from looking at the histogram\n",
    "df_var = data['numwords_in_url']\n",
    "bins = [0, 6, 8, 13, 25]\n",
    "df_var_temp = pd.cut(x=df_var, bins=bins, right=True, labels=['num_words_url_bin_0', 'num_words_url_bin_1', 'num_words_url_bin_2', 'num_words_url_bin_3'])\n",
    "dummies = pd.get_dummies(df_var_temp)\n",
    "df = pd.concat([df, dummies], axis=1)\n",
    "\n",
    "# parameterized_link_ratio -- leaving as continuous, right-half gaussian\n",
    "df['parameterized_link_ratio'] = data['parametrizedLinkRatio']\n",
    "\n",
    "# spelling errors ratio -- leaving as continuous\n",
    "df['spelling_errors_ratio'] = data['spelling_errors_ratio']\n",
    "\n",
    "# embed_ratio -- bimodal continuous binned into 2 bins\n",
    "df_var = pd.DataFrame(data['embed_ratio'])\n",
    "df_var = df_var['embed_ratio'].apply(lambda x: 1 if x > -1 else 0)\n",
    "dummies = pd.get_dummies(df_var)\n",
    "rename = {0: 'embed_ratio_0', 1: 'embed_ratio_1'}\n",
    "dummies = dummies.rename(columns=rename)\n",
    "df = pd.concat([df, dummies], axis=1)\n",
    "\n",
    "# html_ratio -- leaving continuous\n",
    "df['html_ratio'] = data['html_ratio']\n",
    "\n",
    "# lengthy_link_domain\n",
    "df_var = pd.get_dummies(data['lengthyLinkDomain'])\n",
    "rename = {0: 'lengthy_link_domain_0', 1: 'lengthy_link_domain_1'}\n",
    "df_var = df_var.rename(columns=rename)\n",
    "df = pd.concat([df, df_var], axis=1)\n",
    "\n",
    "df['labels'] = data['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Creating Training and Testing Data Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing\n",
    "train, val = train_test_split(df, test_size=0.5, train_size=0.5, random_state=234)\n",
    "\n",
    "# Split testing into validation and test\n",
    "val, test = train_test_split(val, test_size=0.5, train_size=0.5, random_state=675)\n",
    "\n",
    "# Get labels for training dataset\n",
    "train_labels = train['labels']\n",
    "train = train.drop(['labels'], axis=1, inplace=False)\n",
    "\n",
    "# Get labels for validation dataset\n",
    "val_labels = val['labels']\n",
    "val = val.drop(['labels'], axis=1, inplace=False)\n",
    "\n",
    "# Get labels for testing dataset\n",
    "test_labels = test['labels']\n",
    "test = test.drop(['labels'], axis=1, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Import Data\n",
    "\n",
    "The data for this exercise has been placed within the folder that this notebook is in. Therefore, we can simply reference it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read stumbleupon data using pandas\n",
    "data = pd.read_table(\"train.tsv\", sep= \"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Using Count Vectorizer\n",
    "\n",
    "Below is the code that we will re-run from last week. We have four major features which are relevant! I have written the descriptions from the function documentation for your convenience. Because of this, know that it is often your responsibility to do this step. Understanding what functions you are using solely relies on how diligently you reference the documentation! The link to the documentation is here, but please do this yourself for the exercise. Here is the [link](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.fit) to the `CountVectorizer` class documentation.\n",
    "\n",
    "- *min_df* = minimum frequencey cut-off\n",
    "    - min_dffloat in range [0.0, 1.0] or int, default=1: When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n",
    "- *max_features* = take the top 1000 most common feature\n",
    "    - max_featuresint or None, default=None: If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n",
    "- *strip_accents* = to handle non english letters\n",
    "    - strip_accents{‘ascii’, ‘unicode’, None}: Remove accents and perform other character normalization during the preprocessing step. ‘ascii’ is a fast method that only works on characters that have an direct ASCII mapping. ‘unicode’ is a slightly slower method that works on any characters. None (default) does nothing. Both ‘ascii’ and ‘unicode’ use NFKD normalization from unicodedata.normalize.\n",
    "- *ngram_range* = we are doing bag of word features here\n",
    "    - ngram_rangetuple (min_n, max_n), default=(1, 1): The lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted. All values of n such such that min_n <= n <= max_n will be used. For example an ngram_range of (1, 1) means only unigrams, (1, 2) means unigrams and bigrams, and (2, 2) means only bigrams. Only applies if analyzer is not callable.\n",
    "\n",
    "#### 2.2.1 Instantiate Class\n",
    "\n",
    "In this first function call, we instantiate an instance of the class CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "                lowercase=True, max_df=1.0, max_features=1000, min_df=10,\n",
      "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "                strip_accents='unicode', token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=None, vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate our class\n",
    "unigram_dtm = CountVectorizer(min_df= 10,  max_features= 1000, strip_accents= \"unicode\",\n",
    "                          ngram_range=(1, 1))\n",
    "print(unigram_dtm) # by printing this variable, we see that it outputs a class description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Make Training and Testing Sets to do Bag of Words\n",
    "\n",
    "We have talked about generating training and testing data. Below are functions which do this. If the code is confusing, reference the documentation associated with the functions! Here is an example which shows you what this function does. When writing your own code, try and look at examples or make examples like this to mess around with your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our data:\n",
      "[[0 1]\n",
      " [2 3]\n",
      " [4 5]\n",
      " [6 7]\n",
      " [8 9]]\n",
      "Our training set:\n",
      "[[8 9]\n",
      " [0 1]]\n",
      "Our testing set:\n",
      "[[4 5]\n",
      " [6 7]\n",
      " [2 3]]\n"
     ]
    }
   ],
   "source": [
    "# Example of using the function\n",
    "X = np.arange(10).reshape((5, 2))\n",
    "print(\"Our data:\\n{}\".format(X))\n",
    "\n",
    "X_train, X_test = train_test_split(X, test_size=0.5, random_state=88)\n",
    "print(\"Our training set:\\n{}\".format(X_train))\n",
    "print(\"Our testing set:\\n{}\".format(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is where we split the data relevant with our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data Before Generating Bag of Words Representation\n",
    "train_boilerplate, val_boilerplate = train_test_split(data['boilerplate'], test_size=0.5, train_size=0.5, random_state=234)\n",
    "val_boilerplate, test_boilerplate = train_test_split(val_boilerplate, test_size=0.5, train_size=0.5, random_state=675)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the instantiation of the class, we then run some functions which help us to build a Bag of Words representation of the data.\n",
    "\n",
    "#### 2.2.3 Learn Vocabulary from Document\n",
    "\n",
    "Below, we call the `fit()` function in order to learn a vocabulary from one or more documents. \n",
    "\n",
    "#### 2.2.4 Generate Bag of Words Vectors from Document\n",
    "\n",
    "Following this, we call the `transform()` function on one or more documents as needed to encode each as a vector. \n",
    "\n",
    "In this case, we use the same data for each function. Think about why this is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "  (0, 0)\t2\n",
      "  (0, 1)\t2\n",
      "  (0, 11)\t2\n",
      "  (0, 12)\t1\n",
      "  (0, 13)\t1\n",
      "  (0, 17)\t1\n",
      "  (0, 28)\t3\n",
      "  (0, 29)\t1\n",
      "  (0, 40)\t1\n",
      "  (0, 43)\t1\n",
      "  (0, 45)\t1\n",
      "  (0, 48)\t1\n",
      "  (0, 55)\t1\n",
      "  (0, 57)\t1\n",
      "  (0, 58)\t2\n",
      "  (0, 61)\t1\n",
      "  (0, 64)\t2\n",
      "  (0, 71)\t3\n",
      "  (0, 72)\t17\n",
      "  (0, 73)\t1\n",
      "  (0, 77)\t1\n",
      "  (0, 78)\t6\n",
      "  (0, 80)\t2\n",
      "  (0, 81)\t2\n",
      "  (0, 85)\t3\n",
      "  :\t:\n",
      "  (3696, 920)\t2\n",
      "  (3696, 921)\t2\n",
      "  (3696, 922)\t1\n",
      "  (3696, 923)\t1\n",
      "  (3696, 935)\t1\n",
      "  (3696, 945)\t7\n",
      "  (3696, 946)\t1\n",
      "  (3696, 949)\t1\n",
      "  (3696, 951)\t13\n",
      "  (3696, 956)\t1\n",
      "  (3696, 958)\t1\n",
      "  (3696, 960)\t4\n",
      "  (3696, 963)\t4\n",
      "  (3696, 966)\t2\n",
      "  (3696, 970)\t10\n",
      "  (3696, 971)\t1\n",
      "  (3696, 972)\t1\n",
      "  (3696, 974)\t5\n",
      "  (3696, 977)\t7\n",
      "  (3696, 979)\t1\n",
      "  (3696, 981)\t1\n",
      "  (3696, 987)\t3\n",
      "  (3696, 992)\t4\n",
      "  (3696, 993)\t1\n",
      "  (3696, 995)\t4\n"
     ]
    }
   ],
   "source": [
    "# Make Bag of Words Representation\n",
    "unigram_dtm.fit(train_boilerplate) # here we are creating a dictionary\n",
    "train_text = unigram_dtm.transform(train_boilerplate) # here, we encode this document as a vector in our dictionary space\n",
    "val_text = unigram_dtm.transform(val_boilerplate) # why do we have to transform but not fit?\n",
    "\n",
    "# Look at some of the variables\n",
    "print(type(train_text)) # when looking at the output, keep in mind that this data type is a SPARSE matrix\n",
    "print(train_text) # looking at the data, what may sparse mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we actually implement the functions for our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.5 Explore Your Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cakes', 'against', '29', 'size', 'kind', 'hands', 'pictures',\n",
       "       'usually', 'for', 'september'], dtype='<U11')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly choose features\n",
    "np.random.choice(unigram_dtm.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3697, 1000)\n",
      "\n",
      "Looking at our training text matrix:\n",
      "[[ 2  2  0 ...  2  3  0]\n",
      " [ 0  0  0 ... 30 33  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  0  2  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  7  1 ...  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "xx = train_text.toarray() # convert data type to something easier to look at\n",
    "print(xx.shape) # look at shape of array\n",
    "print(\"\\nLooking at our training text matrix:\\n{}\".format(xx)) # look at data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3697, 28)\n",
      "(3697, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Print dimensionality of training data\n",
    "print(train.shape)\n",
    "# Convert to array and print dimensions\n",
    "print(train_text.toarray().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.6 Add New Features to Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alchemy_cat_?</th>\n",
       "      <th>arts_entertainment</th>\n",
       "      <th>business</th>\n",
       "      <th>computer_internet</th>\n",
       "      <th>culture_politics</th>\n",
       "      <th>gaming</th>\n",
       "      <th>health</th>\n",
       "      <th>law_crime</th>\n",
       "      <th>recreation</th>\n",
       "      <th>religion</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1028 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   alchemy_cat_?  arts_entertainment  business  computer_internet  \\\n",
       "0              1                   0         0                  0   \n",
       "1              1                   0         0                  0   \n",
       "2              1                   0         0                  0   \n",
       "3              0                   0         0                  0   \n",
       "4              1                   0         0                  0   \n",
       "\n",
       "   culture_politics  gaming  health  law_crime  recreation  religion ...   \\\n",
       "0                 0       0       0          0           0         0 ...    \n",
       "1                 0       0       0          0           0         0 ...    \n",
       "2                 0       0       0          0           0         0 ...    \n",
       "3                 0       0       0          0           0         0 ...    \n",
       "4                 0       0       0          0           0         0 ...    \n",
       "\n",
       "   990  991  992  993  994  995  996  997  998  999  \n",
       "0    0    0    0    0    0    0    0    2    3    0  \n",
       "1    0    0    0    0    0    0    0   30   33    0  \n",
       "2    0    0    0    0    0    0    0    0    0    0  \n",
       "3    0    0    0    0    0    1    0    7    1    0  \n",
       "4    0    0    0    0    0    0    0    1    0    0  \n",
       "\n",
       "[5 rows x 1028 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create training and validation datasets with text with concatenation\n",
    "train_with_text = pd.concat([train.reset_index(drop = True), pd.DataFrame(train_text.toarray())], axis=1)\n",
    "val_with_text = pd.concat([val.reset_index(drop = True), pd.DataFrame(val_text.toarray())], axis=1)\n",
    "train_with_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.7 Do Logistic Regression with new Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score is 0.7966274228989841\n"
     ]
    }
   ],
   "source": [
    "# Create logistic regression model with sklearn\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Fit, generate predictions, and evaluate model\n",
    "model.fit(train_with_text, train_labels.values)\n",
    "preds = model.predict_proba(val_with_text)[:,1]\n",
    "score = roc_auc_score(val_labels, preds)\n",
    "print(\"Score is {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Keras Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Instantiate Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Make Training and Testing Sets to do Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3 Learn Vocabulary from Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.4 Generate Bag of Words Vectors from Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.5 Explore Your Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.6 Add New Features to Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.7 Do Logistic Regression with New Feature Set in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
